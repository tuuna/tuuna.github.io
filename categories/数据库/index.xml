<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>数据库 on 洛文小站</title>
    <link>https://tuuna.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/</link>
    <description>Recent content in 数据库 on 洛文小站</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 21 Apr 2020 03:55:57 +0000</lastBuildDate><atom:link href="https://tuuna.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Seata分布式事务框架的流程解析</title>
      <link>https://tuuna.github.io/2020/04/21/seata/</link>
      <pubDate>Tue, 21 Apr 2020 03:55:57 +0000</pubDate>
      
      <guid>https://tuuna.github.io/2020/04/21/seata/</guid>
      <description>XID：全局事务的唯一标识，由 ip:port:sequence 组成； Transaction Coordinator (TC)：事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚； Transaction Manager (TM )：控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议； Resource Manager (RM)：控制分支事务，负责分支注册、状态汇报，并接收事务协调器的指令，驱动分支（本地）事务的提交和回滚；  业务逻辑是经典的下订单、扣余额、减库存流程。 根据模块划分为三个独立的服务，且分别连接对应的数据库：
 订单：order-server 账户：account-server 库存：storage-server 业务：business-server  项目结构如下图
正常业务:   business发起购买请求
  storage扣减库存
  order创建订单
  account扣减余额
  异常业务： 扣减余额异常
正常流程下 2、3、4 步的数据正常更新全局 commit，异常流程下的数据则由于第 4 步的异常报错全局回滚。
使用流程及解析
 首先创建相应的数据库与数据表     库名 含有的表 作用     db_account account_tbl undo_log 存储用户信息   db_order order_tbl undo_log 订单信息   db_storage storage_tbl undo_log 库存信息    注:上面的每一个库可以存储与不同的机器上面。需要在不同的服务里面配置即可</description>
    </item>
    
    <item>
      <title>2e数据量下MongoDB查询性能测试</title>
      <link>https://tuuna.github.io/2019/09/29/mongo_test/</link>
      <pubDate>Sun, 29 Sep 2019 11:16:31 +0000</pubDate>
      
      <guid>https://tuuna.github.io/2019/09/29/mongo_test/</guid>
      <description>硬件环境：三台4核CPU、32G内存、100G磁盘 测试数据量: 1.96e
Order表结构如下：
{ &amp;quot;_id&amp;quot;: ObjectId, &amp;quot;uid&amp;quot;: &amp;quot;uid186642&amp;quot;, //有索引 &amp;quot;name&amp;quot;: &amp;quot;商品214&amp;quot;, &amp;quot;money&amp;quot;: 899 }    语句类型 查询语句 有无索引 测试结果(时间，返回数据量)     等值查询 db.order.find({uid:“uid150902”}) 有 7ms, 949条   等值查询 db.order.find({money:999}) 无 28.57s, 1929条   In查询 db.order.find({uid: {$in: [“uid100000”, “uid123456”, “uid089345”]}}) 有 19ms, 2860条   In查询 db.order.find({money: {$in: [300, 450, 999]}}) 无 30.37s, 5871条   范围查询 db.order.find({uid:{$gt:“uid198800”}}) 有 4.59s，117w条   范围查询 db.order.find({money:{$gt:990}}) 无 29.</description>
    </item>
    
    <item>
      <title>Hbase &#43; PostgreSQL测试</title>
      <link>https://tuuna.github.io/2019/09/29/hbase_postgresql_test/</link>
      <pubDate>Sun, 29 Sep 2019 11:10:32 +0000</pubDate>
      
      <guid>https://tuuna.github.io/2019/09/29/hbase_postgresql_test/</guid>
      <description>文章目录  Hbase + PostgreSQL测试  基本信息 测试方法  HBASE PG   测试图例    基本信息 Hbase与PG测试使用YCSB压力测试工具
采用21w行数据+21w操作数来最大化模拟20G数据场景
测试方法 HBASE   首先下载YCSB，解压
  接下来创建测试数据库
hbase shell命令打开shell命令行
hbase(main):001:0&amp;gt; n_splits = 20 # HBase recommends (10 * number of regionservers) hbase(main):002:0&amp;gt; create &#39;usertable&#39;, &#39;cf&#39;, {SPLITS =&amp;gt; (1..n_splits).map {|i| &amp;quot;user#{1000+i*(9999-1000)/n_splits}&amp;quot;}}   把hbase的conf目录下的|$HBASE_HOME/conf/hbase-site.xml(比如我本机是/usr/local/hbase/conf/hbase-site.xml)复制到我们下载的YCSB中的hbase14下的conf文件夹中
  接下来装载数据
$ ./bin/ycsb load hbase14 -P workloads/workloada -p table=usertable -p columnfamily=cf -p recordcount=210000 -p operationcount=210000 -p measurementtype=hdrhistogram -p hdrhistogram.</description>
    </item>
    
    <item>
      <title>DB与FS比较</title>
      <link>https://tuuna.github.io/2019/09/26/DB_fs_compare/</link>
      <pubDate>Thu, 26 Sep 2019 09:15:52 +0000</pubDate>
      
      <guid>https://tuuna.github.io/2019/09/26/DB_fs_compare/</guid>
      <description>文章目录  数据库以及FS比较  HDFS、Ceph差异对比  HDFS设计目标 HDFS文件目录 Ceph设计目标 Ceph数据结构 HDFS文件导出 Ceph文件导出 暂时的结论 其它FS选型 其它思路 总结   Hbase、Cassendra、TiDB适用场景以及相关生态  Hbase  基本简介 特性 优势 缺点   Cassendra  基本简介 重要特性 优势 劣势   TiDB  基本简介 特性 使用场景 生态     已存在HDFS中的解析成结果数据  导入到 SQL 从 Sqoop 中导出      HDFS、Ceph差异对比 HDFS设计目标   存储非常大的文件：这里非常大指的是几百M、G、或者TB级别。实际应用中已有很多集群存储的数据达到PB级别。根据Hadoop官网，Yahoo！的Hadoop集群约有10万颗CPU，运行在4万个机器节点上。更多世界上的Hadoop集群使用情况，参考Hadoop官网.
  采用流式的数据访问方式: HDFS基于这样的一个假设：最有效的数据处理模式是一次写入、多次读取数据集经常从数据源生成或者拷贝一次，然后在其上做很多分析工作 分析工作经常读取其中的大部分数据，即使不是全部。 因此读取整个数据集所需时间比读取第一条记录的延时更重要。
  运行于商业硬件上: Hadoop不需要特别贵的、reliable的机器，可运行于普通商用机器（可以从多家供应商采购） 商用机器不代表低端机器在集群中（尤其是大的集群），节点失败率是比较高的HDFS的目标是确保集群在节点失败的时候不会让用户感觉到明显的中断。</description>
    </item>
    
  </channel>
</rss>
