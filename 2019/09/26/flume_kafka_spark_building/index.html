<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Flume+Kafka+Spark环境搭建 | 洛文小站</title><meta name=keywords content="大数据"><meta name=description content="文章目录    单机版环境搭建及相关DEMO  Flume  Flume基本介绍与架构 Flume安装部署 案例实操   Kafka  环境搭建 Kafka控制台的一些命令操作 Java API控制Kafka Flume+Kafka配合   Spark  Spark 简介 Spark环境搭建 在Spark Shell 中运行代码 Scala编写wordCount 在Spark-Shell中执行词频统计 词频统计 编写独立应用程序执行词频统计   Flume_Kafka_SparkStreaming实现词频统计  准备工作     分布式环境搭建及相关DEMO  Flume Kafka Spark       本文档主要讲述了flume+kafka+spark的单机分布式搭建，由浅入深，介绍了常见大数据流处理流程
 单机版环境搭建及相关DEMO Flume Flume基本介绍与架构 Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。
Flume出生日记
有很多的服务和系统
 network devices operating system web servers Applications  这些系统都会产生很多的日志，那么把这些日志拿出来，用来分析时非常有用的。"><meta name=author content="赖杰"><link rel=canonical href=http://tuuna.top/2019/09/26/flume_kafka_spark_building/><link href=http://tuuna.top/assets/css/stylesheet.min.46b233709fd2c907e04a2d946c05f4e1d6edaf81b562cbfdaf6f11aeaa32248c.css integrity="sha256-RrIzcJ/SyQfgSi2UbAX04dbtr4G1Ysv9r28RrqoyJIw=" rel="preload stylesheet" as=style><link rel=icon href=http://tuuna.top/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://tuuna.top/favicon16x16.png><link rel=icon type=image/png sizes=32x32 href=http://tuuna.top/favicon32x32.png><link rel=apple-touch-icon href=http://tuuna.top/apple-touch-icon.png><link rel=mask-icon href=http://tuuna.top/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.78.2"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-123-45','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><meta property="og:title" content="Flume+Kafka+Spark环境搭建"><meta property="og:description" content="文章目录    单机版环境搭建及相关DEMO  Flume  Flume基本介绍与架构 Flume安装部署 案例实操   Kafka  环境搭建 Kafka控制台的一些命令操作 Java API控制Kafka Flume+Kafka配合   Spark  Spark 简介 Spark环境搭建 在Spark Shell 中运行代码 Scala编写wordCount 在Spark-Shell中执行词频统计 词频统计 编写独立应用程序执行词频统计   Flume_Kafka_SparkStreaming实现词频统计  准备工作     分布式环境搭建及相关DEMO  Flume Kafka Spark       本文档主要讲述了flume+kafka+spark的单机分布式搭建，由浅入深，介绍了常见大数据流处理流程
 单机版环境搭建及相关DEMO Flume Flume基本介绍与架构 Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。
Flume出生日记
有很多的服务和系统
 network devices operating system web servers Applications  这些系统都会产生很多的日志，那么把这些日志拿出来，用来分析时非常有用的。"><meta property="og:type" content="article"><meta property="og:url" content="http://tuuna.top/2019/09/26/flume_kafka_spark_building/"><meta property="article:published_time" content="2019-09-26T08:52:30+00:00"><meta property="article:modified_time" content="2019-09-26T08:52:30+00:00"><meta property="og:site_name" content="洛文小站"><meta name=twitter:card content="summary"><meta name=twitter:title content="Flume+Kafka+Spark环境搭建"><meta name=twitter:description content="文章目录    单机版环境搭建及相关DEMO  Flume  Flume基本介绍与架构 Flume安装部署 案例实操   Kafka  环境搭建 Kafka控制台的一些命令操作 Java API控制Kafka Flume+Kafka配合   Spark  Spark 简介 Spark环境搭建 在Spark Shell 中运行代码 Scala编写wordCount 在Spark-Shell中执行词频统计 词频统计 编写独立应用程序执行词频统计   Flume_Kafka_SparkStreaming实现词频统计  准备工作     分布式环境搭建及相关DEMO  Flume Kafka Spark       本文档主要讲述了flume+kafka+spark的单机分布式搭建，由浅入深，介绍了常见大数据流处理流程
 单机版环境搭建及相关DEMO Flume Flume基本介绍与架构 Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。
Flume出生日记
有很多的服务和系统
 network devices operating system web servers Applications  这些系统都会产生很多的日志，那么把这些日志拿出来，用来分析时非常有用的。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Flume+Kafka+Spark环境搭建","name":"Flume\u002bKafka\u002bSpark环境搭建","description":"文章目录    单机版环境搭建及相关DEMO  Flume  Flume基本介绍与架构 Flume安装部署 案例实操   Kafka  环境搭建 Kafka控制台的一些命令操作 Java API控制Kafka Flume+Kafka配合   Spark  Spark 简介 Spark环境搭建 在Spark Shell 中运行代码 Scala编 …","keywords":["大数据"],"articleBody":"文章目录    单机版环境搭建及相关DEMO  Flume  Flume基本介绍与架构 Flume安装部署 案例实操   Kafka  环境搭建 Kafka控制台的一些命令操作 Java API控制Kafka Flume+Kafka配合   Spark  Spark 简介 Spark环境搭建 在Spark Shell 中运行代码 Scala编写wordCount 在Spark-Shell中执行词频统计 词频统计 编写独立应用程序执行词频统计   Flume_Kafka_SparkStreaming实现词频统计  准备工作     分布式环境搭建及相关DEMO  Flume Kafka Spark       本文档主要讲述了flume+kafka+spark的单机分布式搭建，由浅入深，介绍了常见大数据流处理流程\n 单机版环境搭建及相关DEMO Flume Flume基本介绍与架构 Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。\nFlume出生日记\n有很多的服务和系统\n network devices operating system web servers Applications  这些系统都会产生很多的日志，那么把这些日志拿出来，用来分析时非常有用的。\n如何解决数据从其他的server上移动到Hadoop上？\nshell cp hadoop集群上的机器上， hadoop fs -put …/ 直接拷贝日志，但是没办法监控，而cp的时效性也不好，容错负载均衡也没办法做\n======\nFlume诞生了\nFlume架构\nFlume组成架构如图1-1，所示：\n图1-1 Flume组成架构\nAgent\nAgent是一个JVM进程，它以事件的形式将数据从源头送至目的，是Flume数据传输的基本单元。\nAgent主要有3个部分组成，Source、Channel、Sink。\nSource\nSource是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。\nChannel\nChannel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。\nFlume自带两种Channel：Memory Channel和File Channel\nMemory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。\nFile Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。\nSink\nSink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。\nSink是完全事务性的。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。\nSink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。\nEvent\n传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。\nFlume拓扑结构\nFlume的拓扑结构如图1-3、1-4、1-5和1-6所示：\n图1-3 Flume Agent连接\n图1-4 单source，多channel、sink\n图1-5 Flume负载均衡\n图1-6 Flume Agent聚合\nFlume安装部署 Flume的安装相对简单，但是前提是要先下好Java环境JDK，1.8以上即可，JDK安装可以查看Kafka安装流程，这里以Linux下的安装为例\nFlume安装地址\n Flume官网地址 文档查看地址 下载地址  安装部署\n 解压apache-flume-1.7.0-bin.tar.gz到/usr/local/目录下(安装包详见安装包文件夹flume文件夹下的tar.gz压缩包)  #把下载的包移动到目录 $ sudo mv apache-flume-1.7.0-bin.tar.gz /usr/local #解压 $ sudo tar -zxvf apache-flume-1.7.0-bin.tar.gz /usr/local/  修改apache-flume-1.7.0-bin的名称为flume  $ sudo mv apache-flume-1.7.0-bin flume  将flume/conf下的flume-env.sh.template文件修改为flume-env.sh，并配置flume-env.sh文件  $ mv flume-env.sh.template flume-env.sh $ vi flume-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144(这里路径替换为本机JDK安装目录) 案例实操  监控端口数据  案例需求：首先，Flume监控本机44444端口，然后通过telnet工具向本机44444端口发送消息，最后Flume将监听的数据实时显示在控制台。 需求分析：      实现步骤：\n  安装telnet工具在/usr/local目录下创建flume-telnet文件夹。\n$ mkdir flume-telnet 再将rpm软件包(xinetd-2.3.14-40.el6.x86_64.rpm、telnet-0.17-48.el6.x86_64.rpm和telnet-server-0.17-48.el6.x86_64.rpm)拷入/usr/local/flume-telnet文件夹下面。执行RPM软件包安装命令：\n$ sudo rpm -ivh xinetd-2.3.14-40.el6.x86_64.rpm $ sudo rpm -ivh telnet-0.17-48.el6.x86_64.rpm $ sudo rpm -ivh telnet-server-0.17-48.el6.x86_64.rpm     判断44444端口是否被占用 判断44444端口是否占用，如果被占用则kill掉或者更换端口\n$ sudo netstat -tunlp | grep 44444 功能描述：netstat命令是一个监控TCP/IP网络的非常有用的工具，它可以显示路由表、实际的网络连接以及每一个网络接口设备的状态信息。 基本语法：netstat [选项] 选项参数： -t或–tcp：显示TCP传输协议的连线状况； -u或–udp：显示UDP传输协议的连线状况； -n或--numeric：直接使用ip地址，而不通过域名服务器； -l或--listening：显示监控中的服务器的Socket； -p或--programs：显示正在使用Socket的程序识别码和程序名称；\n``\n  创建Flume Agent配置文件\nflume-telnet-logger.conf 在flume目录下创建job文件夹并进入job文件夹\n$ mkdir job $ cd job/   在job文件夹下创建Flume Agent配置文件\nflume-telnet-logger.conf $ touch flume-telnet-logger.conf # 如果觉得vim上手难度太大，可以使用gedit来进行编辑 $ vim flume-telnet-logger.conf # 在conf文件中加入以下内容 # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 # Describe the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1   注：配置文件来源于官方手册\n  先开启flume监听端口\n$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-telnet-logger.conf -Dflume.root.logger=INFO,console 参数说明： --conf conf/ ：表示配置文件存储在conf/目录 --name a1 ：表示给agent起名为a1 --conf-file job/flume-telnet.conf ：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。 -Dflume.root.logger==INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。\n``\n  使用telnet工具向本机的44444端口发送内容\n$ telnet localhost 44444   将A服务器上的日志实时采集到B服务器一般跨节点都是使用\navro sink 技术选型有两种方案:\n  exec source + memory channel + avro sink\n// Flume的关键就是写配置文件，仍然是在conf文件夹下创建配置文件 // avro-memory-sink.conf Name the components on this agentexec-memory-avro.sources = exec-source exec-memory-avro.sinks = arvo-sink exec-memory-avro.channels = memory-channel Describe/configure the sourceexec-memory-avro.sources.exec-source.type = exec exec-memory-avro.sources.exec-source.command = tail -F $FLUME_HOME/logs/flume.log exec-memory-avro.sources.exec-source.shell = /bin/sh -c Describe the sinkexec-memory-avro.sinks.arvo-sink.type = avro exec-memory-avro.sinks.arvo-sink.hostname = localhost exec-memory-avro.sinks.arvo-sink.port = 44444 Use a channel which buffers events in memoryexec-memory-avro.channels.memory-channel.type = memory exec-memory-avro.channels.memory-channel.capacity = 1000 exec-memory-avro.channels.memory-channel.transactionCapacity = 100 Bind the source and sink to the channelexec-memory-avro.sources.exec-source.channels = memory-channel exec-memory-avro.sinks.arvo-sink.channel = memory-channel   avro source + memory channel + logger sink\n// avro-logger-sink.conf # Name the components on this agent avro-memory-logger.sources = avro-source avro-memory-logger.sinks = logger-sink avro-memory-logger.channels = memory-channel Describe/configure the sourceavro-memory-logger.sources.avro-source.type = avro avro-memory-logger.sources.avro-source.bind = localhost avro-memory-logger.sources.avro-source.port = 44444 Describe the sinkavro-memory-logger.sinks.logger-sink.type = logger Use a channel which buffers events in memoryavro-memory-logger.channels.memory-channel.type = memory avro-memory-logger.channels.memory-channel.capacity = 1000 avro-memory-logger.channels.memory-channel.transactionCapacity = 100 Bind the source and sink to the channelavro-memory-logger.sources.avro-source.channels = memory-channel avro-memory-logger.sinks.logger-sink.channel = memory-channel   接下来启动两个配置\n先启动avro-memory-logger flume-ng agent \\ –name avro-memory-logger \\ –conf $FLUME_HOME/conf \\ –conf-file $FLUME_HOME/conf/avro-memory-logger.conf \\ -Dflume.root.logger=INFO,console 再启动另外一个 flume-ng agent –name exec-memory-avro –conf $FLUME_HOME/conf \\ –conf-file $FLUME_HOME/conf/exec-memory-avro.conf \\ -Dflume.root.logger=INFO,console    一个可能因为手误出现的bug\n log4j:WARN No appenders could be found for logger (org.apache.flume.lifecycle.LifecycleSupervisor). log4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n 出现这个错误是因为路径没有写对\n 往监听的日志中输入一段字符串，可以看到我们的logger sink 已经成功接收到信息\n上面Flume的基本流程图如下\nKafka Kafka是由Apache软件基金会开发的一个开源流处理平台，由Scala和Java编写。该项目的目标是为处理实时数据提供一个统一、高吞吐、低延迟的平台。其持久化层本质上是一个“按照分布式事务日志架构的大规模发布/订阅消息队列”，[3]这使它作为企业级基础设施来处理流式数据非常有价值。此外，Kafka可以通过Kafka Connect连接到外部系统（用于数据输入/输出），并提供了Kafka Streams——一个Java流式处理库。\n具体的架构可以查看官网的intro部分。\n因为在实际编程中使用kafka_2.11-0.11.00以上版本和使用以下版本的Java API 不一致，所以推荐直接参照官网的文档进行编程。\n环境搭建 单机单节点\n搭建说明\n需要有一定的Linux操作经验，对于没有权限之类的问题要懂得通过命令解决\nKafka的安装相比Flume来说更加复杂，因为Kafka依赖于Zookeeper\n环境说明：\n os：Ubuntu 18.04 zookeeper：zookeeper 3.4.9 kafka：kafka_2.11-0.11.0.0 jdk：jdk 8（kafka启动需要使用到jdk）  详细说明：\n一、jdk安装\njdk分为以下几种：jre、openjdk、 oracle jdk，这里我们要安装的是oracle jdk（推荐安装）\nadd-apt-repository ppa:webupd8team/java apt-get update apt-get install oracle-java8-installer apt-get install oracle-java8-set-default 测试安装版本：\n二、安装配置zookeeper单机模式\n下载zookeeper 3.4.5，开始安装（软件包详见软件包下的kafka中的压缩包）：\ncd /usr/local wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.5/zookeeper-3.4.5.tar.gz 等待安装成功：\n解压：\ntar -zxvf zookeeper-3.4.5.tar.gz 解压后同目录下便存在相同文件夹：\n切换到conf目录下：\ncd zookeeper-3.4.5/conf/ 复制zoo_sample.cfg到zoo.cfg：\ncp zoo_sample.cfg zoo.cfg 然后编辑zoo.cfg如下（其它不用管，默认即可）：\ninitLimit=10 syncLimit=5 dataDir=/home/young/zookeeper/data clientPort=2181 别忘了新建dataDir目录：\nmkdir /home/young/zookeeper/data 为zookeeper创建环境变量，打开/etc/profile文件，并在最末尾添加如下内容：\nvi /etc/profile 添加内容如下：\nexport ZOOKEEPER_HOME=/home/young/zookeeper export PATH=.:$ZOOKEEPER_HOME/bin:$JAVA_HOME/bin:$PATH 配置完成之后，切换到zookeeper/bin目录下，启动服务：\n关闭服务：\n这里暂时先关闭zookeeper服务，防止下面使用kafka启动时报端口占用错误。\n三、安装配置kafka单机模式\n下载kafka（安装包详见软件包kafka下的压缩包）：\ncd /usr/local wget https://www.apache.org/dyn/closer.cgi?path=/kafka/0.11.0.0/kafka_2.11-0.11.0.0.tgz 解压：\ntar -zxvf kafka_2.11-0.11.0.0.tgz 进入kafka/config目录下：\n以上文件是需要修改的文件，下面一个个修改配置：\n配置server.properties：\n以下为修改的，其他为默认即可：\n#broker.id需改成正整数，单机为1就好 broker.id=1 #指定端口号 port=9092 #localhost这一项还有其他要修改，详细见下面说明 host.name=localhost #指定kafka的日志目录 log.dirs=/usr/local/kafka_2.11-0.11.0.0/kafka-logs #连接zookeeper配置项，这里指定的是单机，所以只需要配置localhost，若是实际生产环境，需要在这里添加其他ip地址和端口号 zookeeper.connect=localhost:2181 配置zookeeper.properties：\n#数据目录 dataDir=/usr/local/kafka_2.11-0.11.0.0/zookeeper/data #客户端端口 clientPort=2181 host.name=localhost 配置producer.properties：\nzookeeper.connect=localhost:2181 配置consumer.properties：\nzookeeper.connect=localhost:2181 最后还需要拷贝几个jar文件到kafka的libs目录，分别是zookeeper-xxxx.jar、log4j-xxxx.jar、slf4j-simple-xxxx.jar，最后如下：\n四、kafka的使用\n启动zookeeper服务：\nbin/zookeeper-server-start.sh config/zookeeper.properties 新开一个窗口启动kafka服务：\nbin/kafka-server-start.sh config/server.properties 至此单机服务搭建已经全部完成\n单机多节点\n对于单机单节点只需要使用一个配置文件来启动即可，那么对于单机多节点，只需要建立多个配置文件，并且启动即可。比如我们需要有三个节点。\n然后我们的每个server properies里面的端口以及ID要不一致\nserver-1.properties\nserver-2.properties\nserver-3.properties\n当然其对应的log对应目录也要修改，这个就不多说了\n然后在控制台启动\n bin/kafka-server-start.sh config/server-1.properties \u0026  bin/kafka-server-start.sh config/server-2.properties \u0026  bin/kafka-server-start.sh config/server-3.properties \u0026 通过jps -m 能看到三个kafka即可（可能以普通用户看不到相应的进程，只是因为没给到权限，可以给权限或者直接sudo su切换到超级用户）\nKafka控制台的一些命令操作 控制台中我们可以通过命令建立topic，并且开启一个消费者一个生产者来模拟通信，这些在官网的quickstart中都有详尽的描述\n通过我们的一个叫topic的标签，我们建立了一个生产者和一个消费者，可以明显看到消费者接收到了生产者的消息。其他比较常用的命令，比如describe等可以自行探索。\nJava API控制Kafka 接下来会说一个简单的在Java中使用Kafka小例子\n这里都是基于2.11_0.11.0.0.0版本以及之后的编程来说明，更低版本相应的API有些许变化，低版本中很多函数已经被替代和废除。\n基本配置\n 首先在Idea中建立一个新的Maven项目，这里我们选择一个achetype:scala-archetype-simple    接下来我们把Maven文件配置好，并且auto import dependencies，这里如果没有选择auto import，我们可以在Pom.xml右键找到maven选项里面有一个reload\nproject xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\" modelVersion4.0.0modelVersion groupIdcom.test.sparkgroupId artifactIdspark streamingartifactId version1.0version inceptionYear2008inceptionYear properties scala.version2.7.0scala.version kafka.version0.11.0.0kafka.version properties dependencies dependency groupIdorg.scala-langgroupId artifactIdscala-libraryartifactId version${scala.version}version dependency dependency groupIdorg.apache.kafkagroupId artifactIdkafka_2.11artifactId version${kafka.version}version dependency dependencies build sourceDirectorysrc/main/scalasourceDirectory testSourceDirectorysrc/test/scalatestSourceDirectory plugins plugin groupIdorg.scala-toolsgroupId artifactIdmaven-scala-pluginartifactId executions execution goals goalcompilegoal goaltestCompilegoal goals execution executions configuration scalaVersion${scala.version}scalaVersion args arg-target:jvm-1.5arg args configuration plugin plugin groupIdorg.apache.maven.pluginsgroupId artifactIdmaven-eclipse-pluginartifactId configuration downloadSourcestruedownloadSources buildcommands buildcommandch.epfl.lamp.sdt.core.scalabuilderbuildcommand buildcommands additionalProjectnatures projectnaturech.epfl.lamp.sdt.core.scalanatureprojectnature additionalProjectnatures classpathContainers classpathContainerorg.eclipse.jdt.launching.JRE_CONTAINERclasspathContainer classpathContainerch.epfl.lamp.sdt.launching.SCALA_CONTAINERclasspathContainer classpathContainers configuration plugin plugins build reporting plugins plugin groupIdorg.scala-toolsgroupId artifactIdmaven-scala-pluginartifactId configuration scalaVersion${scala.version}scalaVersion configuration plugin plugins reporting project  因为我们使用Java编程，所以我们在main下面建立一个java文件夹，并且把整个文件夹设为source，如下图    然后我们在这个例子会涉及到几个Class，包括启动的Class，消费者，生产者，配置\n  代码分析\n//KafkaProperties.java  package com.test.spark.kafka; /** Kafka常用配置文件 / public class KafkaProperties { public static final String ZK= “211.83.96.204:2181”; public static final String TOPIC= “test”; public static final String BROKER_LIST = “211.83.96.204:9092”; public static final String GROUP_ID = “test_group1”; } 首先看一下配置文件，为了配置能更加全局化好修改，我们直接建立一个配置文件，把可能需要的一些全局参数放进来，方便后续开发。其中有zookeeper的IP，Topic名称，服务器列表以及group_id。\n// KafkaProducerClient.java package com.test.spark.kafka; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerRecord; import java.util.Properties; /** Kafka 生产者 / public class KafkaProducerClient extends Thread { private String topic; private ProducerString, String producer; public KafkaProducerClient(String topic) { this.topic = topic; Properties properties = new Properties(); properties.put(\"bootstrap.servers\",\"localhost:9092\"); //properties.put(“serializer.class”,”kafka.serializer.StringEncoder”);  properties.put(\"key.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\"); properties.put(\"value.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\"); properties.put(\"request.required.acks\",\"1\"); producer = new KafkaProducerString, String(properties); } @Override public void run() { int messageNo = 1; while(true) { String message = \"message_\" + messageNo; producer.send(new ProducerRecordString, String(topic, message)); System.out.println(\"Sent: \" + message); messageNo ++; try { Thread.sleep(2000); } catch (Exception e) { e.printStackTrace(); } } } } 消费者中我们使用多线程的方式，循环发送消息\n// KafkaConsumerClient.java package com.test.spark.kafka; import kafka.consumer.ConsumerConnector$class; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import org.apache.kafka.common.TopicPartition; import java.util.Arrays; import java.util.List; import java.util.Properties; /** Kafka消费者 */ public class KafkaConsumerClient { private String topic; public KafkaConsumerClient(String topic) { this.topic = topic; } public void start() { Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"group.id\", KafkaProperties.GROUP_ID);//不同ID 可以同时订阅消息  props.put(\"enable.auto.commit\", \"false\");//自动commit  props.put(\"auto.commit.interval.ms\", \"1000\");//定时commit的周期  props.put(\"session.timeout.ms\", \"30000\");//consumer活性超时时间  props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); KafkaConsumerString, String consumer = new KafkaConsumerString, String(props); consumer.subscribe(Arrays.asList(this.topic));//订阅TOPIC  try { while(true) {//轮询ConsumerRecordsString  String records =consumer.poll(Long.MAX_VALUE);//超时等待时间  for (TopicPartition partition : records.partitions()) { ListConsumerRecordString, String partitionRecords = records.records(partition); for (ConsumerRecordString, String record : partitionRecords) { System.out.println(\"receive\" + \": \" + record.value()); } consumer.commitSync();//同步  } } } finally { consumer.close(); } } } 在消费中我们会轮询消息\nFlume+Kafka配合 把logger sink === kafka sink\nsink kafka: producer\n所以启动一个kafka的consumer，直接对接到kafka sink消费掉即可\n//avro-memory-kafka.conf Name the components on this agentavro-memory-kafka.sources = avro-source avro-memory-kafka.sinks = kafka-sink avro-memory-kafka.channels = memory-channel Describe/configure the sourceavro-memory-kafka.sources.avro-source.type = avro avro-memory-kafka.sources.avro-source.bind = localhost avro-memory-kafka.sources.avro-source.port = 44444 Describe the sinkavro-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink avro-memory-kafka.sinks.kafka-sink.brokerList = localhost:9092 avro-memory-kafka.sinks.kafka-sink.topic = test avro-memory-kafka.sinks.kafka-sink.batchSize = 5 avro-memory-kafka.sinks.kafka-sink.requiredAcks = 1 Use a channel which buffers events in memoryavro-memory-kafka.channels.memory-channel.type = memory avro-memory-kafka.channels.memory-channel.capacity = 1000 avro-memory-kafka.channels.memory-channel.transactionCapacity = 100 Bind the source and sink to the channelavro-memory-kafka.sources.avro-source.channels = memory-channel avro-memory-kafka.sinks.kafka-sink.channel = memory-channel 注意这个batchSize,在数据量没有到达设定的阈值时，他会有一个timeout,这之后才会有数据发过来\nSpark Spark 简介  什么是Spark？Spark作为Apache顶级的开源项目，是一个快速、通用的大规模数据处理引擎，和Hadoop的MapReduce计算框架类似，但是相对于MapReduce，Spark凭借其可伸缩、基于内存计算等特点，以及可以直接读写Hadoop上任何格式数据的优势，进行批处理时更加高效，并有更低的延迟。相对于“one stack to rule them all”的目标，实际上，Spark已经成为轻量级大数据快速处理的统一平台，各种不同的应用，如实时流处理、机器学习、交互式查询等，都可以通过Spark建立在不同的存储和运行系统上。 Spark是基于内存计算的大数据并行计算框架。Spark基于内存计算，提高了在大数据环境下数据处理的实时性，同时保证了高容错性和高可伸缩性，允许用户将Spark部署在大量廉价硬件之上，形成集群。 Spark于2009年诞生于加州大学伯克利分校AMPLab。目前，已经成为Apache软件基金会旗下的顶级开源项目。相对于MapReduce上的批量计算、迭代型计算以及基于Hive的SQL查询，Spark可以带来上百倍的性能提升。目前Spark的生态系统日趋完善，Spark SQL的发布、Hive on Spark项目的启动以及大量大数据公司对Spark全栈的支持，让Spark的数据分析范式更加丰富。  Spark环境搭建 Hadoop安装（Spark依赖于Hadoop安装）\n参考链接\nHadoop可以通过HadoopDownloadOne 或者HadoopDownloadTwo 下载，一般选择下载最新的稳定版本，即下载 “stable” 下的hadoop-2.x.y.tar.gz 这个格式的文件(详见安装文件夹中的hadoop-2.7.7)\n$ sudo tar -zxf hadoop-2.7.7.tar.gz -C /usr/local # 解压到/usr/local中 $ cd /usr/local/ $ sudo mv ./hadoop-2.6.0/ ./hadoop # 将文件夹名改为hadoop $ sudo chown -R hadoop ./hadoop # 修改文件权限 Hadoop 解压后即可使用。输入如下命令来检查 Hadoop 是否可用，成功则会显示 Hadoop 版本信息：\n$ cd /usr/local/hadoop $ ./bin/hadoop version Hadoop单机配置及运行测试\nHadoop 默认模式为非分布式模式（本地模式），无需进行其他配置即可运行。非分布式即单 Java 进程，方便进行调试。\n现在我们可以执行例子来感受下 Hadoop 的运行。Hadoop 附带了丰富的例子（运行 ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar 可以看到所有例子），包括 wordcount、terasort、join、grep 等。\n$ cd /usr/local/hadoop $ mkdir ./input $ cp ./etc/hadoop/*.xml ./input # 将配置文件作为输入文件 $ ./bin/Hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep ./input ./output 'dfs[a-z.]+' $ cat ./output/* # 查看运行结果  注意，Hadoop 默认不会覆盖结果文件，因此再次运行上面实例会提示出错，需要先将 ./output 删除。\n如果中间提示 Error: JAVA_HOME is not set and could not be found. 的错误，则说明之前设置 JAVA_HOME 环境变量那边就没设置好，请按教程先设置好 JAVA_HOME 变量，否则后面的过程都是进行不下去的。如果已经按照前面教程在.bashrc文件中设置了JAVA_HOME，还是出现 Error: JAVA_HOME is not set and could not be found. 的错误，那么，请到hadoop的安装目录修改配置文件“/usr/local/hadoop/etc/hadoop/hadoop-env.sh”，在里面找到“export JAVA_HOME=${JAVA_HOME}”这行，然后，把它修改成JAVA安装路径的具体地址，比如，“export JAVA_HOME=/usr/lib/jvm/default-java”，然后，再次启动Hadoop。\n Spark安装\n此处采用Spark和Hadoop一起安装使用，这样，就可以让Spark使用HDFS存取数据。需要说明的是，当安装好Spark以后，里面就自带了scala环境，不需要额外安装scala。在安装spark之前，需要先安装Java和Hadoop。\n需要的具体运行环境如下：\nØ Ubuntu16.04以上\nØ Hadoop 2.7.1以上\nØ Java JDK 1.8以上\nØ Spark 2.1.0 以上\nØ Python 3.4以上\n（此次系统环境使用的Ubuntu16.04，自带Python，不需额外安装）\nSpark官网下载\n由于已经安装了Hadoop，所以在Choose a package type后面需要选择Pre-build with user-provided Hadoop，然后点击Download Spark后面的spark-2.1.0-bin-without-hadoop.tgz下载即可。需要说明的是，Pre-build with user-provided Hadoop:属于“Hadoop free”版，这样下载到的Spark，可应用到任意Hadoop版本。\nSpark部署模式主要有四种：Local模式（单机模式）、Standalone模式（使用Spark自带的简单集群管理器）、YARN模式（使用YARN作为集群管理器）和Mesos模式（使用Mesos作为集群管理器）。\n这里介绍Local模式（单机模式）的 Spark安装。我们选择Spark 2.4.3版本，并且假设当前使用用户名hadoop登录了Linux操作系统。\n$ sudo tar -zxf ~/下载/spark-2.4.3-bin-without-hadoop.tgz -C/usr/local/ $ sudo mv ./spark-2.4.3-bin-without-hadoop/ ./spark $ sudo chown -R hadoop:hadoop ./spark # 此处的 hadoop 为你的用户名 安装后，还需要修改Spark的配置文件spark-env.sh\n$ cd /usr/local/spark $ sudo cp conf/spark-env.sh.template conf/spark-env.sh $ sudo vim conf/spark-env.sh #添加下面的环境变量信息 export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop:classpath) 有了上面的配置信息以后，Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。如果没有配置上面信息，Spark就只能读写本地数据，无法读写HDFS数据。\n配置完成后就可以直接使用，不需要像Hadoop运行启动命令。通过运行Spark自带的示例，验证Spark是否安装成功。\n$ cd /usr/local/spark $ bin/run-example SparkPi 过滤后的运行结果如下图示，可以得到π 的 5 位小数近似值：\nSpark不依赖Hadoop安装\nSpark同样也可以不依赖hadoop进行安装，但是仍然需要JDK环境，同样是在Spark官网上，选择spark-2.4.3-bin-hadoop2.7.tgz。我们直接将其解压出来，下面我们开始配置环境变量。我们进入编辑/etc/profile，在最后加上如下代码。\n#Spark export SPARK_HOME=/opt/spark-2.4.3 export PATH=$PATH:$SPARK_HOME/bin 然后进入/spark-2.3.1/bin目录下即可直接运行spark-shell。\n下面配置本地集群环境，首先我们进入刚刚解压的Spark目录，进入/spark-2.2.1/conf/，拷贝一份spark-env.sh。\n$ cp spark-env.sh.template spark-env.sh 然后我们编辑这个文件，添加如下环境设置（按自身环境修改）\n#export SCALA_HOME=/opt/scala-2.13.0 export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.181-3.b13.el7_5.x86_64 #这里是你jdk的安装路径 export SPARK_HOME=/opt/spark-2.4.3 export SPARK_MASTER_IP=XXX.XX.XX.XXX #将这里的xxx改为自己的Linux的ip地址 #export SPARK_EXECUTOR_MEMORY=512M #export SPARK_WORKER_MEMORY=1G #export master=spark://XXX.XX.XX.XXX:7070 再回到conf目录下，拷贝一份slaves。\n$ cp slaves.template slaves 在slaves最后加上localhost，保存即可。最后想要启动spark，进入安装目录下的sbin文件夹下，运行start-all.sh输入登录密码，master和worker进程就能按照配置文件启动。\n在Spark Shell 中运行代码 这里介绍Spark Shell的基本使用。Spark shell提供了简单的方式来学习API，并且提供了交互的方式来分析数据。它属于REPL（Read-Eval-Print Loop，交互式解释器），提供了交互式执行环境，表达式计算完成就会输出结果，而不必等到整个程序运行完毕，因此可即时查看中间结果，并对程序进行修改，这样可以在很大程度上提升开发效率。\nSpark Shell支持Scala和Python，本文使用 Scala 来进行介绍。前面已经安装了Hadoop和Spark，如果Spark不使用HDFS和YARN，那么就不用启动Hadoop也可以正常使用Spark。如果在使用Spark的过程中需要用到 HDFS，就要首先启动 Hadoop。\n这里假设不需要用到HDFS，因此，就没有启动Hadoop。现在直接开始使用Spark。Spark-shell命令及其常用的参数如下：\n$ ./bin/spark-shell —master Spark的运行模式取决于传递给SparkContext的Master URL的值。Master URL可以是以下任一种形式：\n local 使用一个Worker线程本地化运行SPARK(完全不并行) local[*] 使用逻辑CPU个数数量的线程来本地化运行Spark local[K] 使用K个Worker线程本地化运行Spark（理想情况下，K应该根据运行机器的CPU核数设定） spark://HOST:PORT 连接到指定的Spark standalone master。默认端口是7077. yarn-client 以客户端模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR 环境变量中找到。 yarn-cluster 以集群模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR 环境变量中找到。 mesos://HOST:PORT 连接到指定的Mesos集群。默认接口是5050。  需要强调的是，本文采用“本地模式”（local）运行Spark，关于如何在集群模式下运行Spark，之后的文章会着重介绍。\n在Spark中采用本地模式启动Spark Shell的命令主要包含以下参数：\n–master：这个参数表示当前的Spark Shell要连接到哪个master，如果是local[*]，就是使用本地模式启动spark-shell，其中，中括号内的星号表示需要使用几个CPU核心(core)；\n–jars： 这个参数用于把相关的JAR包添加到CLASSPATH中；如果有多个jar包，可以使用逗号分隔符连接它们；\n比如，要采用本地模式，在4个CPU核心上运行spark-shell：\n$ cd /usr/local/spark $ /bin/spark-shell —master local[4] 或者，可以在CLASSPATH中添加code.jar，命令如下：\n$ cd /usr/local/spark $ ./bin/spark-shell -master local[4] --jars code.jar 可以执行spark-shell –help命令，获取完整的选项列表，具体如下：\n$ cd /usr/local/spark $ ./bin/spark-shell —help [外链图片转存失败(img-36hzRWo9-1569486879033)(spark-shell.png)]\n上面是命令使用方法介绍，下面正式使用命令进入spark-shell环境，可以通过下面命令启动spark-shell环境：\nscala 8*2+5 res0: Int = 21 最后，可以使用命令“:quit”退出Spark Shell，如下所示：\nscala:quit 或者，也可以直接使用“Ctrl+D”组合键，退出Spark Shell\nScala编写wordCount 任务需求\n学会了上文基本的安装和执行后，现在练习一个任务：编写一个Spark应用程序，对某个文件中的单词进行词频统计。\n准备工作：进入Linux系统，打开“终端”，进入Shell命令提示符状态，然后，执行如下命令新建目录：\n$ cd /usr/local/spark $ mkdir mycode $ cd mycode $ mkdir wordcount $ cd wordcount 然后，在/usr/local/spark/mycode/wordcount目录下新建一个包含了一些语句的文本文件word.txt，命令如下：\n$ vim word.txt 首先可以在文本文件中随意输入一些单词，用空格隔开，编写Spark程序对该文件进行单词词频统计。然后，按键盘Esc键退出vim编辑状态，输入“:wq”保存文件并退出vim编辑器。\n在Spark-Shell中执行词频统计  启动Spark-Shell首先，登录Linux系统(要注意记住登录采用的用户名，本教程统一采用hadoop用户名进行登录)，打开“终端”（可以在Linux系统中使用Ctrl+Alt+T组合键开启终端），进入shell命令提示符状态，然后执行以下命令进入spark-shell：  $ cd /usr/local/spark $ ./bin_spark-shell $ …这里省略启动过程显示的一大堆信息 $ scala 启动进入spark-shell需要一点时间，在进入spark-shell后，我们可能还需要到Linux文件系统中对相关目录下的文件进行编辑和操作（比如要查看spark程序执行过程生成的文件），这个无法在park-shell中完成，因此，这里再打开第二个终端，用来在Linux系统的Shell命令提示符下操作。\n  加载本地文件在开始具体词频统计代码之前，需要考虑如何加载文件，文件可能位于本地文件系统中，也有可能存放在分布式文件系统HDFS中，下面先介绍介绍如何加载本地文件，以及如何加载HDFS中的文件。首先，请在第二个终端窗口下操作，用下面命令到达\n/usr/local/spark/mycode/wordcount 目录，查看一下上面已经建好的word.txt的内容：\n$ cd /usr/local/spark/mycode/wordcount $ cat word.txt Cat命令会把word.txt文件的内容全部显示到屏幕上。\n现在切换回spark-shell，然后输入下面命令：\nscala val textFile = sc.textFile(“file:///usr/local/spark/mycode/wordcount/word.txt”) 上面代码中，val后面的是变量textFile，而sc.textFile()中的这个textFile是sc的一个方法名称，这个方法用来加载文件数据。这两个textFile不是一个东西，不要混淆。实际上，val后面的是变量textFile，你完全可以换个变量名称，比如,val lines = sc.textFile(“file:///usr/local/spark/mycode/wordcount/word.txt”)。这里使用相同名称，就是有意强调二者的区别。\n注意要加载本地文件，必须采用“file:///”开头的这种格式。执行上上面这条命令以后，并不会马上显示结果，因为Spark采用惰性机制，只有遇到“行动”类型的操作，才会从头到尾执行所有操作。所以，下面我们执行一条“行动”类型的语句，就可以看到结果：\nscalatextFile.first() first()是一个“行动”（Action）类型的操作，会启动真正的计算过程，从文件中加载数据到变量textFile中，并取出第一行文本。屏幕上会显示很多反馈信息，这里不再给出，你可以从这些结果信息中，找到word.txt文件中的第一行的内容。\n正因为Spark采用了惰性机制，在执行转换操作的时候，即使我们输入了错误的语句，spark-shell也不会马上报错，而是等到执行“行动”类型的语句时启动真正的计算，那个时候“转换”操作语句中的错误就会显示出来，比如：\nval textFile = sc.textFile(“file:///usr/local/spark/mycode/wordcount/word123.txt”) 上面我们使用了一个根本就不存在的word123.txt，执行上面语句时，spark-shell根本不会报错，因为，没有遇到“行动”类型的first()操作之前，这个加载操作时不会真正执行的。然后，我们执行一个“行动”类型的操作first()，如下：\nscala textFile.first() 执行上面语句后，会返回错误信息“拒绝连接”，因为这个word123.txt文件根本就不存在。现在我们可以练习一下如何把textFile变量中的内容再次写回到另外一个文本文件wordback.txt中：\nval textFile = sc.textFile(“file:///usr/local/spark/mycode/wordcount/word.txt”) textFile.saveAsTextFile(“file:///usr/local/spark/mycode/wordcount/writeback”) 上面的saveAsTextFile()括号里面的参数是保存文件的路径，不是文件名。saveAsTextFile()是一个“行动”（Action）类型的操作，所以马上会执行真正的计算过程，从word.txt中加载数据到变量textFile中，然后，又把textFile中的数据写回到本地文件目录“_usr_local_spark_mycode_wordcount_writeback/”下面，现在让我们切换到Linux Shell命令提示符窗口中，执行下面命令：\n$ cd /usr/local/spark/mycode/wordcount/writeback/ $ ls 执行结果会显示，有两个文件part-00000和_SUCCESS，我们可以使用cat命令查看一下part-00000文件，会发现结果是和上面word.txt中的内容一样的。\n词频统计 有了前面的铺垫性介绍，下面我们开始第一个Spark应用程序：WordCount。请切换到spark-shell窗口，输入如下命令：\nscala val textFile = sc.textFile(“file:///usr/local/spark/mycode/wordcount/word.txt”) scala val wordCount = textFile.flatMap(line = line.split(“ “)).map(word = (word, 1)).reduceByKey((a, b) = a + b) scala wordCount.collect() 上面只给出了代码，省略了执行过程中返回的结果信息，因为返回信息很多。下面简单解释一下上面的语句。\n textFile包含了多行文本内容，textFile.flatMap(line = line.split(” “))会遍历textFile中的每行文本内容，当遍历到其中一行文本内容时，会把文本内容赋值给变量line，并执行Lamda表达式line = line.split(” “)。line = line.split(” “)是一个Lamda表达式，左边表示输入参数，右边表示函数里面执行的处理逻辑，这里执行line.split(” “)，也就是针对line中的一行文本内容，采用空格作为分隔符进行单词切分，从一行文本切分得到很多个单词构成的单词集合。这样，对于textFile中的每行文本，都会使用Lamda表达式得到一个单词集合，最终，多行文本，就得到多个单词集合。textFile.flatMap()操作就把这多个单词集合“拍扁”得到一个大的单词集合。 然后，针对这个大的单词集合，执行map()操作，也就是map(word = (word, 1))，这个map操作会遍历这个集合中的每个单词，当遍历到其中一个单词时，就把当前这个单词赋值给变量word，并执行Lamda表达式word = (word, 1)，这个Lamda表达式的含义是，word作为函数的输入参数，然后，执行函数处理逻辑，这里会执行(word, 1)，也就是针对输入的word，构建得到一个tuple，形式为(word,1)，key是word，value是1（表示该单词出现1次）。 程序执行到这里，已经得到一个RDD，这个RDD的每个元素是(key,value)形式的tuple。最后，针对这个RDD，执行reduceByKey((a, b) = a + b)操作，这个操作会把所有RDD元素按照key进行分组，然后使用给定的函数（这里就是Lamda表达式：(a, b) = a + b），对具有相同的key的多个value进行reduce操作，返回reduce后的(key,value)，比如(“hadoop”,1)和(“hadoop”,1)，具有相同的key，进行reduce以后就得到(“hadoop”,2)，这样就计算得到了这个单词的词频。    编写独立应用程序执行词频统计 在上面spark-shell编写wordcount后，下面我们编写一个Scala应用程序来实现词频统计。首先登录Linux系统，进入Shell命令提示符状态，然后执行下面命令：\n$ cd /usr/local/spark/mycode/wordcount/ $ mkdir -p src/main/scala 这里加入-p选项，可以一起创建src目录及其子目录 然后在“/usr/local/spark/mycode/wordcount/src/main/scala”目录下新建一个test.scala文件，里面包含如下代码：\nimport org.apache.spark.SparkContext import org.apache.spark.SparkContext./ import org.apache.spark.SparkConf object WordCount { def main(args: Array[String]) { val inputFile = “file:///usr/local/spark/mycode/wordcount/word.txt” val conf = new SparkConf().setAppName(“WordCount”).setMaster(“local[2]”) val sc = new SparkContext(conf) val textFile = sc.textFile(inputFile) val wordCount = textFile.flatMap(line = line.split(“ “)).map(word = (word, 1)).reduceByKey((a, b) = a + b) wordCount.foreach(println) } } 注意，SparkConf().setAppName(“WordCount”).setMaster(“local[2]”)这句语句，也可以删除.setMaster(“local[2]”)，只保留 val conf = new SparkConf().setAppName(“WordCount”)。\n如果test.scala没有调用SparkAPI，则只要使用scalac命令编译后执行即可。此处test.scala程序依赖 Spark API，因此需要通过 sbt 进行编译打包。首先执行如下命令：\n$ cd /usr/local/spark/mycode/wordcount/ $ vim simple.sbt 通过上面代码，新建一个simple.sbt文件，请在该文件中输入下面代码：\nname := “Simple Project” version := “1.0” scalaVersion := “2.11.8” libraryDependencies += “org.apache.spark” %% “spark-core” % “2.1.0” 下面我们使用sbt打包Scala程序。为保证sbt能正常运行，先执行如下命令检查整个应用程序的文件结构，应该是类似下面的文件结构：\n$ ./src $ ./src/main $ ./src/main/scala $ ./src/main/scala/test.scala $ ./simple.sbt $ ./word.txt 接着，我们就可以通过如下代码将整个应用程序打包成 JAR（首次运行同样需要下载依赖包 ）：\n$ cd /usr/local/spark/mycode/wordcount/ 请一定把这目录设置为当前目录 $ /usr/local/sbt/sbt package 上面执行过程需要消耗几分钟时间，屏幕上会返回一下信息：\nhadoop@dblab-VirtualBox:_usr_local_spark_mycode_wordcount$ /usr_local_sbt_sbt package OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256M; support was removed in 8.0 [info] Set current project to Simple Project (in build file:/usr_local_spark_mycode_wordcount/) [info] Updating {file:/usr_local_spark_mycode_wordcount/}wordcount… [info] Resolving jline#jline;2.12.1 … [info] Done updating. [info] Packaging _usr_local_spark_mycode_wordcount_target_scala-2.11_simple-project_2.11-1.0.jar … [info] Done packaging. [success] Total time: 34 s, completed 2017-2-20 10:13:13 若屏幕上返回上述信息表明打包成功，生成的 jar 包的位置为/usr/local/spark/mycode/wordcount/target/scala-2.11_simple-project_2.11-1.0.jar。\n最后通过spark-submit 运行程序。我们就可以将生成的jar包通过spark-submit提交到Spark中运行了，命令如下：\n$ /usr/local/spark/bin/spark-submit —class “WordCount” /usr/local/spark/mycode/wordcount/target/scala-2.11_simple-project_2.11-1.0.jar 最终得到的词频统计结果类似如下：\n(Spark,1) (is,1) (than,1) (fast,1) (love,2) (i,1) (I,1) (hadoop,2) Flume_Kafka_SparkStreaming实现词频统计 准备工作 在做这个project之前，需要预先准备好的环境如下：\n安装kafka（参考第一节）、安装flume（参考第二节）、安装Spark（参考第三节） 。\n做完上面三个工作之后，我们开始进入正式的词频统计Demo。\nSpark准备工作\n要通过Kafka连接Spark来进行Spark Streaming操作，Kafka和Flume等高级输入源，需要依赖独立的库（jar文件）。也就是说Spark需要jar包让Kafka和Spark streaming相连。按照我们前面安装好的Spark版本，这些jar包都不在里面，为了证明这一点，我们现在可以测试一下。请打开一个新的终端，输入以下命令启动spark-shell：\n$ cd /usr/local/spark $ ./bin/spark-shell 启动成功后，在spark-shell中执行下面import语句：\nimport org.apache.spark.streaming.kafka._\n程序报错，因为找不到相关jar包。根据Spark官网的说明，对于Spark版本，如果要使用Kafka，则需要下载spark-streaming-kafka相关jar包。Jar包下载地址（注意版本对应关系）。\n接下来需要把这个文件复制到Spark目录的jars目录下，输入以下命令：\n$ cd /usr/local/spark/jars $ mkdir kafka $ cp ./spark-streaming-kafka-0-8_2.11-2.1.0.jar /usr/local/spark/jars/kafka 下面把Kafka安装目录的libs目录下的所有jar文件复制到/usr/local/spark/jars/kafka目录下输入以下命令：至此，所有环境准备工作已全部完成，下面开始编写代码。\nProject 过程\n  编写Flume配置文件flume_to_kafka.conf输入命令：\n$ cd /usr/local/kafka/libs $ ls $ cp ./* /usr/local/spark/jars/kafka 内容如下：\na1.sources=r1 a1.channels=c1 a1.sinks=k1 #Describe/configure the source a1.sources.r1.type=netcat a1.sources.r1.bind=localhost a1.sources.r1.port=33333 #Describe the sink a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink a1.sinks.k1.kafka.topic=test a1.sinks.k1.kafka.bootstrap.servers=localhost:9092 a1.sinks.k1.kafka.producer.acks=1 a1.sinks.k1.flumeBatchSize=20 #Use a channel which buffers events in memory a1.channels.c1.type=memory a1.channels.c1.capacity=1000000 a1.channels.c1.transactionCapacity=1000000 #Bind the source and sink to the channel a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1   编写Spark Streaming程序(进行词频统计的程序)首先创建scala代码的目录结构，输入命令：\n$ cd /usr/local/spark/mycode $ mkdir flume_to_kafka $ cd flume_to_kafka $ mkdir -p src/main/scala $ cd src/main/scala $ vim KafkaWordCounter.scala KafkaWordCounter.scala是用于单词词频统计，它会把从kafka发送过来的单词进行词频统计，代码内容如下：\nreduceByKeyAndWindow函数作用解释如下：\npackage org.apache.spark.examples.streaming import org.apache.spark._ import org.apache.spark.SparkConf import org.apache.spark.streaming._ import org.apache.spark.streaming.kafka._ import org.apache.spark.streaming.StreamingContext._ import org.apache.spark.streaming.kafka.KafkaUtils object KafkaWordCounter{ def main(args:Array[String]){ StreamingExamples.setStreamingLogLevels() val sc=new SparkConf().setAppName(\"KafkaWordCounter\").setMaster(\"local[2]\") val ssc=new StreamingContext(sc,Seconds(10)) ssc.checkpoint(\"file:///usr/local/spark/mycode/flume_to_kafka/checkpoint\") //设置检查点 val zkQuorum=\"localhost:2181\" //Zookeeper服务器地址 val group=\"1\" //topic所在的group，可以设置为自己想要的名称，比如不用1，而是val group = \"test-consumer-group\" val topics=\"test\" //topics的名称 val numThreads=1 //每个topic的分区数 val topicMap=topics.split(\",\").map((_,numThreads.toInt)).toMap val lineMap=KafkaUtils.createStream(ssc,zkQuorum,group,topicMap) val lines=lineMap.map(_._2) val words=lines.flatMap(_.split(\" \")) val pair=words.map(x = (x,1)) val wordCounts=pair.reduceByKeyAndWindow(_ + _,_ - _,Minutes(2),Seconds(10),2) wordCounts.print ssc.start ssc.awaitTermination } }  reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]) 更加高效的reduceByKeyAndWindow，每个窗口的reduce值，是基于先前窗口的reduce值进行增量计算得到的；它会对进入滑动窗口的新数据进行reduce操作，并对离开窗口的老数据进行“逆向reduce”操作。但是，只能用于“可逆reduce函数”，即那些reduce函数都有一个对应的“逆向reduce函数”（以InvFunc参数传入）；\n此代码中就是一个窗口转换操作reduceByKeyAndWindow，其中，Minutes(2)是滑动窗口长度，Seconds(10)是滑动窗口时间间隔（每隔多长时间滑动一次窗口）。reduceByKeyAndWindow中就使用了加法和减法这两个reduce函数，加法和减法这两种reduce函数都是“可逆的reduce函数”，也就是说，当滑动窗口到达一个新的位置时，原来之前被窗口框住的部分数据离开了窗口，又有新的数据被窗口框住，但是，这时计算窗口内单词的词频时，不需要对当前窗口内的所有单词全部重新执行统计，而是只要把窗口内新增进来的元素，增量加入到统计结果中，把离开窗口的元素从统计结果中减去，这样，就大大提高了统计的效率。尤其对于窗口长度较大时，这种“逆函数”带来的效率的提高是很明显的。\n   创建StreamingExamples.scala继续在当前目录(/usr/local/spark/mycode/flume_to_kafka/src/main/scala)下创建StreamingExamples.scala代码文件，用于设置log4j，输入命令：\nvim StreamingExamples.scala\npackage org.apache.spark.examples.streaming import org.apache.spark.internal.Logging import org.apache.log4j.{Level, Logger} //Utility functions for Spark Streaming examples. object StreamingExamples extends Logging { //Set reasonable logging levels for streaming if the user has not configured log4j.  def setStreamingLogLevels() { val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements if (!log4jInitialized) { // We first log something to initialize Spark's default logging, then we override the  // logging level.  logInfo(\"Setting log level to [WARN] for streaming example.\" +\" To override add a custom log4j.properties to the classpath.\") Logger.getRootLogger.setLevel(Level.WARN) } } }   创建StreamingExamples.scala继续在当前目录(/usr/local/spark/mycode/flume_to_kafka/src/main/scala)下创建StreamingExamples.scala代码文件，用于设置log4j，输入命令：\nvim StreamingExamples.scala\npackage org.apache.spark.examples.streaming import org.apache.spark.internal.Logging import org.apache.log4j.{Level, Logger} //Utility functions for Spark Streaming examples. object StreamingExamples extends Logging { //Set reasonable logging levels for streaming if the user has not configured log4j.  def setStreamingLogLevels() { val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements if (!log4jInitialized) { // We first log something to initialize Spark's default logging, then we override the  // logging level.  logInfo(\"Setting log level to [WARN] for streaming example.\" +\" To override add a custom log4j.properties to the classpath.\") Logger.getRootLogger.setLevel(Level.WARN) } } }   打包文件simple.sbt输入命令：\n$ cd /usr/local/spark/mycode/flume_to_kafka $ vim simple.sbt 内容如下：\nname := \"Simple Project\" version := \"1.0\" scalaVersion := \"2.11.8\" libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.1.0\" libraryDependencies += \"org.apache.spark\" % \"spark-streaming_2.11\" % \"2.1.0\" libraryDependencies += \"org.apache.spark\" % \"spark-streaming-kafka-0-8_2.11\" % \"2.1.0\" 要注意版本号一定要设置正确,在/usr/local/spark/mycode/flume_to_kafka目录下输入命令：\n$ cd /usr/local/spark/mycode/flume_to_kafka $ find . 打包之前，这条命令用来查看代码结构，目录结构如下所示：\n   打包编译  一定要在/usr/local/spark/mycode/flume_to_kafka目录下运行打包命令。\n输入命令：\n$ cd /usr/local/spark/mycode/flume_to_kafka $ /usr/local/sbt/sbt package 第一次打包的过程可能会很慢，请耐心等待几分钟。打包成功后，会看到SUCCESS的提示。\n 启动zookeeper和kafka  #启动zookeeper： $ cd /usr/local/kafka $ ./bin/zookeeper-server-start.sh config/zookeeper.properties # 新开一个终端，启动Kafka： $ cd /usr/local/kafka $ bin/kafka-server-start.sh config/server.properties  运行程序KafkaWordCounter  打开一个新的终端，我们已经创建过topic，名为test（这是之前在flume_to_kafka.conf中设置的topic名字），端口号2181。在终端运行KafkaWordCounter程序，进行词频统计，由于现在没有启动输入，所以只有提示信息，没有结果。\n输入命令：\n$ cd /usr/local/spark $/usr/local/spark/bin/spark-submit --driver-class-path /usr/local/spark/jars/*:/usr/local/spark/jars/kafka/* --class \"org.apache.spark.examples.streaming.KafkaWordCounter\" /usr/local/spark/mycode/flume_to_kafka/target/scala-2.11/simple-project_2.11-1.0.jar  其中”/usr/local/spark/jars/“和”/usr/local/spark/jars/kafka/”用来指明引用的jar包，“org.apache.spark.examples.streaming.KafkaWordCounter”代表包名和类名，这是编写KafkaWordCounter.scala里面的包名和类名，最后一个参数用来说明打包文件的位置。\n 执行该命令后，屏幕上会显示程序运行的相关信息，并会每隔10秒钟刷新一次信息，用来输出词频统计的结果，此时还只有提示信息，如下所示：\n[外链图片转存失败(img-ZnMkMxIN-1569486879035)(result_one.png)]\n在启动Flume之前，Zookeeper和Kafka要先启动成功，不然启动Flume会报连不上Kafka的错误。\n 启动flume agent  打开第四个终端，在这个新的终端中启动Flume Agent\n输入命令：\n$ cd /usr/local/flume $ bin/flume-ng agent --conf ./conf --conf-file ./conf/flume_to_kafka.conf --name a1 -Dflume.root.logger=INFO,console 启动agent以后，该agent就会一直监听localhost的33333端口，这样，我们下面就可以通过“telnet localhost 33333”命令向Flume Source发送消息。这个终端也不要关闭，让它一直处于监听状态。\n 发送消息  打开第五个终端，发送消息。输入命令：\n$ telnet localhost 33333 这个端口33333是在flume conf文件中设置的source\n在这个窗口里面随便敲入若干个字符和若干个回车，这些消息都会被Flume监听到，Flume把消息采集到以后汇集到Sink，然后由Sink发送给Kafka的topic(test)。因为spark Streaming程序不断地在监控topic，在输入终端和前面运行词频统计程序那个终端窗口内看到统计结果。\n分布式环境搭建及相关DEMO Flume Flume在分布式环境下跟单机下一致，只需要在一台机器上搭建即可。\nKafka 搭建高吞吐量Kafka分布式发布订阅消息集群\n Zookeeper集群: 121.48.163.195:2181 , 113.54.154.68:2181，113.54.159.232:2181 kafka 集群: 121.48.163.195 , 113.54.154.68，113.54.159.232  搭建 kafka 集群\nkafka 集群: 121.48.163.195 , 113.54.154.68，113.54.159.232\n  下载kafka和zookeeper步骤和前面单机版一致\n  修改配置\n$ vim /usr/local/kafka_2.12-0.11.0.0/config/server.properties 设置broker.id 第一台为broker.id = 0 第二台为broker.id = 1 第三台为broker.id = 2 注意这个broker.id每台服务器不能重复 然后设置zookeeper的集群地址 zookeeper.connect=121.48.163.195:2181 , 113.54.154.68:2181，113.54.159.232:2181   修改zookeeper配置文件\n$ vim /usr/local/zookeeper-3.4.5/conf/zoo.cfg #添加server.1 server.2 server.3 server.1=121.48.163.195:2888:3888 server.2=113.54.154.68:2888:3888 server.3=113.54.159.232:2888:3888 #添加id $ sudo echo “1”  /usr/local/zookeeper-3.4.5/data/myid(每台机器的id可以和brokerid保持一致)   启动服务\n# 每台机器运行命令，但是在实际大型集群中可以使用脚本的方式一键启动 $ bin/kafka-server-start.sh config/server.properties \u0026   创建主题\n$ /usr/local/kafka_2.12-0.11.0.0/bin/kafka-topics.sh --create --zookeeper 121.48.163.195:2181 , 113.54.154.68:2181，113.54.159.232:2181 --replication-factor 2 --partitions 1 --topic ymq –replication-factor 2 #复制两份 –partitions 1 #创建1个分区 –topic #主题为ymq # 运行list topic命令，可以看到该主题： $ /usr/local/kafka_2.12-0.11.0.0/bin/kafka-topics.sh –list –zookeeper 121.48.163.195:2181 , 113.54.154.68:2181，113.54.159.232:2181   其它操作其它操作基本语法差不多一致，不再赘述，详情可以参考官网\n  Kafka ManagerYahoo开源Kafka集群管理器Kafka Manager\n  Spark   选取三台服务器\n 121.48.163.195 主节点 113.54.154.68 从节点 113.54.159.232 从节点  设置三台服务器root用户，之后操作都用root用户进行，便于管理\n  修改hosts文件\n$ sudo vim /etc/hosts # 在上面加上服务器ip 121.48.163.195 Master 113.54.154.68 Slave1 113.54.159.232 Slave2 修改完之后\nsource /etc/hosts   SSH无密码验证配置\n  安装和启动ssh协议我们需要两个服务：ssh和rsync。可以通过下面命令查看是否已经安装：\nrpm -qa|grep openssh rpm -qa|grep rsync 如果没有安装ssh和rsync，可以通过下面命令进行安装： sudo apt install ssh （安装ssh协议） sudo apt install rsync （rsync是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件） service sshd restart （启动服务）   配置Master无密码登录所有Slave配置Master节点，以下是在Master节点的配置操作。\n 在Master节点上生成密码对，在Master节点上执行以下命令：  ssh-keygen -t rsa -P ‘’\n生成的密钥对：id_rsa和id_rsa.pub，默认存储在”/root/.ssh”目录下。\n 接着在Master节点上做如下配置，把id_rsa.pub追加到授权的key里面去。  cat ~/.ssh/id_rsa.pub » ~/.ssh/authorized_keys\n  修改ssh配置文件\"/etc/ssh/sshd_config\"的下列内容，将以下内容的注释去掉，在三台机器上均进行修改：\nRSAAuthentication yes # 启用 RSA 认证 PubkeyAuthentication yes # 启用公钥私钥配对认证方式 AuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径（和上面生成的文件同）   重启ssh服务，才能使刚才设置有效。service sshd restart\n  验证无密码登录本机是否成功ssh localhost\n  接下来的就是把公钥复制到所有的Slave机器上。使用下面的命令进行复制公钥：\n$ scp /root/.ssh/id_rsa.pub root@Slave1:/root/ $ scp /root/.ssh/id_rsa.pub root@Slave2:/root/   接着配置Slave节点，以下是在Slave1节点的配置操作。\n1在”/root/“下创建”.ssh”文件夹，如果已经存在就不需要创建了。\nmkdir /root/.ssh\n2）将Master的公钥追加到Slave1的授权文件”authorized_keys”中去。\ncat /root/id_rsa.pub » /root/.ssh/authorized_keys\n3）修改”/etc/ssh/sshd_config”，具体步骤参考前面Master设置的第3步和第4步。\n4）用Master使用ssh无密码登录Slave1\nssh 114.55.246.77\n5）把”/root/“目录下的”id_rsa.pub”文件删除掉。\nrm –r /root/id_rsa.pub\n重复上面的5个步骤把Slave2服务器进行相同的配置。\n  配置Slave无密码登录Master以下是在Slave1节点的配置操作。\n1）创建”Slave1”自己的公钥和私钥，并把自己的公钥追加到”authorized_keys”文件中，执行下面命令：\nssh-keygen -t rsa -P ‘’\ncat /root/.ssh/id_rsa.pub » /root/.ssh/authorized_keys\n2）将Slave1节点的公钥”id_rsa.pub”复制到Master节点的”/root/“目录下。\nscp /root/.ssh/id_rsa.pub root@Master:/root/\n以下是在Master节点的配置操作。\n1）将Slave1的公钥追加到Master的授权文件”authorized_keys”中去。\ncat ~/id_rsa.pub » ~/.ssh/authorized_keys\n2）删除Slave1复制过来的”id_rsa.pub”文件。\nrm –r /root/id_rsa.pub\n配置完成后测试从Slave1到Master无密码登录。\nssh 114.55.246.88\n按照上面的步骤把Slave2和Master之间建立起无密码登录。这样，Master能无密码验证登录每个Slave，每个Slave也能无密码验证登录到Master。\n     安装基础环境（JAVA和SCALA环境）这里不再赘述\n  Hadoop2.7.3完全分布式搭建以下是在Master节点操作：\n  下载二进制包hadoop-2.7.7.tar.gz\n  解压并移动到相应目录，我习惯将软件放到/opt目录下，命令如下：\n$ tar -zxvf hadoop-2.7.3.tar.gz $ mv hadoop-2.7.7 /opt   修改对应的配置文件，修改/etc/profile，增加如下内容：\nexport HADOOP_HOME=/opt/hadoop-2.7.3/ export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME export HADOOP_ROOT_LOGGER=INFO,console export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib\"   修改完成后执行$ source /etc/profile\n  修改$HADOOP_HOME/etc/hadoop/hadoop-env.sh，修改JAVA_HOME 如下： export JAVA_HOME=/usr/local/jdk1.8.0_121\n  修改$HADOOP_HOME/etc/hadoop/slaves，将原来的localhost删除，改成如下内容：\n Slave1 Slave2    修改$HADOOP_HOME/etc/hadoop/core-site.xml\n  fs.defaultFS hdfs://Master:9000   io.file.buffer.size 131072   hadoop.tmp.dir /opt/hadoop-2.7.7/tmp     修改$HADOOP_HOME/etc/hadoop/hdfs-site.xml\n  dfs.namenode.secondary.http-address Master:50090   dfs.replication 2   dfs.namenode.name.dir file:/opt/hadoop-2.7.7/hdfs/name   dfs.datanode.data.dir file:/opt/hadoop-2.7.7/hdfs/data     cp mapred-site.xml.template mapred-site.xml ，并修改$HADOOP_HOME/etc/hadoop/mapred-site.xml\n  mapreduce.framework.name yarn   mapreduce.jobhistory.address Master:10020   mapreduce.jobhistory.address Master:19888     修改$HADOOP_HOME/etc/hadoop/yarn-site.xml\n  yarn.nodemanager.aux-services mapreduce_shuffle   yarn.resourcemanager.address Master:8032   yarn.resourcemanager.scheduler.address Master:8030   yarn.resourcemanager.resource-tracker.address Master:8031   yarn.resourcemanager.admin.address Master:8033   yarn.resourcemanager.webapp.address Master:8088     复制Master节点的hadoop文件夹到Slave1和Slave2上\n  $ scp -r /opt/hadoop-2.7.7 root@Slave1:/opt $ scp -r /opt/hadoop-2.7.7 root@Slave2:/opt  在Slave1和Slave2上分别修改/etc/profile，过程同Master一样 在Master节点启动集群，启动之前格式化一下namenode：  Hadoop namenode -format 启动：/opt/hadoop-2.7.7/sbin/start-all.sh 至此hadoop的完全分布式搭建完毕   查看集群是否启动成功：  $ jps -m Master显示： SecondaryNameNode ResourceManager NameNode Slave显示： NodeManager DataNode   Spark完全分布式环境搭建以下操作都在Master节点进行。\n  下载二进制包spark-2.4.3-bin-hadoop2.7.tgz\n  解压并移动到相应目录，命令如下：\n$ tar -zxvf spark-2.4.3-bin-hadoop2.7.tgz $ mv hadoop-2.7.3 /opt   修改相应的配置文件，修改/etc/profie，增加如下内容：\nexport SPARK_HOME=/opt/spark-2.4.3-bin-hadoop2.7/ export PATH=$PATH:$SPARK_HOME/bin   复制spark-env.sh.template成spark-env.sh\n$ cp spark-env.sh.template spark-env.sh   修改$SPARK_HOME/conf/spark-env.sh，添加如下内容：\nexport JAVA_HOME=/usr/local/jdk1.8.0_121 export SCALA_HOME=/usr/share/scala export HADOOP_HOME=/opt/hadoop-2.7.3 export HADOOP_CONF_DIR=/opt/hadoop-2.7.3/etc/hadoop export SPARK_MASTER_IP=114.55.246.88 export SPARK_MASTER_HOST=114.55.246.88 export SPARK_LOCAL_IP=114.55.246.88 export SPARK_WORKER_MEMORY=1g export SPARK_WORKER_CORES=2 export SPARK_HOME=/opt/spark-2.4.3-bin-hadoop2.7 export SPARK_DIST_CLASSPATH=$(/opt/hadoop-2.7.3/bin/hadoop classpath)   复制slaves.template成slaves\n$ cp slaves.template slaves   修改$SPARK_HOME/conf/slaves，添加如下内容：\nMaster Slave1 Slave2   将配置好的spark文件复制到Slave1和Slave2节点\n$ scp /opt/spark-2.4.3-bin-hadoop2.7 root@Slave1:/opt $ scp /opt/spark-2.4.3-bin-hadoop2.7 root@Slave2:/opt   修改Slave1和Slave2配置在Slave1和Slave2上分别修改/etc/profile，增加Spark的配置，过程同Master一样。\n在Slave1和Slave2修改$SPARK_HOME/conf/spark-env.sh，将export SPARK_LOCAL_IP=114.55.246.88改成Slave1和Slave2对应节点的IP。\n  在Master节点启动集群/opt/spark-2.4.3-bin-hadoop2.7/sbin/start-all.sh\n  查看集群是否启动成功\n$ jps -m Master在Hadoop的基础上新增了： Master Slave在Hadoop的基础上新增了： Worker     在我的博客查看更多\n","wordCount":"2680","inLanguage":"en","datePublished":"2019-09-26T08:52:30Z","dateModified":"2019-09-26T08:52:30Z","author":{"@type":"Person","name":"赖杰"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://tuuna.top/2019/09/26/flume_kafka_spark_building/"},"publisher":{"@type":"Organization","name":"洛文小站","logo":{"@type":"ImageObject","url":"http://tuuna.top/favicon.ico"}}}</script></head><body class=single id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><noscript><style type=text/css>.theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=http://tuuna.top accesskey=h><img src=/cloud.png alt=logo aria-label=logo height=35>Home</a>
<span class=logo-switches><span class=theme-toggle><a id=theme-toggle accesskey=t><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></a></span><span class=lang-switch><span>|</span><ul><li><a href=http://tuuna.top/about/ title=About aria-label=About>About</a></li></ul></span></span></div><ul class=menu id=menu onscroll=menu_on_scroll()><li><a href=http://tuuna.top/archives/><span>Archive</span></a></li><li><a href=http://tuuna.top/tags/><span>Tag</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Flume+Kafka+Spark环境搭建</h1><div class=post-meta>September 26, 2019&nbsp;·&nbsp;13 min&nbsp;·&nbsp;赖杰</div></header><div class=post-content><h3 id=文章目录>文章目录<a hidden class=anchor aria-hidden=true href=#文章目录>#</a></h3><ul><li><ul><li>单机版环境搭建及相关DEMO<ul><li>Flume<ul><li><a href=https://mp.csdn.net/mdeditor/101452109#Flume_9>Flume基本介绍与架构</a></li><li><a href=https://mp.csdn.net/mdeditor/101452109#Flume_91>Flume安装部署</a></li><li><a href=https://mp.csdn.net/mdeditor/101452109#_133>案例实操</a></li></ul></li><li>Kafka<ul><li><a href=https://mp.csdn.net/mdeditor/101452109#_402>环境搭建</a></li><li><a href=https://mp.csdn.net/mdeditor/101452109#Kafka_663>Kafka控制台的一些命令操作</a></li><li><a href=https://mp.csdn.net/mdeditor/101452109#Java_APIKafka_673>Java API控制Kafka</a></li><li><a href=https://mp.csdn.net/mdeditor/101452109#FlumeKafka_920>Flume+Kafka配合</a></li></ul></li><li>Spark<ul><li><a href=https://mp.csdn.net/mdeditor/101452109#Spark__966>Spark 简介</a></li><li><a href=https://mp.csdn.net/mdeditor/101452109#Spark_971>Spark环境搭建</a></li><li><a href=https://mp.csdn.net/mdeditor/101452109#Spark_Shell__1107>在Spark Shell 中运行代码</a></li><li><a href=https://mp.csdn.net/mdeditor/101452109#ScalawordCount_1184>Scala编写wordCount</a></li><li><a href=https://mp.csdn.net/mdeditor/101452109#SparkShell_1209>在Spark-Shell中执行词频统计</a></li><li><a href=https://mp.csdn.net/mdeditor/101452109#_1281>词频统计</a></li><li><a href=https://mp.csdn.net/mdeditor/101452109#_1299>编写独立应用程序执行词频统计</a></li></ul></li><li>Flume_Kafka_SparkStreaming实现词频统计<ul><li><a href=https://mp.csdn.net/mdeditor/101452109#_1407>准备工作</a></li></ul></li></ul></li><li>分布式环境搭建及相关DEMO<ul><li><a href=https://mp.csdn.net/mdeditor/101452109#Flume_1698>Flume</a></li><li><a href=https://mp.csdn.net/mdeditor/101452109#Kafka_1702>Kafka</a></li><li><a href=https://mp.csdn.net/mdeditor/101452109#Spark_1779>Spark</a></li></ul></li></ul></li></ul><blockquote><p>本文档主要讲述了flume+kafka+spark的单机分布式搭建，由浅入深，介绍了常见大数据流处理流程</p></blockquote><h2 id=单机版环境搭建及相关demo>单机版环境搭建及相关DEMO<a hidden class=anchor aria-hidden=true href=#单机版环境搭建及相关demo>#</a></h2><h3 id=flume>Flume<a hidden class=anchor aria-hidden=true href=#flume>#</a></h3><h4 id=flume基本介绍与架构>Flume基本介绍与架构<a hidden class=anchor aria-hidden=true href=#flume基本介绍与架构>#</a></h4><p>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。</p><p><strong>Flume出生日记</strong></p><p>有很多的服务和系统</p><ul><li>network devices</li><li>operating system</li><li>web servers</li><li>Applications</li></ul><p>这些系统都会产生很多的日志，那么把这些日志拿出来，用来分析时非常有用的。</p><p><strong>如何解决数据从其他的server上移动到Hadoop上？</strong></p><p>shell cp hadoop集群上的机器上， hadoop fs -put …/ 直接拷贝日志，但是没办法监控，而cp的时效性也不好，容错负载均衡也没办法做</p><p>======></p><p>Flume诞生了</p><p><strong>Flume架构</strong></p><p>Flume组成架构如图1-1，所示：</p><p><img src="https://img-blog.csdnimg.cn/2019092616354844.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述> 图1-1 Flume组成架构</p><p><strong>Agent</strong></p><p>Agent是一个JVM进程，它以事件的形式将数据从源头送至目的，是Flume数据传输的基本单元。</p><p>Agent主要有3个部分组成，Source、Channel、Sink。</p><p><strong>Source</strong></p><p>Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。</p><p><strong>Channel</strong></p><p>Channel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。</p><p>Flume自带两种Channel：<code>Memory Channel</code>和<code>File Channel</code></p><p><code>Memory Channel</code>是内存中的队列。<code>Memory Channel</code>在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么<code>Memory Channel</code>就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</p><p><code>File Channel</code>将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。</p><p><strong>Sink</strong></p><p>Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。</p><p>Sink是完全事务性的。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p><p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。</p><p><strong>Event</strong></p><p>传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。</p><p><strong>Flume拓扑结构</strong></p><p>Flume的拓扑结构如图1-3、1-4、1-5和1-6所示：</p><p><img src=https://img-blog.csdnimg.cn/20190926163622528.png alt=在这里插入图片描述></p><p>图1-3 Flume Agent连接</p><p><img src="https://img-blog.csdnimg.cn/20190926163633705.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><p>图1-4 单source，多channel、sink</p><p><img src="https://img-blog.csdnimg.cn/20190926163647260.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><p>图1-5 Flume负载均衡</p><p><img src="https://img-blog.csdnimg.cn/20190926163658171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><p>图1-6 Flume Agent聚合</p><h4 id=flume安装部署>Flume安装部署<a hidden class=anchor aria-hidden=true href=#flume安装部署>#</a></h4><p>Flume的安装相对简单，但是<strong>前提是要先下好Java环境JDK，1.8以上即可，JDK安装可以查看Kafka安装流程</strong>，这里以Linux下的安装为例</p><p><strong>Flume安装地址</strong></p><ul><li><a href=http://flume.apache.org/>Flume官网地址</a></li><li><a href=http://flume.apache.org/FlumeUserGuide.html>文档查看地址</a></li><li><a href=http://archive.apache.org/dist/flume/>下载地址</a></li></ul><p><strong>安装部署</strong></p><ol><li>解压apache-flume-1.7.0-bin.tar.gz到/usr/local/目录下(安装包详见安装包文件夹flume文件夹下的tar.gz压缩包)</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e>#把下载的包移动到目录</span>
$ sudo mv apache-flume-1.7.0-bin.tar.gz /usr/local
<span style=color:#75715e>#解压</span>
$ sudo tar -zxvf apache-flume-1.7.0-bin.tar.gz  /usr/local/
</code></pre></div><ol><li>修改apache-flume-1.7.0-bin的名称为flume</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ sudo mv apache-flume-1.7.0-bin flume
</code></pre></div><ol><li>将flume/conf下的flume-env.sh.template文件修改为flume-env.sh，并配置flume-env.sh文件</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ mv flume-env.sh.template flume-env.sh

$ vi flume-env.sh
export JAVA_HOME<span style=color:#f92672>=</span>/opt/module/jdk1.8.0_144<span style=color:#f92672>(</span>这里路径替换为本机JDK安装目录<span style=color:#f92672>)</span>
</code></pre></div><h4 id=案例实操>案例实操<a hidden class=anchor aria-hidden=true href=#案例实操>#</a></h4><ul><li>监控端口数据<ul><li><code>案例需求</code>：首先，Flume监控本机44444端口，然后通过telnet工具向本机44444端口发送消息，最后Flume将监听的数据实时显示在控制台。</li><li><code>需求分析</code>：</li></ul></li></ul><p><img src="https://img-blog.csdnimg.cn/20190926164918735.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><ul><li><p>实现步骤：</p><ul><li><p>安装telnet工具在/usr/local目录下创建flume-telnet文件夹。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ mkdir flume-telnet
</code></pre></div><p>再将rpm软件包(xinetd-2.3.14-40.el6.x86_64.rpm、telnet-0.17-48.el6.x86_64.rpm和telnet-server-0.17-48.el6.x86_64.rpm)拷入/usr/local/flume-telnet文件夹下面。执行RPM软件包安装命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ sudo rpm -ivh xinetd-2.3.14-40.el6.x86_64.rpm
    
$ sudo rpm -ivh telnet-0.17-48.el6.x86_64.rpm
$ sudo rpm -ivh telnet-server-0.17-48.el6.x86_64.rpm
</code></pre></div></li></ul></li><li><pre><code>判断44444端口是否被占用
</code></pre><p>判断44444端口是否占用，如果被占用则kill掉或者更换端口</p><p><code>$ sudo netstat -tunlp | grep 44444 功能描述：netstat命令是一个监控TCP/IP网络的非常有用的工具，它可以显示路由表、实际的网络连接以及每一个网络接口设备的状态信息。 基本语法：netstat [选项] 选项参数： -t或–tcp：显示TCP传输协议的连线状况； -u或–udp：显示UDP传输协议的连线状况； -n或--numeric：直接使用ip地址，而不通过域名服务器； -l或--listening：显示监控中的服务器的Socket； -p或--programs：显示正在使用Socket的程序识别码和程序名称；</code></p><p>``</p></li><li><p>创建Flume Agent配置文件</p><pre><code>flume-telnet-logger.conf
</code></pre><p>在flume目录下创建job文件夹并进入job文件夹</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ mkdir job
$ cd job/   
</code></pre></div></li><li><p>在job文件夹下创建Flume Agent配置文件</p><pre><code>flume-telnet-logger.conf
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ touch flume-telnet-logger.conf
<span style=color:#75715e># 如果觉得vim上手难度太大，可以使用gedit来进行编辑</span>
$ vim flume-telnet-logger.conf
<span style=color:#75715e># 在conf文件中加入以下内容</span>
  
<span style=color:#75715e># Name the components on this agent</span>
a1.sources <span style=color:#f92672>=</span> r1
a1.sinks <span style=color:#f92672>=</span> k1
a1.channels <span style=color:#f92672>=</span> c1
<span style=color:#75715e># Describe/configure the source</span>
a1.sources.r1.type <span style=color:#f92672>=</span> netcat
a1.sources.r1.bind <span style=color:#f92672>=</span> localhost
a1.sources.r1.port <span style=color:#f92672>=</span> <span style=color:#ae81ff>44444</span>
<span style=color:#75715e># Describe the sink</span>
a1.sinks.k1.type <span style=color:#f92672>=</span> logger
<span style=color:#75715e># Use a channel which buffers events in memory</span>
a1.channels.c1.type <span style=color:#f92672>=</span> memory
a1.channels.c1.capacity <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
a1.channels.c1.transactionCapacity <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
<span style=color:#75715e># Bind the source and sink to the channel</span>
a1.sources.r1.channels <span style=color:#f92672>=</span> c1
a1.sinks.k1.channel <span style=color:#f92672>=</span> c1
</code></pre></div></li></ul><p>注：配置文件来源于<a href=http://flume.apache.org/FlumeUserGuide.html>官方手册</a></p><p><img src="https://img-blog.csdnimg.cn/20190926164900369.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><ul><li><p>先开启flume监听端口</p><p><code>$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-telnet-logger.conf -Dflume.root.logger=INFO,console 参数说明： --conf conf/ ：表示配置文件存储在conf/目录 --name a1 ：表示给agent起名为a1 --conf-file job/flume-telnet.conf ：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。 -Dflume.root.logger&lt;span class="token operator">==&lt;/span>INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</code></p><p>``</p></li><li><p>使用telnet工具向本机的44444端口发送内容</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ telnet localhost <span style=color:#ae81ff>44444</span>
</code></pre></div></li><li><p>将A服务器上的日志实时采集到B服务器一般跨节点都是使用</p><pre><code>avro sink
</code></pre><p>技术选型有两种方案:</p><ul><li><p>exec source + memory channel + avro sink</p><pre><code>// Flume的关键就是写配置文件，仍然是在conf文件夹下创建配置文件
// avro-memory-sink.conf
    
Name the components on this agentexec-memory-avro.sources = exec-source
exec-memory-avro.sinks = arvo-sink
exec-memory-avro.channels = memory-channel
Describe/configure the sourceexec-memory-avro.sources.exec-source.type = exec
exec-memory-avro.sources.exec-source.command = tail -F $FLUME_HOME/logs/flume.log
exec-memory-avro.sources.exec-source.shell = /bin/sh -c
Describe the sinkexec-memory-avro.sinks.arvo-sink.type = avro
exec-memory-avro.sinks.arvo-sink.hostname = localhost
exec-memory-avro.sinks.arvo-sink.port = 44444
Use a channel which buffers events in memoryexec-memory-avro.channels.memory-channel.type = memory
exec-memory-avro.channels.memory-channel.capacity = 1000
exec-memory-avro.channels.memory-channel.transactionCapacity = 100
Bind the source and sink to the channelexec-memory-avro.sources.exec-source.channels = memory-channel
exec-memory-avro.sinks.arvo-sink.channel = memory-channel
</code></pre></li><li><p>avro source + memory channel + logger sink</p><pre><code>// avro-logger-sink.conf
# Name the components on this agent
avro-memory-logger.sources = avro-source
avro-memory-logger.sinks = logger-sink
avro-memory-logger.channels = memory-channel
    
Describe/configure the sourceavro-memory-logger.sources.avro-source.type = avro
avro-memory-logger.sources.avro-source.bind = localhost
avro-memory-logger.sources.avro-source.port = 44444
Describe the sinkavro-memory-logger.sinks.logger-sink.type = logger
Use a channel which buffers events in memoryavro-memory-logger.channels.memory-channel.type = memory
avro-memory-logger.channels.memory-channel.capacity = 1000
avro-memory-logger.channels.memory-channel.transactionCapacity = 100
Bind the source and sink to the channelavro-memory-logger.sources.avro-source.channels = memory-channel
avro-memory-logger.sinks.logger-sink.channel = memory-channel
</code></pre></li></ul><p>接下来启动两个配置</p><pre><code>先启动avro-memory-logger
  
flume-ng agent \
–name avro-memory-logger \
–conf $FLUME_HOME/conf \
–conf-file $FLUME_HOME/conf/avro-memory-logger.conf \
-Dflume.root.logger=INFO,console
再启动另外一个
flume-ng agent –name exec-memory-avro 
–conf $FLUME_HOME/conf \
–conf-file $FLUME_HOME/conf/exec-memory-avro.conf \
-Dflume.root.logger=INFO,console
</code></pre></li></ul><p><img src="https://img-blog.csdnimg.cn/20190926163736672.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><p><img src="https://img-blog.csdnimg.cn/20190926163750470.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><blockquote><p>一个可能因为手误出现的bug</p><blockquote><p>log4j:WARN No appenders could be found for logger (org.apache.flume.lifecycle.LifecycleSupervisor). log4j:WARN Please initialize the log4j system properly.</p><p>log4j:WARN See <a href=http://logging.apache.org/log4j/1.2/faq.html#noconfig>http://logging.apache.org/log4j/1.2/faq.html#noconfig</a> for more info.</p></blockquote><p>出现这个错误是因为路径没有写对</p></blockquote><p>往监听的日志中输入一段字符串，可以看到我们的logger sink 已经成功接收到信息</p><p><img src="https://img-blog.csdnimg.cn/20190926163808861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><p>上面Flume的基本流程图如下</p><p><img src="https://img-blog.csdnimg.cn/20190926163825666.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><h3 id=kafka>Kafka<a hidden class=anchor aria-hidden=true href=#kafka>#</a></h3><p><strong>Kafka</strong>是由<a href=https://zh.wikipedia.org/wiki/Apache%E8%BD%AF%E4%BB%B6%E5%9F%BA%E9%87%91%E4%BC%9A>Apache软件基金会</a>开发的一个<a href=https://zh.wikipedia.org/wiki/%E5%BC%80%E6%BA%90>开源</a><a href=https://zh.wikipedia.org/wiki/%E6%B5%81%E5%A4%84%E7%90%86>流处理</a>平台，由<a href=https://zh.wikipedia.org/wiki/Scala>Scala</a>和<a href=https://zh.wikipedia.org/wiki/Java>Java</a>编写。该项目的目标是为处理实时数据提供一个统一、高吞吐、低延迟的平台。其持久化层本质上是一个“按照分布式事务日志架构的大规模发布/订阅消息队列”，[<a href=https://zh.wikipedia.org/wiki/Kafka#cite_note-3>3]</a>这使它作为企业级基础设施来处理流式数据非常有价值。此外，Kafka可以通过Kafka Connect连接到外部系统（用于数据输入/输出），并提供了Kafka Streams——一个<a href=https://zh.wikipedia.org/wiki/Java>Java</a>流式处理<a href=https://zh.wikipedia.org/wiki/%E5%BA%93_(%E8%AE%A1%E7%AE%97%E6%9C%BA)>库</a>。</p><p>具体的架构可以查看官网的<a href=https://kafka.apache.org/intro>intro部分</a>。</p><p>因为在实际编程中使用kafka_2.11-0.11.00以上版本和使用以下版本的Java API 不一致，所以推荐直接参照官网的文档进行编程。</p><h4 id=环境搭建>环境搭建<a hidden class=anchor aria-hidden=true href=#环境搭建>#</a></h4><p><strong>单机单节点</strong></p><p><strong>搭建说明</strong></p><p>需要有一定的Linux操作经验，对于没有权限之类的问题要懂得通过命令解决</p><p>Kafka的安装相比Flume来说更加复杂，因为Kafka依赖于Zookeeper</p><p><strong>环境说明：</strong></p><ul><li>os：Ubuntu 18.04</li><li>zookeeper：zookeeper 3.4.9</li><li>kafka：kafka_2.11-0.11.0.0</li><li>jdk：jdk 8（kafka启动需要使用到jdk）</li></ul><p><strong>详细说明：</strong></p><p><strong>一、jdk安装</strong></p><p>jdk分为以下几种：jre、openjdk、 oracle jdk，这里我们要安装的是oracle jdk（推荐安装）</p><pre><code>add-apt-repository ppa:webupd8team/java
apt-get update
apt-get install oracle-java8-installer
apt-get install oracle-java8-set-default
</code></pre><p>测试安装版本：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL2hldGJwc3FjZW4ucG5n?x-oss-process=image/format,png" alt=img></p><p><strong>二、安装配置zookeeper单机模式</strong></p><p>下载<a href=https://archive.apache.org/dist/zookeeper/zookeeper-3.4.5/zookeeper-3.4.5.tar.gz>zookeeper 3.4.5</a>，开始安装（软件包详见软件包下的kafka中的压缩包）：</p><pre><code>cd /usr/local
wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.5/zookeeper-3.4.5.tar.gz
</code></pre><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL3F6MmpuZHU2dTEucG5n?x-oss-process=image/format,png" alt=img></p><p>等待安装成功：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL3FhamUyemk4bnMucG5n?x-oss-process=image/format,png" alt=img></p><p>解压：</p><pre><code>tar -zxvf zookeeper-3.4.5.tar.gz
</code></pre><p>解压后同目录下便存在相同文件夹：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL2M4dXFqNGQ5aXQucG5n?x-oss-process=image/format,png" alt=img></p><p>切换到conf目录下：</p><pre><code>cd zookeeper-3.4.5/conf/
</code></pre><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL2RzZWdpeng2MXcucG5n?x-oss-process=image/format,png" alt=img></p><p>复制zoo_sample.cfg到zoo.cfg：</p><pre><code>cp zoo_sample.cfg zoo.cfg
</code></pre><p>然后编辑zoo.cfg如下（其它不用管，默认即可）：</p><pre><code>initLimit=10
syncLimit=5
dataDir=/home/young/zookeeper/data
clientPort=2181
</code></pre><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL3l4OXg5emxwMzQucG5n?x-oss-process=image/format,png" alt=img></p><p>别忘了新建dataDir目录：</p><pre><code>mkdir /home/young/zookeeper/data
</code></pre><p>为zookeeper创建环境变量，打开/etc/profile文件，并在最末尾添加如下内容：</p><pre><code>vi /etc/profile
</code></pre><p>添加内容如下：</p><pre><code>export ZOOKEEPER_HOME=/home/young/zookeeper
export PATH=.:$ZOOKEEPER_HOME/bin:$JAVA_HOME/bin:$PATH
</code></pre><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL2tta3hiNDhzbjcucG5n?x-oss-process=image/format,png" alt=img></p><p>配置完成之后，切换到zookeeper/bin目录下，启动服务：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL2NyZW9pY2E3Z2gucG5n?x-oss-process=image/format,png" alt=img></p><p>关闭服务：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL3B3ZXd0dDM2Z3EucG5n?x-oss-process=image/format,png" alt=img></p><p>这里暂时先关闭zookeeper服务，防止下面使用kafka启动时报端口占用错误。</p><p><strong>三、安装配置kafka单机模式</strong></p><p>下载<a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/0.11.0.0/kafka_2.11-0.11.0.0.tgz">kafka</a>（安装包详见软件包kafka下的压缩包）：</p><pre><code>cd /usr/local
wget https://www.apache.org/dyn/closer.cgi?path=/kafka/0.11.0.0/kafka_2.11-0.11.0.0.tgz
</code></pre><p>解压：</p><pre><code>tar -zxvf kafka_2.11-0.11.0.0.tgz
</code></pre><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL3lhaTgzMXVza3IucG5n?x-oss-process=image/format,png" alt=img></p><p>进入kafka/config目录下：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL201anlzZHg2YjcucG5n?x-oss-process=image/format,png" alt=img></p><p>以上文件是需要修改的文件，下面一个个修改配置：</p><p>配置server.properties：</p><p>以下为修改的，其他为默认即可：</p><pre><code>#broker.id需改成正整数，单机为1就好
broker.id=1
#指定端口号
port=9092
#localhost这一项还有其他要修改，详细见下面说明
host.name=localhost
#指定kafka的日志目录
log.dirs=/usr/local/kafka_2.11-0.11.0.0/kafka-logs
#连接zookeeper配置项，这里指定的是单机，所以只需要配置localhost，若是实际生产环境，需要在这里添加其他ip地址和端口号
zookeeper.connect=localhost:2181
</code></pre><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL2c0eHluNmQ1bWkucG5n?x-oss-process=image/format,png" alt=img></p><p>配置zookeeper.properties：</p><pre><code>#数据目录
dataDir=/usr/local/kafka_2.11-0.11.0.0/zookeeper/data
#客户端端口
clientPort=2181
host.name=localhost
</code></pre><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL3dzamd6bXZhZ3IucG5n?x-oss-process=image/format,png" alt=img></p><p>配置producer.properties：</p><pre><code>zookeeper.connect=localhost:2181
</code></pre><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL2E3c2Z1a2szaDgucG5n?x-oss-process=image/format,png" alt=img></p><p>配置consumer.properties：</p><pre><code>zookeeper.connect=localhost:2181
</code></pre><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL2x6eTVxeWlzOG8ucG5n?x-oss-process=image/format,png" alt=img></p><p>最后还需要拷贝几个jar文件到kafka的libs目录，分别是zookeeper-xxxx.jar、log4j-xxxx.jar、slf4j-simple-xxxx.jar，最后如下：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL28wYm45b3RldWkucG5n?x-oss-process=image/format,png" alt=img></p><p><strong>四、kafka的使用</strong></p><p>启动zookeeper服务：</p><pre><code>bin/zookeeper-server-start.sh config/zookeeper.properties
</code></pre><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwLzF5bTd1eDVtZGoucG5n?x-oss-process=image/format,png" alt=img></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwL2hwcGJsYTZ2ZngucG5n?x-oss-process=image/format,png" alt=img></p><p>新开一个窗口启动kafka服务：</p><pre><code>bin/kafka-server-start.sh config/server.properties
</code></pre><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwLzEyYmthN3RrbnEucG5n?x-oss-process=image/format,png" alt=img></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUveWVoZS0xMTQxNTYwLzFjN2V3Y250d2MucG5n?x-oss-process=image/format,png" alt=img></p><p>至此单机服务搭建已经全部完成</p><p><strong>单机多节点</strong></p><p>对于单机单节点只需要使用一个配置文件来启动即可，那么对于单机多节点，只需要建立多个配置文件，并且启动即可。比如我们需要有三个节点。</p><p><img src=https://img-blog.csdnimg.cn/20190926164312541.png alt=在这里插入图片描述></p><p>然后我们的每个<code>server properies</code>里面的端口以及ID要不一致</p><p><strong>server-1.properties</strong></p><p><img src="https://img-blog.csdnimg.cn/20190926164324284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><p><strong>server-2.properties</strong></p><p><img src="https://img-blog.csdnimg.cn/20190926164347214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><p><strong>server-3.properties</strong></p><p><img src="https://img-blog.csdnimg.cn/20190926164356494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><p>当然其对应的log对应目录也要修改，这个就不多说了</p><p>然后在控制台启动</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>&gt; bin/kafka-server-start.sh config/server-1.properties &amp;
&gt; bin/kafka-server-start.sh config/server-2.properties &amp;
&gt; bin/kafka-server-start.sh config/server-3.properties &amp;
</code></pre></div><p>通过jps -m 能看到三个kafka即可（可能以普通用户看不到相应的进程，只是因为没给到权限，可以给权限或者直接sudo su切换到超级用户）</p><h4 id=kafka控制台的一些命令操作>Kafka控制台的一些命令操作<a hidden class=anchor aria-hidden=true href=#kafka控制台的一些命令操作>#</a></h4><p>控制台中我们可以通过命令建立topic，并且开启一个消费者一个生产者来模拟通信，这些在官网的<a href=https://kafka.apache.org/quickstart>quickstart</a>中都有详尽的描述</p><p><img src=https://img-blog.csdnimg.cn/20190926164415344.png alt=[外链图片转存失败(img-cCfCODtn-1569486879029)(../%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%80%E7%BB%88%E7%89%88%E6%96%87%E6%A1%A3/kafka%E5%AD%A6%E4%B9%A0/producer.png)]></p><p><img src=https://img-blog.csdnimg.cn/20190926164503390.png alt=在这里插入图片描述></p><p>通过我们的一个叫topic的标签，我们建立了一个生产者和一个消费者，可以明显看到消费者接收到了生产者的消息。其他比较常用的命令，比如<code>describe</code>等可以自行探索。</p><h4 id=java-api控制kafka>Java API控制Kafka<a hidden class=anchor aria-hidden=true href=#java-api控制kafka>#</a></h4><p>接下来会说一个简单的在Java中使用Kafka小例子</p><p>这里都是基于<code>2.11_0.11.0.0.0</code>版本以及之后的编程来说明，更低版本相应的API有些许变化，低版本中很多函数已经被替代和废除。</p><p><strong>基本配置</strong></p><ul><li>首先在Idea中建立一个新的Maven项目，这里我们选择一个achetype:scala-archetype-simple</li></ul><p><img src="https://img-blog.csdnimg.cn/20190926164517440.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><ul><li><p>接下来我们把Maven文件配置好，并且auto import dependencies，这里如果没有选择auto import，我们可以在Pom.xml右键找到maven选项里面有一个reload</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=color:#f92672>&lt;</span>project xmlns<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;http://maven.apache.org/POM/4.0.0&#34;</span> xmlns<span style=color:#f92672>:</span>xsi<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;http://www.w3.org/2001/XMLSchema-instance&#34;</span> xsi<span style=color:#f92672>:</span>schemaLocation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&#34;</span><span style=color:#f92672>&gt;</span>
  <span style=color:#f92672>&lt;</span>modelVersion<span style=color:#f92672>&gt;</span>4<span style=color:#f92672>.</span><span style=color:#a6e22e>0</span><span style=color:#f92672>.</span><span style=color:#a6e22e>0</span><span style=color:#f92672>&lt;/</span>modelVersion<span style=color:#f92672>&gt;</span>
  <span style=color:#f92672>&lt;</span>groupId<span style=color:#f92672>&gt;</span>com<span style=color:#f92672>.</span><span style=color:#a6e22e>test</span><span style=color:#f92672>.</span><span style=color:#a6e22e>spark</span><span style=color:#f92672>&lt;/</span>groupId<span style=color:#f92672>&gt;</span>
  <span style=color:#f92672>&lt;</span>artifactId<span style=color:#f92672>&gt;</span>spark streaming<span style=color:#f92672>&lt;/</span>artifactId<span style=color:#f92672>&gt;</span>
  <span style=color:#f92672>&lt;</span>version<span style=color:#f92672>&gt;</span>1<span style=color:#f92672>.</span><span style=color:#a6e22e>0</span><span style=color:#f92672>&lt;/</span>version<span style=color:#f92672>&gt;</span>
  <span style=color:#f92672>&lt;</span>inceptionYear<span style=color:#f92672>&gt;</span>2008<span style=color:#f92672>&lt;/</span>inceptionYear<span style=color:#f92672>&gt;</span>
  <span style=color:#f92672>&lt;</span>properties<span style=color:#f92672>&gt;</span>
    <span style=color:#f92672>&lt;</span>scala<span style=color:#f92672>.</span><span style=color:#a6e22e>version</span><span style=color:#f92672>&gt;</span>2<span style=color:#f92672>.</span><span style=color:#a6e22e>7</span><span style=color:#f92672>.</span><span style=color:#a6e22e>0</span><span style=color:#f92672>&lt;/</span>scala<span style=color:#f92672>.</span><span style=color:#a6e22e>version</span><span style=color:#f92672>&gt;</span>
    <span style=color:#f92672>&lt;</span>kafka<span style=color:#f92672>.</span><span style=color:#a6e22e>version</span><span style=color:#f92672>&gt;</span>0<span style=color:#f92672>.</span><span style=color:#a6e22e>11</span><span style=color:#f92672>.</span><span style=color:#a6e22e>0</span><span style=color:#f92672>.</span><span style=color:#a6e22e>0</span><span style=color:#f92672>&lt;/</span>kafka<span style=color:#f92672>.</span><span style=color:#a6e22e>version</span><span style=color:#f92672>&gt;</span>
  <span style=color:#f92672>&lt;/</span>properties<span style=color:#f92672>&gt;</span>
  
  
  <span style=color:#f92672>&lt;</span>dependencies<span style=color:#f92672>&gt;</span>
    <span style=color:#f92672>&lt;</span>dependency<span style=color:#f92672>&gt;</span>
      <span style=color:#f92672>&lt;</span>groupId<span style=color:#f92672>&gt;</span>org<span style=color:#f92672>.</span><span style=color:#a6e22e>scala</span><span style=color:#f92672>-</span>lang<span style=color:#f92672>&lt;/</span>groupId<span style=color:#f92672>&gt;</span>
      <span style=color:#f92672>&lt;</span>artifactId<span style=color:#f92672>&gt;</span>scala<span style=color:#f92672>-</span>library<span style=color:#f92672>&lt;/</span>artifactId<span style=color:#f92672>&gt;</span>
      <span style=color:#f92672>&lt;</span>version<span style=color:#f92672>&gt;</span>$<span style=color:#f92672>{</span>scala<span style=color:#f92672>.</span><span style=color:#a6e22e>version</span><span style=color:#f92672>}&lt;/</span>version<span style=color:#f92672>&gt;</span>
    <span style=color:#f92672>&lt;/</span>dependency<span style=color:#f92672>&gt;</span>
    <span style=color:#f92672>&lt;</span>dependency<span style=color:#f92672>&gt;</span>
      <span style=color:#f92672>&lt;</span>groupId<span style=color:#f92672>&gt;</span>org<span style=color:#f92672>.</span><span style=color:#a6e22e>apache</span><span style=color:#f92672>.</span><span style=color:#a6e22e>kafka</span><span style=color:#f92672>&lt;/</span>groupId<span style=color:#f92672>&gt;</span>
      <span style=color:#f92672>&lt;</span>artifactId<span style=color:#f92672>&gt;</span>kafka_2<span style=color:#f92672>.</span><span style=color:#a6e22e>11</span><span style=color:#f92672>&lt;/</span>artifactId<span style=color:#f92672>&gt;</span>
      <span style=color:#f92672>&lt;</span>version<span style=color:#f92672>&gt;</span>$<span style=color:#f92672>{</span>kafka<span style=color:#f92672>.</span><span style=color:#a6e22e>version</span><span style=color:#f92672>}&lt;/</span>version<span style=color:#f92672>&gt;</span>
    <span style=color:#f92672>&lt;/</span>dependency<span style=color:#f92672>&gt;</span>
  <span style=color:#f92672>&lt;/</span>dependencies<span style=color:#f92672>&gt;</span>
  <span style=color:#f92672>&lt;</span>build<span style=color:#f92672>&gt;</span>
    <span style=color:#f92672>&lt;</span>sourceDirectory<span style=color:#f92672>&gt;</span>src<span style=color:#f92672>/</span>main<span style=color:#f92672>/</span>scala<span style=color:#f92672>&lt;/</span>sourceDirectory<span style=color:#f92672>&gt;</span>
    <span style=color:#f92672>&lt;</span>testSourceDirectory<span style=color:#f92672>&gt;</span>src<span style=color:#f92672>/</span>test<span style=color:#f92672>/</span>scala<span style=color:#f92672>&lt;/</span>testSourceDirectory<span style=color:#f92672>&gt;</span>
    <span style=color:#f92672>&lt;</span>plugins<span style=color:#f92672>&gt;</span>
      <span style=color:#f92672>&lt;</span>plugin<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;</span>groupId<span style=color:#f92672>&gt;</span>org<span style=color:#f92672>.</span><span style=color:#a6e22e>scala</span><span style=color:#f92672>-</span>tools<span style=color:#f92672>&lt;/</span>groupId<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;</span>artifactId<span style=color:#f92672>&gt;</span>maven<span style=color:#f92672>-</span>scala<span style=color:#f92672>-</span>plugin<span style=color:#f92672>&lt;/</span>artifactId<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;</span>executions<span style=color:#f92672>&gt;</span>
          <span style=color:#f92672>&lt;</span>execution<span style=color:#f92672>&gt;</span>
            <span style=color:#f92672>&lt;</span>goals<span style=color:#f92672>&gt;</span>
              <span style=color:#f92672>&lt;</span>goal<span style=color:#f92672>&gt;</span>compile<span style=color:#f92672>&lt;/</span>goal<span style=color:#f92672>&gt;</span>
              <span style=color:#f92672>&lt;</span>goal<span style=color:#f92672>&gt;</span>testCompile<span style=color:#f92672>&lt;/</span>goal<span style=color:#f92672>&gt;</span>
            <span style=color:#f92672>&lt;/</span>goals<span style=color:#f92672>&gt;</span>
          <span style=color:#f92672>&lt;/</span>execution<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;/</span>executions<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;</span>configuration<span style=color:#f92672>&gt;</span>
          <span style=color:#f92672>&lt;</span>scalaVersion<span style=color:#f92672>&gt;</span>$<span style=color:#f92672>{</span>scala<span style=color:#f92672>.</span><span style=color:#a6e22e>version</span><span style=color:#f92672>}&lt;/</span>scalaVersion<span style=color:#f92672>&gt;</span>
          <span style=color:#f92672>&lt;</span>args<span style=color:#f92672>&gt;</span>
            <span style=color:#f92672>&lt;</span>arg<span style=color:#f92672>&gt;-</span>target<span style=color:#f92672>:</span>jvm<span style=color:#f92672>-</span>1<span style=color:#f92672>.</span><span style=color:#a6e22e>5</span><span style=color:#f92672>&lt;/</span>arg<span style=color:#f92672>&gt;</span>
          <span style=color:#f92672>&lt;/</span>args<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;/</span>configuration<span style=color:#f92672>&gt;</span>
      <span style=color:#f92672>&lt;/</span>plugin<span style=color:#f92672>&gt;</span>
      <span style=color:#f92672>&lt;</span>plugin<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;</span>groupId<span style=color:#f92672>&gt;</span>org<span style=color:#f92672>.</span><span style=color:#a6e22e>apache</span><span style=color:#f92672>.</span><span style=color:#a6e22e>maven</span><span style=color:#f92672>.</span><span style=color:#a6e22e>plugins</span><span style=color:#f92672>&lt;/</span>groupId<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;</span>artifactId<span style=color:#f92672>&gt;</span>maven<span style=color:#f92672>-</span>eclipse<span style=color:#f92672>-</span>plugin<span style=color:#f92672>&lt;/</span>artifactId<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;</span>configuration<span style=color:#f92672>&gt;</span>
          <span style=color:#f92672>&lt;</span>downloadSources<span style=color:#f92672>&gt;</span><span style=color:#66d9ef>true</span><span style=color:#f92672>&lt;/</span>downloadSources<span style=color:#f92672>&gt;</span>
          <span style=color:#f92672>&lt;</span>buildcommands<span style=color:#f92672>&gt;</span>
            <span style=color:#f92672>&lt;</span>buildcommand<span style=color:#f92672>&gt;</span>ch<span style=color:#f92672>.</span><span style=color:#a6e22e>epfl</span><span style=color:#f92672>.</span><span style=color:#a6e22e>lamp</span><span style=color:#f92672>.</span><span style=color:#a6e22e>sdt</span><span style=color:#f92672>.</span><span style=color:#a6e22e>core</span><span style=color:#f92672>.</span><span style=color:#a6e22e>scalabuilder</span><span style=color:#f92672>&lt;/</span>buildcommand<span style=color:#f92672>&gt;</span>
          <span style=color:#f92672>&lt;/</span>buildcommands<span style=color:#f92672>&gt;</span>
          <span style=color:#f92672>&lt;</span>additionalProjectnatures<span style=color:#f92672>&gt;</span>
            <span style=color:#f92672>&lt;</span>projectnature<span style=color:#f92672>&gt;</span>ch<span style=color:#f92672>.</span><span style=color:#a6e22e>epfl</span><span style=color:#f92672>.</span><span style=color:#a6e22e>lamp</span><span style=color:#f92672>.</span><span style=color:#a6e22e>sdt</span><span style=color:#f92672>.</span><span style=color:#a6e22e>core</span><span style=color:#f92672>.</span><span style=color:#a6e22e>scalanature</span><span style=color:#f92672>&lt;/</span>projectnature<span style=color:#f92672>&gt;</span>
          <span style=color:#f92672>&lt;/</span>additionalProjectnatures<span style=color:#f92672>&gt;</span>
          <span style=color:#f92672>&lt;</span>classpathContainers<span style=color:#f92672>&gt;</span>
            <span style=color:#f92672>&lt;</span>classpathContainer<span style=color:#f92672>&gt;</span>org<span style=color:#f92672>.</span><span style=color:#a6e22e>eclipse</span><span style=color:#f92672>.</span><span style=color:#a6e22e>jdt</span><span style=color:#f92672>.</span><span style=color:#a6e22e>launching</span><span style=color:#f92672>.</span><span style=color:#a6e22e>JRE_CONTAINER</span><span style=color:#f92672>&lt;/</span>classpathContainer<span style=color:#f92672>&gt;</span>
            <span style=color:#f92672>&lt;</span>classpathContainer<span style=color:#f92672>&gt;</span>ch<span style=color:#f92672>.</span><span style=color:#a6e22e>epfl</span><span style=color:#f92672>.</span><span style=color:#a6e22e>lamp</span><span style=color:#f92672>.</span><span style=color:#a6e22e>sdt</span><span style=color:#f92672>.</span><span style=color:#a6e22e>launching</span><span style=color:#f92672>.</span><span style=color:#a6e22e>SCALA_CONTAINER</span><span style=color:#f92672>&lt;/</span>classpathContainer<span style=color:#f92672>&gt;</span>
          <span style=color:#f92672>&lt;/</span>classpathContainers<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;/</span>configuration<span style=color:#f92672>&gt;</span>
      <span style=color:#f92672>&lt;/</span>plugin<span style=color:#f92672>&gt;</span>
    <span style=color:#f92672>&lt;/</span>plugins<span style=color:#f92672>&gt;</span>
  <span style=color:#f92672>&lt;/</span>build<span style=color:#f92672>&gt;</span>
  <span style=color:#f92672>&lt;</span>reporting<span style=color:#f92672>&gt;</span>
    <span style=color:#f92672>&lt;</span>plugins<span style=color:#f92672>&gt;</span>
      <span style=color:#f92672>&lt;</span>plugin<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;</span>groupId<span style=color:#f92672>&gt;</span>org<span style=color:#f92672>.</span><span style=color:#a6e22e>scala</span><span style=color:#f92672>-</span>tools<span style=color:#f92672>&lt;/</span>groupId<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;</span>artifactId<span style=color:#f92672>&gt;</span>maven<span style=color:#f92672>-</span>scala<span style=color:#f92672>-</span>plugin<span style=color:#f92672>&lt;/</span>artifactId<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;</span>configuration<span style=color:#f92672>&gt;</span>
          <span style=color:#f92672>&lt;</span>scalaVersion<span style=color:#f92672>&gt;</span>$<span style=color:#f92672>{</span>scala<span style=color:#f92672>.</span><span style=color:#a6e22e>version</span><span style=color:#f92672>}&lt;/</span>scalaVersion<span style=color:#f92672>&gt;</span>
        <span style=color:#f92672>&lt;/</span>configuration<span style=color:#f92672>&gt;</span>
      <span style=color:#f92672>&lt;/</span>plugin<span style=color:#f92672>&gt;</span>
    <span style=color:#f92672>&lt;/</span>plugins<span style=color:#f92672>&gt;</span>
  <span style=color:#f92672>&lt;/</span>reporting<span style=color:#f92672>&gt;</span>
<span style=color:#f92672>&lt;/</span>project<span style=color:#f92672>&gt;</span>
</code></pre></div><ul><li>因为我们使用Java编程，所以我们在main下面建立一个java文件夹，并且把整个文件夹设为source，如下图<img src="https://img-blog.csdnimg.cn/20190926164534606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></li></ul></li><li><p>然后我们在这个例子会涉及到几个Class，包括启动的Class，消费者，生产者，配置</p></li></ul><p><img src="https://img-blog.csdnimg.cn/20190926164551330.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><p><strong>代码分析</strong></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=color:#75715e>//KafkaProperties.java
</span><span style=color:#75715e></span>
<span style=color:#f92672>package</span> com.test.spark.kafka<span style=color:#f92672>;</span>
<span style=color:#f92672>/**</span>

Kafka常用配置文件
<span style=color:#f92672>/</span>
<span style=color:#66d9ef>public</span> <span style=color:#66d9ef>class</span> <span style=color:#a6e22e>KafkaProperties</span> <span style=color:#f92672>{</span>
  <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>static</span> <span style=color:#66d9ef>final</span> String ZK<span style=color:#f92672>=</span> <span style=color:#960050;background-color:#1e0010>“</span>211<span style=color:#f92672>.</span><span style=color:#a6e22e>83</span><span style=color:#f92672>.</span><span style=color:#a6e22e>96</span><span style=color:#f92672>.</span><span style=color:#a6e22e>204</span><span style=color:#f92672>:</span>2181<span style=color:#960050;background-color:#1e0010>”</span><span style=color:#f92672>;</span>
  <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>static</span> <span style=color:#66d9ef>final</span> String TOPIC<span style=color:#f92672>=</span> <span style=color:#960050;background-color:#1e0010>“</span>test<span style=color:#960050;background-color:#1e0010>”</span><span style=color:#f92672>;</span>
  <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>static</span> <span style=color:#66d9ef>final</span> String BROKER_LIST <span style=color:#f92672>=</span> <span style=color:#960050;background-color:#1e0010>“</span>211<span style=color:#f92672>.</span><span style=color:#a6e22e>83</span><span style=color:#f92672>.</span><span style=color:#a6e22e>96</span><span style=color:#f92672>.</span><span style=color:#a6e22e>204</span><span style=color:#f92672>:</span>9092<span style=color:#960050;background-color:#1e0010>”</span><span style=color:#f92672>;</span>
  <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>static</span> <span style=color:#66d9ef>final</span> String GROUP_ID <span style=color:#f92672>=</span> <span style=color:#960050;background-color:#1e0010>“</span>test_group1<span style=color:#960050;background-color:#1e0010>”</span><span style=color:#f92672>;</span>

<span style=color:#f92672>}</span>
</code></pre></div><p>首先看一下配置文件，为了配置能更加全局化好修改，我们直接建立一个配置文件，把可能需要的一些全局参数放进来，方便后续开发。其中有<code>zookeeper的IP</code>，<code>Topic名称</code>，<code>服务器列表</code>以及<code>group_id</code>。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=color:#75715e>// KafkaProducerClient.java  
</span><span style=color:#75715e></span><span style=color:#f92672>package</span> com.test.spark.kafka<span style=color:#f92672>;</span> 
<span style=color:#f92672>import</span> org.apache.kafka.clients.producer.KafkaProducer<span style=color:#f92672>;</span> 
<span style=color:#f92672>import</span> org.apache.kafka.clients.producer.Producer<span style=color:#f92672>;</span> 
<span style=color:#f92672>import</span> org.apache.kafka.clients.producer.ProducerRecord<span style=color:#f92672>;</span> 
<span style=color:#f92672>import</span> java.util.Properties<span style=color:#f92672>;</span> 
<span style=color:#f92672>/**</span>  Kafka 生产者 <span style=color:#f92672>/</span> 
<span style=color:#66d9ef>public</span> <span style=color:#66d9ef>class</span> <span style=color:#a6e22e>KafkaProducerClient</span> <span style=color:#66d9ef>extends</span> Thread <span style=color:#f92672>{</span>   
  <span style=color:#66d9ef>private</span>  String topic<span style=color:#f92672>;</span>   
  <span style=color:#66d9ef>private</span>  Producer<span style=color:#f92672>&lt;</span>String<span style=color:#f92672>,</span> String<span style=color:#f92672>&gt;</span> producer<span style=color:#f92672>;</span>   

  <span style=color:#66d9ef>public</span> <span style=color:#a6e22e>KafkaProducerClient</span><span style=color:#f92672>(</span>String topic<span style=color:#f92672>)</span> <span style=color:#f92672>{</span>   
    <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>topic</span> <span style=color:#f92672>=</span> topic<span style=color:#f92672>;</span>    
    Properties properties <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> Properties<span style=color:#f92672>();</span>   
    properties<span style=color:#f92672>.</span><span style=color:#a6e22e>put</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;bootstrap.servers&#34;</span><span style=color:#f92672>,</span><span style=color:#e6db74>&#34;localhost:9092&#34;</span><span style=color:#f92672>);</span>
    <span style=color:#75715e>//properties.put(“serializer.class”,”kafka.serializer.StringEncoder”);
</span><span style=color:#75715e></span>    properties<span style=color:#f92672>.</span><span style=color:#a6e22e>put</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;key.serializer&#34;</span><span style=color:#f92672>,</span><span style=color:#e6db74>&#34;org.apache.kafka.common.serialization.StringSerializer&#34;</span><span style=color:#f92672>);</span>
    properties<span style=color:#f92672>.</span><span style=color:#a6e22e>put</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;value.serializer&#34;</span><span style=color:#f92672>,</span><span style=color:#e6db74>&#34;org.apache.kafka.common.serialization.StringSerializer&#34;</span><span style=color:#f92672>);</span>
    properties<span style=color:#f92672>.</span><span style=color:#a6e22e>put</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;request.required.acks&#34;</span><span style=color:#f92672>,</span><span style=color:#e6db74>&#34;1&#34;</span><span style=color:#f92672>);</span>
    producer <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> KafkaProducerString<span style=color:#f92672>,</span> String<span style=color:#f92672>(</span>properties<span style=color:#f92672>);</span>  
  <span style=color:#f92672>}</span>  

  <span style=color:#a6e22e>@Override</span>   
  <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>run</span><span style=color:#f92672>()</span> <span style=color:#f92672>{</span>  
    <span style=color:#66d9ef>int</span> messageNo <span style=color:#f92672>=</span> 1<span style=color:#f92672>;</span>    
    <span style=color:#66d9ef>while</span><span style=color:#f92672>(</span><span style=color:#66d9ef>true</span><span style=color:#f92672>)</span> <span style=color:#f92672>{</span>      
        String message <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;message_&#34;</span> <span style=color:#f92672>+</span> messageNo<span style=color:#f92672>;</span>       
        producer<span style=color:#f92672>.</span><span style=color:#a6e22e>send</span><span style=color:#f92672>(</span><span style=color:#66d9ef>new</span> ProducerRecordString<span style=color:#f92672>,</span> String<span style=color:#f92672>(</span>topic<span style=color:#f92672>,</span> message<span style=color:#f92672>));</span>       
        System<span style=color:#f92672>.</span><span style=color:#a6e22e>out</span><span style=color:#f92672>.</span><span style=color:#a6e22e>println</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Sent: &#34;</span> <span style=color:#f92672>+</span> message<span style=color:#f92672>);</span>        
        messageNo <span style=color:#f92672>++;</span>        
      <span style=color:#66d9ef>try</span> <span style=color:#f92672>{</span>           
        Thread<span style=color:#f92672>.</span><span style=color:#a6e22e>sleep</span><span style=color:#f92672>(</span>2000<span style=color:#f92672>);</span>      
      <span style=color:#f92672>}</span> <span style=color:#66d9ef>catch</span> <span style=color:#f92672>(</span>Exception e<span style=color:#f92672>)</span> <span style=color:#f92672>{</span>          
        e<span style=color:#f92672>.</span><span style=color:#a6e22e>printStackTrace</span><span style=color:#f92672>();</span>       
      <span style=color:#f92672>}</span>   
    <span style=color:#f92672>}</span>  
  <span style=color:#f92672>}</span> 
<span style=color:#f92672>}</span>
</code></pre></div><p>消费者中我们使用多线程的方式，循环发送消息</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=color:#75715e>// KafkaConsumerClient.java  
</span><span style=color:#75715e></span><span style=color:#f92672>package</span> com.test.spark.kafka<span style=color:#f92672>;</span> 
<span style=color:#f92672>import</span> kafka.consumer.ConsumerConnector$class<span style=color:#f92672>;</span> 
<span style=color:#f92672>import</span> org.apache.kafka.clients.consumer.ConsumerRecord<span style=color:#f92672>;</span> 
<span style=color:#f92672>import</span> org.apache.kafka.clients.consumer.ConsumerRecords<span style=color:#f92672>;</span> 
<span style=color:#f92672>import</span> org.apache.kafka.clients.consumer.KafkaConsumer<span style=color:#f92672>;</span> 
<span style=color:#f92672>import</span> org.apache.kafka.common.TopicPartition<span style=color:#f92672>;</span> 
<span style=color:#f92672>import</span> java.util.Arrays<span style=color:#f92672>;</span> 
<span style=color:#f92672>import</span> java.util.List<span style=color:#f92672>;</span> 
<span style=color:#f92672>import</span> java.util.Properties<span style=color:#f92672>;</span> 
<span style=color:#75715e>/**  Kafka消费者 */</span>
<span style=color:#66d9ef>public</span> <span style=color:#66d9ef>class</span> <span style=color:#a6e22e>KafkaConsumerClient</span> <span style=color:#f92672>{</span>  
  <span style=color:#66d9ef>private</span> String topic<span style=color:#f92672>;</span>   
  <span style=color:#66d9ef>public</span> <span style=color:#a6e22e>KafkaConsumerClient</span><span style=color:#f92672>(</span>String topic<span style=color:#f92672>)</span> <span style=color:#f92672>{</span>   
    <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>topic</span> <span style=color:#f92672>=</span> topic<span style=color:#f92672>;</span> 
  <span style=color:#f92672>}</span>  
  <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>start</span><span style=color:#f92672>()</span> <span style=color:#f92672>{</span>     
    Properties props <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> Properties<span style=color:#f92672>();</span>      
    props<span style=color:#f92672>.</span><span style=color:#a6e22e>put</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;bootstrap.servers&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;localhost:9092&#34;</span><span style=color:#f92672>);</span>     
    props<span style=color:#f92672>.</span><span style=color:#a6e22e>put</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;group.id&#34;</span><span style=color:#f92672>,</span> KafkaProperties<span style=color:#f92672>.</span><span style=color:#a6e22e>GROUP_ID</span><span style=color:#f92672>);</span><span style=color:#75715e>//不同ID 可以同时订阅消息     
</span><span style=color:#75715e></span>    props<span style=color:#f92672>.</span><span style=color:#a6e22e>put</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;enable.auto.commit&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;false&#34;</span><span style=color:#f92672>);</span><span style=color:#75715e>//自动commit     
</span><span style=color:#75715e></span>    props<span style=color:#f92672>.</span><span style=color:#a6e22e>put</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;auto.commit.interval.ms&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;1000&#34;</span><span style=color:#f92672>);</span><span style=color:#75715e>//定时commit的周期     
</span><span style=color:#75715e></span>    props<span style=color:#f92672>.</span><span style=color:#a6e22e>put</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;session.timeout.ms&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;30000&#34;</span><span style=color:#f92672>);</span><span style=color:#75715e>//consumer活性超时时间     
</span><span style=color:#75715e></span>    props<span style=color:#f92672>.</span><span style=color:#a6e22e>put</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;key.deserializer&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;org.apache.kafka.common.serialization.StringDeserializer&#34;</span><span style=color:#f92672>);</span>
    props<span style=color:#f92672>.</span><span style=color:#a6e22e>put</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;value.deserializer&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;org.apache.kafka.common.serialization.StringDeserializer&#34;</span><span style=color:#f92672>);</span>     
    KafkaConsumerString<span style=color:#f92672>,</span> String consumer <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> KafkaConsumerString<span style=color:#f92672>,</span> String<span style=color:#f92672>(</span>props<span style=color:#f92672>);</span>
    consumer<span style=color:#f92672>.</span><span style=color:#a6e22e>subscribe</span><span style=color:#f92672>(</span>Arrays<span style=color:#f92672>.</span><span style=color:#a6e22e>asList</span><span style=color:#f92672>(</span><span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>topic</span><span style=color:#f92672>));</span><span style=color:#75715e>//订阅TOPIC
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>try</span> <span style=color:#f92672>{</span>        
      <span style=color:#66d9ef>while</span><span style=color:#f92672>(</span><span style=color:#66d9ef>true</span><span style=color:#f92672>)</span> <span style=color:#f92672>{</span><span style=color:#75715e>//轮询ConsumerRecordsString        
</span><span style=color:#75715e></span>        String records <span style=color:#f92672>=</span>consumer<span style=color:#f92672>.</span><span style=color:#a6e22e>poll</span><span style=color:#f92672>(</span>Long<span style=color:#f92672>.</span><span style=color:#a6e22e>MAX_VALUE</span><span style=color:#f92672>);</span><span style=color:#75715e>//超时等待时间            
</span><span style=color:#75715e></span>        <span style=color:#66d9ef>for</span> <span style=color:#f92672>(</span>TopicPartition partition <span style=color:#f92672>:</span> records<span style=color:#f92672>.</span><span style=color:#a6e22e>partitions</span><span style=color:#f92672>())</span> <span style=color:#f92672>{</span>                 
          ListConsumerRecordString<span style=color:#f92672>,</span> String partitionRecords <span style=color:#f92672>=</span> records<span style=color:#f92672>.</span><span style=color:#a6e22e>records</span><span style=color:#f92672>(</span>partition<span style=color:#f92672>);</span>                 
          <span style=color:#66d9ef>for</span> <span style=color:#f92672>(</span>ConsumerRecordString<span style=color:#f92672>,</span> String record <span style=color:#f92672>:</span> partitionRecords<span style=color:#f92672>)</span> <span style=color:#f92672>{</span>                    
            System<span style=color:#f92672>.</span><span style=color:#a6e22e>out</span><span style=color:#f92672>.</span><span style=color:#a6e22e>println</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;receive&#34;</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;: &#34;</span> <span style=color:#f92672>+</span> record<span style=color:#f92672>.</span><span style=color:#a6e22e>value</span><span style=color:#f92672>());</span>                
          <span style=color:#f92672>}</span>                
          consumer<span style=color:#f92672>.</span><span style=color:#a6e22e>commitSync</span><span style=color:#f92672>();</span><span style=color:#75715e>//同步             
</span><span style=color:#75715e></span>        <span style=color:#f92672>}</span>         
      <span style=color:#f92672>}</span>     
    <span style=color:#f92672>}</span> <span style=color:#66d9ef>finally</span> <span style=color:#f92672>{</span>         
      consumer<span style=color:#f92672>.</span><span style=color:#a6e22e>close</span><span style=color:#f92672>();</span>     
    <span style=color:#f92672>}</span> 
  <span style=color:#f92672>}</span>
<span style=color:#f92672>}</span>
</code></pre></div><p>在消费中我们会<code>轮询消息</code></p><p><img src="https://img-blog.csdnimg.cn/20190926164606148.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><h4 id=flumekafka配合>Flume+Kafka配合<a hidden class=anchor aria-hidden=true href=#flumekafka配合>#</a></h4><p>把logger sink ===> kafka sink</p><p><code>sink kafka: producer</code></p><p>所以<code>启动一个kafka的consumer，直接对接到kafka sink消费掉即可</code></p><pre><code>//avro-memory-kafka.conf

Name the components on this agentavro-memory-kafka.sources = avro-source
avro-memory-kafka.sinks = kafka-sink
avro-memory-kafka.channels = memory-channel
Describe/configure the sourceavro-memory-kafka.sources.avro-source.type = avro
avro-memory-kafka.sources.avro-source.bind = localhost
avro-memory-kafka.sources.avro-source.port = 44444
Describe the sinkavro-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink
avro-memory-kafka.sinks.kafka-sink.brokerList = localhost:9092
avro-memory-kafka.sinks.kafka-sink.topic = test
avro-memory-kafka.sinks.kafka-sink.batchSize = 5
avro-memory-kafka.sinks.kafka-sink.requiredAcks = 1
Use a channel which buffers events in memoryavro-memory-kafka.channels.memory-channel.type = memory
avro-memory-kafka.channels.memory-channel.capacity = 1000
avro-memory-kafka.channels.memory-channel.transactionCapacity = 100
Bind the source and sink to the channelavro-memory-kafka.sources.avro-source.channels = memory-channel
avro-memory-kafka.sinks.kafka-sink.channel = memory-channel
</code></pre><p>注意这个batchSize,在数据量没有到达设定的阈值时，他会有一个timeout,这之后才会有数据发过来</p><h3 id=spark>Spark<a hidden class=anchor aria-hidden=true href=#spark>#</a></h3><h4 id=spark-简介>Spark 简介<a hidden class=anchor aria-hidden=true href=#spark-简介>#</a></h4><ol><li>什么是Spark？Spark作为Apache顶级的开源项目，是一个快速、通用的大规模数据处理引擎，和Hadoop的MapReduce计算框架类似，但是相对于MapReduce，Spark凭借其可伸缩、基于内存计算等特点，以及可以直接读写Hadoop上任何格式数据的优势，进行批处理时更加高效，并有更低的延迟。相对于“one stack to rule them all”的目标，实际上，Spark已经成为轻量级大数据快速处理的统一平台，各种不同的应用，如实时流处理、机器学习、交互式查询等，都可以通过Spark建立在不同的存储和运行系统上。</li><li>Spark是基于内存计算的大数据并行计算框架。Spark基于内存计算，提高了在大数据环境下数据处理的实时性，同时保证了高容错性和高可伸缩性，允许用户将Spark部署在大量廉价硬件之上，形成集群。</li><li>Spark于2009年诞生于加州大学伯克利分校AMPLab。目前，已经成为Apache软件基金会旗下的顶级开源项目。相对于MapReduce上的批量计算、迭代型计算以及基于Hive的SQL查询，Spark可以带来上百倍的性能提升。目前Spark的生态系统日趋完善，Spark SQL的发布、Hive on Spark项目的启动以及大量大数据公司对Spark全栈的支持，让Spark的数据分析范式更加丰富。</li></ol><h4 id=spark环境搭建>Spark环境搭建<a hidden class=anchor aria-hidden=true href=#spark环境搭建>#</a></h4><p><strong>Hadoop安装</strong>（Spark依赖于Hadoop安装）</p><p><a href=http://dblab.xmu.edu.cn/blog/install-hadoop>参考链接</a></p><p>Hadoop可以通过<a href=http://mirror.bit.edu.cn/apache/hadoop/common/>HadoopDownloadOne</a> 或者<a href=http://mirrors.cnnic.cn/apache/hadoop/common/>HadoopDownloadTwo</a> 下载，一般选择下载最新的稳定版本，即下载 “stable” 下的<code>hadoop-2.x.y.tar.gz</code> 这个格式的文件(详见安装文件夹中的hadoop-2.7.7)</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ sudo tar -zxf  hadoop-2.7.7.tar.gz  -C /usr/local    <span style=color:#75715e># 解压到/usr/local中</span>
$ cd /usr/local/
$ sudo mv ./hadoop-2.6.0/ ./hadoop            <span style=color:#75715e># 将文件夹名改为hadoop</span>
$ sudo chown -R hadoop ./hadoop       <span style=color:#75715e># 修改文件权限</span>
</code></pre></div><p>Hadoop 解压后即可使用。输入如下命令来检查 Hadoop 是否可用，成功则会显示 Hadoop 版本信息：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/hadoop
$ ./bin/hadoop version
</code></pre></div><p><strong>Hadoop单机配置及运行测试</strong></p><p>Hadoop 默认模式为非分布式模式（本地模式），无需进行其他配置即可运行。非分布式即单 Java 进程，方便进行调试。</p><p>现在我们可以执行例子来感受下 Hadoop 的运行。Hadoop 附带了丰富的例子（<code>运行 ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar 可以看到所有例子</code>），包括 wordcount、terasort、join、grep 等。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/hadoop
$ mkdir ./input
$ cp ./etc/hadoop/*.xml ./input   <span style=color:#75715e># 将配置文件作为输入文件</span>
$ ./bin/Hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep ./input ./output <span style=color:#e6db74>&#39;dfs[a-z.]+&#39;</span>
$ cat ./output/*          <span style=color:#75715e># 查看运行结果</span>
</code></pre></div><blockquote><p>注意，Hadoop 默认不会覆盖结果文件，因此再次运行上面实例会提示出错，需要先将 ./output 删除。</p><p>如果中间提示 Error: JAVA_HOME is not set and could not be found. 的错误，则说明之前设置 JAVA_HOME 环境变量那边就没设置好，请按教程先设置好 JAVA_HOME 变量，否则后面的过程都是进行不下去的。如果已经按照前面教程在.bashrc文件中设置了JAVA_HOME，还是出现 Error: JAVA_HOME is not set and could not be found. 的错误，那么，请到hadoop的安装目录修改配置文件“/usr/local/hadoop/etc/hadoop/hadoop-env.sh”，在里面找到“export JAVA_HOME=${JAVA_HOME}”这行，然后，把它修改成JAVA安装路径的具体地址，比如，“export JAVA_HOME=/usr/lib/jvm/default-java”，然后，再次启动Hadoop。</p></blockquote><p><strong>Spark安装</strong></p><p>此处采用Spark和Hadoop一起安装使用，这样，就可以让Spark使用HDFS存取数据。需要说明的是，当安装好Spark以后，里面就自带了scala环境，不需要额外安装scala。在安装spark之前，需要先安装Java和Hadoop。</p><p>需要的具体运行环境如下：</p><p>Ø Ubuntu16.04以上</p><p>Ø Hadoop 2.7.1以上</p><p>Ø Java JDK 1.8以上</p><p>Ø Spark 2.1.0 以上</p><p>Ø Python 3.4以上</p><p>（此次系统环境使用的Ubuntu16.04，自带Python，不需额外安装）</p><p><a href=http://spark.apache.org/downloads.html>Spark官网下载</a></p><p>由于已经安装了Hadoop，所以在<code>Choose a package type</code>后面需要选择<code>Pre-build with user-provided Hadoop</code>，然后点击<code>Download Spark</code>后面的<code>spark-2.1.0-bin-without-hadoop.tgz</code>下载即可。需要说明的是，Pre-build with user-provided Hadoop:属于“Hadoop free”版，这样下载到的Spark，可应用到任意Hadoop版本。</p><p>Spark部署模式主要有四种：<code>Local模式</code>（单机模式）、<code>Standalone模式</code>（使用Spark自带的简单集群管理器）、<code>YARN模式</code>（使用YARN作为集群管理器）和<code>Mesos模式</code>（使用Mesos作为集群管理器）。</p><p>这里介绍Local模式（单机模式）的 Spark安装。我们选择Spark 2.4.3版本，并且假设当前使用用户名hadoop登录了Linux操作系统。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ sudo tar -zxf ~/下载/spark-2.4.3-bin-without-hadoop.tgz -C/usr/local/
$ sudo mv ./spark-2.4.3-bin-without-hadoop/ ./spark
$ sudo chown -R hadoop:hadoop ./spark       <span style=color:#75715e># 此处的 hadoop 为你的用户名</span>
</code></pre></div><p>安装后，还需要修改Spark的配置文件spark-env.sh</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark
$ sudo cp conf/spark-env.sh.template conf/spark-env.sh
$ sudo vim conf/spark-env.sh
<span style=color:#75715e>#添加下面的环境变量信息</span>
export SPARK_DIST_CLASSPATH<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>/usr/local/hadoop/bin/hadoop:classpath<span style=color:#66d9ef>)</span>
</code></pre></div><p>有了上面的配置信息以后，Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。如果没有配置上面信息，Spark就只能读写本地数据，无法读写HDFS数据。</p><p>配置完成后就可以直接使用，不需要像Hadoop运行启动命令。通过运行Spark自带的示例，验证Spark是否安装成功。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark
$ bin/run-example SparkPi
</code></pre></div><p>过滤后的运行结果如下图示，可以得到π 的 5 位小数近似值：</p><p><img src=https://img-blog.csdnimg.cn/20190926164708610.png alt=在这里插入图片描述></p><p><strong>Spark不依赖Hadoop安装</strong></p><p>Spark同样也可以不依赖hadoop进行安装，但是仍然需要JDK环境，同样是在Spark官网上，选择spark-2.4.3-bin-hadoop2.7.tgz。我们直接将其解压出来，下面我们开始配置环境变量。我们进入编辑/etc/profile，在最后加上如下代码。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e>#Spark</span>
export SPARK_HOME<span style=color:#f92672>=</span>/opt/spark-2.4.3
export PATH<span style=color:#f92672>=</span>$PATH:$SPARK_HOME/bin
</code></pre></div><p>然后进入/spark-2.3.1/bin目录下即可直接运行spark-shell。</p><p>下面配置本地集群环境，首先我们进入刚刚解压的Spark目录，进入/spark-2.2.1/conf/，拷贝一份spark-env.sh。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cp spark-env.sh.template spark-env.sh
</code></pre></div><p>然后我们编辑这个文件，添加如下环境设置（按自身环境修改）</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e>#export SCALA_HOME=/opt/scala-2.13.0</span>
export JAVA_HOME<span style=color:#f92672>=</span>/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.181-3.b13.el7_5.x86_64  <span style=color:#75715e>#这里是你jdk的安装路径</span>
export SPARK_HOME<span style=color:#f92672>=</span>/opt/spark-2.4.3
export SPARK_MASTER_IP<span style=color:#f92672>=</span>XXX.XX.XX.XXX  <span style=color:#75715e>#将这里的xxx改为自己的Linux的ip地址</span>
<span style=color:#75715e>#export SPARK_EXECUTOR_MEMORY=512M</span>
<span style=color:#75715e>#export SPARK_WORKER_MEMORY=1G</span>
<span style=color:#75715e>#export master=spark://XXX.XX.XX.XXX:7070</span>
</code></pre></div><p>再回到conf目录下，拷贝一份slaves。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cp slaves.template slaves
</code></pre></div><p>在slaves最后加上<code>localhost</code>，保存即可。最后想要启动spark，进入安装目录下的<code>sbin</code>文件夹下，运行<code>start-all.sh</code>输入登录密码，master和worker进程就能按照配置文件启动。</p><h4 id=在spark-shell-中运行代码>在Spark Shell 中运行代码<a hidden class=anchor aria-hidden=true href=#在spark-shell-中运行代码>#</a></h4><p>这里介绍Spark Shell的基本使用。Spark shell提供了简单的方式来学习API，并且提供了交互的方式来分析数据。它属于REPL（Read-Eval-Print Loop，交互式解释器），提供了交互式执行环境，表达式计算完成就会输出结果，而不必等到整个程序运行完毕，因此可即时查看中间结果，并对程序进行修改，这样可以在很大程度上提升开发效率。</p><p>Spark Shell支持Scala和Python，本文使用 Scala 来进行介绍。前面已经安装了Hadoop和Spark，如果Spark不使用HDFS和YARN，那么就不用启动Hadoop也可以正常使用Spark。如果在使用Spark的过程中需要用到 HDFS，就要首先启动 Hadoop。</p><p>这里假设不需要用到HDFS，因此，就没有启动Hadoop。现在直接开始使用Spark。Spark-shell命令及其常用的参数如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ ./bin/spark-shell —master
</code></pre></div><p>Spark的运行模式取决于传递给SparkContext的Master URL的值。Master URL可以是以下任一种形式：</p><ul><li>local 使用一个Worker线程本地化运行SPARK(完全不并行)</li><li>local[*] 使用逻辑CPU个数数量的线程来本地化运行Spark</li><li>local[K] 使用K个Worker线程本地化运行Spark（理想情况下，K应该根据运行机器的CPU核数设定）</li><li>spark://HOST:PORT 连接到指定的Spark standalone master。默认端口是7077.</li><li>yarn-client 以客户端模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR 环境变量中找到。</li><li>yarn-cluster 以集群模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR 环境变量中找到。</li><li>mesos://HOST:PORT 连接到指定的Mesos集群。默认接口是5050。</li></ul><p>需要强调的是，本文采用“本地模式”（local）运行Spark，关于如何在集群模式下运行Spark，之后的文章会着重介绍。</p><p>在Spark中采用本地模式启动Spark Shell的命令主要包含以下参数：</p><p><code>–master</code>：这个参数表示当前的Spark Shell要连接到哪个master，如果是local[*]，就是使用本地模式启动spark-shell，其中，中括号内的星号表示需要使用几个CPU核心(core)；</p><p><code>–jars</code>： 这个参数用于把相关的JAR包添加到CLASSPATH中；如果有多个jar包，可以使用逗号分隔符连接它们；</p><p>比如，要采用本地模式，在4个CPU核心上运行spark-shell：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark
$ /bin/spark-shell —master local<span style=color:#f92672>[</span>4<span style=color:#f92672>]</span>
</code></pre></div><p>或者，可以在CLASSPATH中添加code.jar，命令如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark
$ ./bin/spark-shell -master local<span style=color:#f92672>[</span>4<span style=color:#f92672>]</span> --jars code.jar
</code></pre></div><p>可以执行<code>spark-shell –help</code>命令，获取完整的选项列表，具体如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark
$ ./bin/spark-shell —help
</code></pre></div><p>[外链图片转存失败(img-36hzRWo9-1569486879033)(spark-shell.png)]</p><p>上面是命令使用方法介绍，下面正式使用命令进入spark-shell环境，可以通过下面命令启动spark-shell环境：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala>scala<span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>8</span><span style=color:#f92672>*</span><span style=color:#ae81ff>2</span><span style=color:#f92672>+</span><span style=color:#ae81ff>5</span>
res0<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>Int</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>21</span>
</code></pre></div><p>最后，可以使用命令“:quit”退出Spark Shell，如下所示：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala>scala<span style=color:#66d9ef>&gt;:</span>quit
</code></pre></div><p>或者，也可以直接使用“Ctrl+D”组合键，退出Spark Shell</p><h4 id=scala编写wordcount>Scala编写wordCount<a hidden class=anchor aria-hidden=true href=#scala编写wordcount>#</a></h4><p><strong>任务需求</strong></p><p>学会了上文基本的安装和执行后，现在练习一个任务：编写一个Spark应用程序，对某个文件中的单词进行词频统计。</p><p>准备工作：进入Linux系统，打开“终端”，进入Shell命令提示符状态，然后，执行如下命令新建目录：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark
$ mkdir mycode
$ cd mycode
$ mkdir wordcount
$ cd wordcount
</code></pre></div><p>然后，在<code>/usr/local/spark/mycode/wordcount</code>目录下新建一个包含了一些语句的文本文件<code>word.txt</code>，命令如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$  vim word.txt
</code></pre></div><p>首先可以在文本文件中随意输入一些单词，用空格隔开，编写Spark程序对该文件进行单词词频统计。然后，按键盘Esc键退出vim编辑状态，输入“:wq”保存文件并退出vim编辑器。</p><h4 id=在spark-shell中执行词频统计>在Spark-Shell中执行词频统计<a hidden class=anchor aria-hidden=true href=#在spark-shell中执行词频统计>#</a></h4><ul><li><strong>启动Spark-Shell</strong>首先，登录Linux系统(要注意记住登录采用的用户名，本教程统一采用hadoop用户名进行登录)，打开“终端”（可以在Linux系统中使用Ctrl+Alt+T组合键开启终端），进入shell命令提示符状态，然后执行以下命令进入spark-shell：</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark
$ ./bin_spark-shell
$ …这里省略启动过程显示的一大堆信息
$ scala&gt;
</code></pre></div><p>启动进入spark-shell需要一点时间，在进入spark-shell后，我们可能还需要到Linux文件系统中对相关目录下的文件进行编辑和操作（比如要查看spark程序执行过程生成的文件），这个无法在park-shell中完成，因此，这里再打开第二个终端，用来在Linux系统的Shell命令提示符下操作。</p><ul><li><p>加载本地文件在开始具体词频统计代码之前，需要考虑如何加载文件，文件可能位于本地文件系统中，也有可能存放在分布式文件系统HDFS中，下面先介绍介绍如何加载本地文件，以及如何加载HDFS中的文件。首先，请在第二个终端窗口下操作，用下面命令到达</p><pre><code>/usr/local/spark/mycode/wordcount
</code></pre><p>目录，查看一下上面已经建好的word.txt的内容：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark/mycode/wordcount
$ cat word.txt
</code></pre></div><p>Cat命令会把word.txt文件的内容全部显示到屏幕上。</p><p>现在切换回spark-shell，然后输入下面命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala>scala<span style=color:#f92672>&gt;</span> <span style=color:#66d9ef>val</span> textFile <span style=color:#66d9ef>=</span> sc<span style=color:#f92672>.</span>textFile<span style=color:#f92672>(</span><span style=color:#960050;background-color:#1e0010>“</span>file<span style=color:#66d9ef>:</span><span style=color:#75715e>///usr/local/spark/mycode/wordcount/word.txt”)
</span></code></pre></div><p>上面代码中，val后面的是变量textFile，而sc.textFile()中的这个textFile是sc的一个方法名称，这个方法用来加载文件数据。这两个textFile不是一个东西，不要混淆。实际上，val后面的是变量textFile，你完全可以换个变量名称，比如,val lines = sc.textFile(“file:///usr/local/spark/mycode/wordcount/word.txt”)。这里使用相同名称，就是有意强调二者的区别。</p><p>注意要加载本地文件，必须采用“file:///”开头的这种格式。执行上上面这条命令以后，并不会马上显示结果，因为Spark采用惰性机制，只有遇到“行动”类型的操作，才会从头到尾执行所有操作。所以，下面我们执行一条“行动”类型的语句，就可以看到结果：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scala&gt;textFile.first<span style=color:#f92672>()</span>
</code></pre></div><p>first()是一个“行动”（Action）类型的操作，会启动真正的计算过程，从文件中加载数据到变量textFile中，并取出第一行文本。屏幕上会显示很多反馈信息，这里不再给出，你可以从这些结果信息中，找到word.txt文件中的第一行的内容。</p><p>正因为Spark采用了惰性机制，在执行转换操作的时候，即使我们输入了错误的语句，spark-shell也不会马上报错，而是等到执行“行动”类型的语句时启动真正的计算，那个时候“转换”操作语句中的错误就会显示出来，比如：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash> val textFile <span style=color:#f92672>=</span> sc.textFile<span style=color:#f92672>(</span>“file:///usr/local/spark/mycode/wordcount/word123.txt”<span style=color:#f92672>)</span>
</code></pre></div><p>上面我们使用了一个根本就不存在的word123.txt，执行上面语句时，spark-shell根本不会报错，因为，没有遇到“行动”类型的first()操作之前，这个加载操作时不会真正执行的。然后，我们执行一个“行动”类型的操作first()，如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash> scala&gt; textFile.first<span style=color:#f92672>()</span>
</code></pre></div><p>执行上面语句后，会返回错误信息“拒绝连接”，因为这个word123.txt文件根本就不存在。现在我们可以练习一下如何把textFile变量中的内容再次写回到另外一个文本文件wordback.txt中：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>val textFile <span style=color:#f92672>=</span> sc.textFile<span style=color:#f92672>(</span>“file:///usr/local/spark/mycode/wordcount/word.txt”<span style=color:#f92672>)</span>
textFile.saveAsTextFile<span style=color:#f92672>(</span>“file:///usr/local/spark/mycode/wordcount/writeback”<span style=color:#f92672>)</span>
</code></pre></div><p>上面的saveAsTextFile()括号里面的参数是保存文件的路径，不是文件名。saveAsTextFile()是一个“行动”（Action）类型的操作，所以马上会执行真正的计算过程，从word.txt中加载数据到变量textFile中，然后，又把textFile中的数据写回到本地文件目录“_usr_local_spark_mycode_wordcount_writeback/”下面，现在让我们切换到Linux Shell命令提示符窗口中，执行下面命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark/mycode/wordcount/writeback/
$ ls
</code></pre></div><p>执行结果会显示，有两个文件part-00000和_SUCCESS，我们可以使用cat命令查看一下part-00000文件，会发现结果是和上面word.txt中的内容一样的。</p><h4 id=词频统计>词频统计<a hidden class=anchor aria-hidden=true href=#词频统计>#</a></h4><p>有了前面的铺垫性介绍，下面我们开始第一个Spark应用程序：WordCount。请切换到spark-shell窗口，输入如下命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scala&gt; val textFile <span style=color:#f92672>=</span> sc.textFile<span style=color:#f92672>(</span>“file:///usr/local/spark/mycode/wordcount/word.txt”<span style=color:#f92672>)</span>
scala&gt; val wordCount <span style=color:#f92672>=</span> textFile.flatMap<span style=color:#f92672>(</span>line <span style=color:#f92672>=</span>&gt; line.split<span style=color:#f92672>(</span>“ “<span style=color:#f92672>))</span>.map<span style=color:#f92672>(</span>word <span style=color:#f92672>=</span>&gt; <span style=color:#f92672>(</span>word, 1<span style=color:#f92672>))</span>.reduceByKey<span style=color:#f92672>((</span>a, b<span style=color:#f92672>)</span> <span style=color:#f92672>=</span>&gt; a + b<span style=color:#f92672>)</span>
scala&gt; wordCount.collect<span style=color:#f92672>()</span>
</code></pre></div><p>上面只给出了代码，省略了执行过程中返回的结果信息，因为返回信息很多。下面简单解释一下上面的语句。</p><ul><li>textFile包含了多行文本内容，textFile.flatMap(line => line.split(” “))会遍历textFile中的每行文本内容，当遍历到其中一行文本内容时，会把文本内容赋值给变量line，并执行Lamda表达式line => line.split(” “)。line => line.split(” “)是一个Lamda表达式，左边表示输入参数，右边表示函数里面执行的处理逻辑，这里执行line.split(” “)，也就是针对line中的一行文本内容，采用空格作为分隔符进行单词切分，从一行文本切分得到很多个单词构成的单词集合。这样，对于textFile中的每行文本，都会使用Lamda表达式得到一个单词集合，最终，多行文本，就得到多个单词集合。textFile.flatMap()操作就把这多个单词集合“拍扁”得到一个大的单词集合。</li><li>然后，针对这个大的单词集合，执行map()操作，也就是map(word => (word, 1))，这个map操作会遍历这个集合中的每个单词，当遍历到其中一个单词时，就把当前这个单词赋值给变量word，并执行Lamda表达式word => (word, 1)，这个Lamda表达式的含义是，word作为函数的输入参数，然后，执行函数处理逻辑，这里会执行(word, 1)，也就是针对输入的word，构建得到一个tuple，形式为(word,1)，key是word，value是1（表示该单词出现1次）。</li><li>程序执行到这里，已经得到一个RDD，这个RDD的每个元素是(key,value)形式的tuple。最后，针对这个RDD，执行reduceByKey((a, b) => a + b)操作，这个操作会把所有RDD元素按照key进行分组，然后使用给定的函数（这里就是Lamda表达式：(a, b) => a + b），对具有相同的key的多个value进行reduce操作，返回reduce后的(key,value)，比如(“hadoop”,1)和(“hadoop”,1)，具有相同的key，进行reduce以后就得到(“hadoop”,2)，这样就计算得到了这个单词的词频。</li></ul></li></ul><h4 id=编写独立应用程序执行词频统计>编写独立应用程序执行词频统计<a hidden class=anchor aria-hidden=true href=#编写独立应用程序执行词频统计>#</a></h4><p>在上面spark-shell编写wordcount后，下面我们编写一个Scala应用程序来实现词频统计。首先登录Linux系统，进入Shell命令提示符状态，然后执行下面命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark/mycode/wordcount/
$ mkdir -p src/main/scala  这里加入-p选项，可以一起创建src目录及其子目录
</code></pre></div><p>然后在“/usr/local/spark/mycode/wordcount/src/main/scala”目录下新建一个test.scala文件，里面包含如下代码：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=color:#66d9ef>import</span> org.apache.spark.SparkContext
<span style=color:#66d9ef>import</span> org.apache.spark.SparkContext.<span style=color:#f92672>/</span>
<span style=color:#66d9ef>import</span> org.apache.spark.SparkConf

<span style=color:#66d9ef>object</span> <span style=color:#a6e22e>WordCount</span> <span style=color:#f92672>{</span>
<span style=color:#66d9ef>def</span> main<span style=color:#f92672>(</span>args<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>Array</span><span style=color:#f92672>[</span><span style=color:#66d9ef>String</span><span style=color:#f92672>])</span> <span style=color:#f92672>{</span>
<span style=color:#66d9ef>val</span> inputFile <span style=color:#66d9ef>=</span>  <span style=color:#960050;background-color:#1e0010>“</span>file<span style=color:#66d9ef>:</span><span style=color:#75715e>///usr/local/spark/mycode/wordcount/word.txt”
</span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> conf <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>SparkConf</span><span style=color:#f92672>().</span>setAppName<span style=color:#f92672>(</span><span style=color:#960050;background-color:#1e0010>“</span><span style=color:#a6e22e>WordCount</span><span style=color:#960050;background-color:#1e0010>”</span><span style=color:#f92672>).</span>setMaster<span style=color:#f92672>(</span><span style=color:#960050;background-color:#1e0010>“</span>local<span style=color:#f92672>[</span><span style=color:#960050;background-color:#1e0010>2</span><span style=color:#f92672>]</span><span style=color:#960050;background-color:#1e0010>”</span><span style=color:#f92672>)</span>
<span style=color:#66d9ef>val</span> sc <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>SparkContext</span><span style=color:#f92672>(</span>conf<span style=color:#f92672>)</span>
<span style=color:#66d9ef>val</span> textFile <span style=color:#66d9ef>=</span> sc<span style=color:#f92672>.</span>textFile<span style=color:#f92672>(</span>inputFile<span style=color:#f92672>)</span>
<span style=color:#66d9ef>val</span> wordCount <span style=color:#66d9ef>=</span> textFile<span style=color:#f92672>.</span>flatMap<span style=color:#f92672>(</span>line <span style=color:#66d9ef>=&gt;</span> line<span style=color:#f92672>.</span>split<span style=color:#f92672>(</span><span style=color:#960050;background-color:#1e0010>“</span> <span style=color:#960050;background-color:#1e0010>“</span><span style=color:#f92672>)).</span>map<span style=color:#f92672>(</span>word <span style=color:#66d9ef>=&gt;</span> <span style=color:#f92672>(</span>word<span style=color:#f92672>,</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>)).</span>reduceByKey<span style=color:#f92672>((</span>a<span style=color:#f92672>,</span> b<span style=color:#f92672>)</span> <span style=color:#66d9ef>=&gt;</span> a <span style=color:#f92672>+</span> b<span style=color:#f92672>)</span>
wordCount<span style=color:#f92672>.</span>foreach<span style=color:#f92672>(</span>println<span style=color:#f92672>)</span>
<span style=color:#f92672>}</span>
<span style=color:#f92672>}</span>
</code></pre></div><p>注意，SparkConf().setAppName(“WordCount”).setMaster(“local[2]”)这句语句，也可以删除.setMaster(“local[2]”)，只保留 <code>val conf = new SparkConf().setAppName(“WordCount”)</code>。</p><p>如果test.scala没有调用SparkAPI，则只要使用scalac命令编译后执行即可。此处test.scala程序依赖 Spark API，因此需要通过 sbt 进行编译打包。首先执行如下命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark/mycode/wordcount/
$ vim simple.sbt
</code></pre></div><p>通过上面代码，新建一个simple.sbt文件，请在该文件中输入下面代码：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala>name <span style=color:#66d9ef>:</span><span style=color:#f92672>=</span> <span style=color:#960050;background-color:#1e0010>“</span><span style=color:#a6e22e>Simple</span> <span style=color:#a6e22e>Project</span><span style=color:#960050;background-color:#1e0010>”</span>
version <span style=color:#66d9ef>:</span><span style=color:#f92672>=</span> <span style=color:#960050;background-color:#1e0010>“</span><span style=color:#ae81ff>1.0</span><span style=color:#960050;background-color:#1e0010>”</span>
scalaVersion <span style=color:#66d9ef>:</span><span style=color:#f92672>=</span> <span style=color:#960050;background-color:#1e0010>“</span><span style=color:#ae81ff>2.11</span><span style=color:#f92672>.</span><span style=color:#ae81ff>8</span><span style=color:#960050;background-color:#1e0010>”</span>
libraryDependencies <span style=color:#f92672>+=</span> <span style=color:#960050;background-color:#1e0010>“</span>org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#960050;background-color:#1e0010>”</span> <span style=color:#f92672>%%</span> <span style=color:#960050;background-color:#1e0010>“</span>spark<span style=color:#f92672>-</span>core<span style=color:#960050;background-color:#1e0010>”</span> <span style=color:#f92672>%</span> <span style=color:#960050;background-color:#1e0010>“</span><span style=color:#ae81ff>2.1</span><span style=color:#f92672>.</span><span style=color:#ae81ff>0</span><span style=color:#960050;background-color:#1e0010>”</span>
</code></pre></div><p>下面我们使用sbt打包Scala程序。为保证sbt能正常运行，先执行如下命令检查整个应用程序的文件结构，应该是类似下面的文件结构：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ ./src
$ ./src/main
$ ./src/main/scala
$ ./src/main/scala/test.scala
$ ./simple.sbt
$ ./word.txt
</code></pre></div><p>接着，我们就可以通过如下代码将整个应用程序打包成 JAR（首次运行同样需要下载依赖包 ）：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark/mycode/wordcount/  请一定把这目录设置为当前目录
$ /usr/local/sbt/sbt package
</code></pre></div><p>上面执行过程需要消耗几分钟时间，屏幕上会返回一下信息：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>hadoop@dblab-VirtualBox:_usr_local_spark_mycode_wordcount$ /usr_local_sbt_sbt package
OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize<span style=color:#f92672>=</span>256M; support was removed in 8.0
<span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Set current project to Simple Project <span style=color:#f92672>(</span>in build file:/usr_local_spark_mycode_wordcount/<span style=color:#f92672>)</span>
<span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Updating <span style=color:#f92672>{</span>file:/usr_local_spark_mycode_wordcount/<span style=color:#f92672>}</span>wordcount…
<span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Resolving jline#jline;2.12.1 …
<span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Done updating.
<span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Packaging _usr_local_spark_mycode_wordcount_target_scala-2.11_simple-project_2.11-1.0.jar …
<span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Done packaging.
<span style=color:#f92672>[</span>success<span style=color:#f92672>]</span> Total time: <span style=color:#ae81ff>34</span> s, completed 2017-2-20 10:13:13
</code></pre></div><p>若屏幕上返回上述信息表明打包成功，生成的 jar 包的位置为<code>/usr/local/spark/mycode/wordcount/target/scala-2.11_simple-project_2.11-1.0.jar</code>。</p><p>最后通过spark-submit 运行程序。我们就可以将生成的jar包通过spark-submit提交到Spark中运行了，命令如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ /usr/local/spark/bin/spark-submit —class “WordCount”  /usr/local/spark/mycode/wordcount/target/scala-2.11_simple-project_2.11-1.0.jar
</code></pre></div><p>最终得到的词频统计结果类似如下：</p><pre><code>(Spark,1)
(is,1)
(than,1)
(fast,1)
(love,2)
(i,1)
(I,1)
(hadoop,2)
</code></pre><h3 id=flume_kafka_sparkstreaming实现词频统计>Flume_Kafka_SparkStreaming实现词频统计<a hidden class=anchor aria-hidden=true href=#flume_kafka_sparkstreaming实现词频统计>#</a></h3><h4 id=准备工作>准备工作<a hidden class=anchor aria-hidden=true href=#准备工作>#</a></h4><p>在做这个project之前，需要预先准备好的环境如下：</p><p>安装kafka（参考第一节）、安装flume（参考第二节）、安装Spark（参考第三节） 。</p><p>做完上面三个工作之后，我们开始进入正式的词频统计Demo。</p><p><strong>Spark准备工作</strong></p><p>要通过Kafka连接Spark来进行Spark Streaming操作，Kafka和Flume等高级输入源，需要依赖独立的库（jar文件）。也就是说Spark需要jar包让Kafka和Spark streaming相连。按照我们前面安装好的Spark版本，这些jar包都不在里面，为了证明这一点，我们现在可以测试一下。请打开一个新的终端，输入以下命令启动spark-shell：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ cd /usr/local/spark
$ ./bin/spark-shell
</code></pre></div><p>启动成功后，在spark-shell中执行下面import语句：</p><p><strong>import org.apache.spark.streaming.kafka._</strong></p><p>程序报错，因为找不到相关jar包。根据Spark官网的说明，对于Spark版本，如果要使用Kafka，则需要下载spark-streaming-kafka相关jar包。<a href=https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-8>Jar包下载地址</a>（注意版本对应关系）。</p><p><img src="https://img-blog.csdnimg.cn/2019092616474757.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><p>接下来需要把这个文件复制到Spark目录的jars目录下，输入以下命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark/jars
$ mkdir kafka
$ cp ./spark-streaming-kafka-0-8_2.11-2.1.0.jar /usr/local/spark/jars/kafka
</code></pre></div><p>下面把Kafka安装目录的libs目录下的所有jar文件复制到<code>/usr/local/spark/jars/kafka</code>目录下输入以下命令：至此，所有环境准备工作已全部完成，下面开始编写代码。</p><p><strong>Project 过程</strong></p><ul><li><p>编写Flume配置文件flume_to_kafka.conf输入命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/kafka/libs
$ ls
$ cp ./* /usr/local/spark/jars/kafka
</code></pre></div><p>内容如下：</p><pre><code>a1.sources=r1
a1.channels=c1
a1.sinks=k1
#Describe/configure the source 
a1.sources.r1.type=netcat
a1.sources.r1.bind=localhost
a1.sources.r1.port=33333
#Describe the sink
a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink  
a1.sinks.k1.kafka.topic=test  
a1.sinks.k1.kafka.bootstrap.servers=localhost:9092  
a1.sinks.k1.kafka.producer.acks=1  
a1.sinks.k1.flumeBatchSize=20  
#Use a channel which buffers events in memory  
a1.channels.c1.type=memory
a1.channels.c1.capacity=1000000
a1.channels.c1.transactionCapacity=1000000
#Bind the source and sink to the channel
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1
</code></pre></li><li><p>编写Spark Streaming程序(进行词频统计的程序)首先创建scala代码的目录结构，输入命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark/mycode
$ mkdir flume_to_kafka
$ cd flume_to_kafka
$ mkdir -p src/main/scala
$ cd src/main/scala
$ vim KafkaWordCounter.scala
</code></pre></div><p>KafkaWordCounter.scala是用于单词词频统计，它会把从kafka发送过来的单词进行词频统计，代码内容如下：</p><p>reduceByKeyAndWindow函数作用解释如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=color:#66d9ef>package</span> org.apache.spark.examples.streaming
<span style=color:#66d9ef>import</span> org.apache.spark._
<span style=color:#66d9ef>import</span> org.apache.spark.SparkConf
<span style=color:#66d9ef>import</span> org.apache.spark.streaming._
<span style=color:#66d9ef>import</span> org.apache.spark.streaming.kafka._
<span style=color:#66d9ef>import</span> org.apache.spark.streaming.StreamingContext._
<span style=color:#66d9ef>import</span> org.apache.spark.streaming.kafka.KafkaUtils
  
<span style=color:#66d9ef>object</span> <span style=color:#a6e22e>KafkaWordCounter</span><span style=color:#f92672>{</span>
<span style=color:#66d9ef>def</span> main<span style=color:#f92672>(</span>args<span style=color:#66d9ef>:</span><span style=color:#66d9ef>Array</span><span style=color:#f92672>[</span><span style=color:#66d9ef>String</span><span style=color:#f92672>]){</span>
<span style=color:#a6e22e>StreamingExamples</span><span style=color:#f92672>.</span>setStreamingLogLevels<span style=color:#f92672>()</span>
<span style=color:#66d9ef>val</span> sc<span style=color:#66d9ef>=new</span> <span style=color:#a6e22e>SparkConf</span><span style=color:#f92672>().</span>setAppName<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;KafkaWordCounter&#34;</span><span style=color:#f92672>).</span>setMaster<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;local[2]&#34;</span><span style=color:#f92672>)</span>
<span style=color:#66d9ef>val</span> ssc<span style=color:#66d9ef>=new</span> <span style=color:#a6e22e>StreamingContext</span><span style=color:#f92672>(</span>sc<span style=color:#f92672>,</span><span style=color:#a6e22e>Seconds</span><span style=color:#f92672>(</span><span style=color:#ae81ff>10</span><span style=color:#f92672>))</span>
ssc<span style=color:#f92672>.</span>checkpoint<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;file:///usr/local/spark/mycode/flume_to_kafka/checkpoint&#34;</span><span style=color:#f92672>)</span> <span style=color:#75715e>//设置检查点
</span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> zkQuorum<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;localhost:2181&#34;</span> <span style=color:#75715e>//Zookeeper服务器地址
</span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> group<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;1&#34;</span>  <span style=color:#75715e>//topic所在的group，可以设置为自己想要的名称，比如不用1，而是val group = &#34;test-consumer-group&#34; 
</span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> topics<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;test&#34;</span> <span style=color:#75715e>//topics的名称          
</span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> numThreads<span style=color:#66d9ef>=</span><span style=color:#ae81ff>1</span> <span style=color:#75715e>//每个topic的分区数
</span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> topicMap<span style=color:#66d9ef>=</span>topics<span style=color:#f92672>.</span>split<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;,&#34;</span><span style=color:#f92672>).</span>map<span style=color:#f92672>((</span><span style=color:#66d9ef>_</span><span style=color:#f92672>,</span>numThreads<span style=color:#f92672>.</span>toInt<span style=color:#f92672>)).</span>toMap
<span style=color:#66d9ef>val</span> lineMap<span style=color:#66d9ef>=</span><span style=color:#a6e22e>KafkaUtils</span><span style=color:#f92672>.</span>createStream<span style=color:#f92672>(</span>ssc<span style=color:#f92672>,</span>zkQuorum<span style=color:#f92672>,</span>group<span style=color:#f92672>,</span>topicMap<span style=color:#f92672>)</span>
<span style=color:#66d9ef>val</span> lines<span style=color:#66d9ef>=</span>lineMap<span style=color:#f92672>.</span>map<span style=color:#f92672>(</span><span style=color:#66d9ef>_</span><span style=color:#f92672>.</span>_2<span style=color:#f92672>)</span>
<span style=color:#66d9ef>val</span> words<span style=color:#66d9ef>=</span>lines<span style=color:#f92672>.</span>flatMap<span style=color:#f92672>(</span><span style=color:#66d9ef>_</span><span style=color:#f92672>.</span>split<span style=color:#f92672>(</span><span style=color:#e6db74>&#34; &#34;</span><span style=color:#f92672>))</span>
<span style=color:#66d9ef>val</span> pair<span style=color:#66d9ef>=</span>words<span style=color:#f92672>.</span>map<span style=color:#f92672>(</span>x <span style=color:#66d9ef>=&gt;</span> <span style=color:#f92672>(</span>x<span style=color:#f92672>,</span><span style=color:#ae81ff>1</span><span style=color:#f92672>))</span>
<span style=color:#66d9ef>val</span> wordCounts<span style=color:#66d9ef>=</span>pair<span style=color:#f92672>.</span>reduceByKeyAndWindow<span style=color:#f92672>(</span><span style=color:#66d9ef>_</span> <span style=color:#f92672>+</span> <span style=color:#66d9ef>_</span><span style=color:#f92672>,</span><span style=color:#66d9ef>_</span> <span style=color:#f92672>-</span> <span style=color:#66d9ef>_</span><span style=color:#f92672>,</span><span style=color:#a6e22e>Minutes</span><span style=color:#f92672>(</span><span style=color:#ae81ff>2</span><span style=color:#f92672>),</span><span style=color:#a6e22e>Seconds</span><span style=color:#f92672>(</span><span style=color:#ae81ff>10</span><span style=color:#f92672>),</span><span style=color:#ae81ff>2</span><span style=color:#f92672>)</span> 
wordCounts<span style=color:#f92672>.</span>print
ssc<span style=color:#f92672>.</span>start
ssc<span style=color:#f92672>.</span>awaitTermination
<span style=color:#f92672>}</span>
<span style=color:#f92672>}</span>
</code></pre></div><blockquote><p>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]) 更加高效的reduceByKeyAndWindow，每个窗口的reduce值，是基于先前窗口的reduce值进行增量计算得到的；它会对进入滑动窗口的新数据进行reduce操作，并对离开窗口的老数据进行“逆向reduce”操作。但是，只能用于“可逆reduce函数”，即那些reduce函数都有一个对应的“逆向reduce函数”（以InvFunc参数传入）；</p><p>此代码中就是一个窗口转换操作reduceByKeyAndWindow，其中，Minutes(2)是滑动窗口长度，Seconds(10)是滑动窗口时间间隔（每隔多长时间滑动一次窗口）。reduceByKeyAndWindow中就使用了加法和减法这两个reduce函数，加法和减法这两种reduce函数都是“可逆的reduce函数”，也就是说，当滑动窗口到达一个新的位置时，原来之前被窗口框住的部分数据离开了窗口，又有新的数据被窗口框住，但是，这时计算窗口内单词的词频时，不需要对当前窗口内的所有单词全部重新执行统计，而是只要把窗口内新增进来的元素，增量加入到统计结果中，把离开窗口的元素从统计结果中减去，这样，就大大提高了统计的效率。尤其对于窗口长度较大时，这种“逆函数”带来的效率的提高是很明显的。</p></blockquote></li><li><p>创建StreamingExamples.scala继续在当前目录(/usr/local/spark/mycode/flume_to_kafka/src/main/scala)下创建StreamingExamples.scala代码文件，用于设置log4j，输入命令：</p><p><strong>vim StreamingExamples.scala</strong></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=color:#66d9ef>package</span> org.apache.spark.examples.streaming
<span style=color:#66d9ef>import</span> org.apache.spark.internal.Logging
<span style=color:#66d9ef>import</span> org.apache.log4j.<span style=color:#f92672>{</span><span style=color:#a6e22e>Level</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>Logger</span><span style=color:#f92672>}</span>
<span style=color:#75715e>//Utility functions for Spark Streaming examples. 
</span><span style=color:#75715e></span><span style=color:#66d9ef>object</span> <span style=color:#a6e22e>StreamingExamples</span> <span style=color:#66d9ef>extends</span> <span style=color:#a6e22e>Logging</span> <span style=color:#f92672>{</span>
<span style=color:#75715e>//Set reasonable logging levels for streaming if the user has not configured log4j. 
</span><span style=color:#75715e></span>  <span style=color:#66d9ef>def</span> setStreamingLogLevels<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
    <span style=color:#66d9ef>val</span> log4jInitialized <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>Logger</span><span style=color:#f92672>.</span>getRootLogger<span style=color:#f92672>.</span>getAllAppenders<span style=color:#f92672>.</span>hasMoreElements
    <span style=color:#66d9ef>if</span> <span style=color:#f92672>(!</span>log4jInitialized<span style=color:#f92672>)</span> <span style=color:#f92672>{</span>
      <span style=color:#75715e>// We first log something to initialize Spark&#39;s default logging, then we override the
</span><span style=color:#75715e></span>      <span style=color:#75715e>// logging level.
</span><span style=color:#75715e></span>      logInfo<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Setting log level to [WARN] for streaming example.&#34;</span> <span style=color:#f92672>+</span><span style=color:#e6db74>&#34; To override add a custom log4j.properties to the classpath.&#34;</span><span style=color:#f92672>)</span>
      <span style=color:#a6e22e>Logger</span><span style=color:#f92672>.</span>getRootLogger<span style=color:#f92672>.</span>setLevel<span style=color:#f92672>(</span><span style=color:#a6e22e>Level</span><span style=color:#f92672>.</span><span style=color:#a6e22e>WARN</span><span style=color:#f92672>)</span>
    <span style=color:#f92672>}</span>
  <span style=color:#f92672>}</span>
<span style=color:#f92672>}</span>
</code></pre></div></li><li><p>创建StreamingExamples.scala继续在当前目录(/usr/local/spark/mycode/flume_to_kafka/src/main/scala)下创建StreamingExamples.scala代码文件，用于设置log4j，输入命令：</p><p><strong>vim StreamingExamples.scala</strong></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=color:#66d9ef>package</span> org.apache.spark.examples.streaming
<span style=color:#66d9ef>import</span> org.apache.spark.internal.Logging
<span style=color:#66d9ef>import</span> org.apache.log4j.<span style=color:#f92672>{</span><span style=color:#a6e22e>Level</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>Logger</span><span style=color:#f92672>}</span>
<span style=color:#75715e>//Utility functions for Spark Streaming examples. 
</span><span style=color:#75715e></span><span style=color:#66d9ef>object</span> <span style=color:#a6e22e>StreamingExamples</span> <span style=color:#66d9ef>extends</span> <span style=color:#a6e22e>Logging</span> <span style=color:#f92672>{</span>
<span style=color:#75715e>//Set reasonable logging levels for streaming if the user has not configured log4j. 
</span><span style=color:#75715e></span>  <span style=color:#66d9ef>def</span> setStreamingLogLevels<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
    <span style=color:#66d9ef>val</span> log4jInitialized <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>Logger</span><span style=color:#f92672>.</span>getRootLogger<span style=color:#f92672>.</span>getAllAppenders<span style=color:#f92672>.</span>hasMoreElements
    <span style=color:#66d9ef>if</span> <span style=color:#f92672>(!</span>log4jInitialized<span style=color:#f92672>)</span> <span style=color:#f92672>{</span>
      <span style=color:#75715e>// We first log something to initialize Spark&#39;s default logging, then we override the
</span><span style=color:#75715e></span>      <span style=color:#75715e>// logging level.
</span><span style=color:#75715e></span>      logInfo<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Setting log level to [WARN] for streaming example.&#34;</span> <span style=color:#f92672>+</span><span style=color:#e6db74>&#34; To override add a custom log4j.properties to the classpath.&#34;</span><span style=color:#f92672>)</span>
      <span style=color:#a6e22e>Logger</span><span style=color:#f92672>.</span>getRootLogger<span style=color:#f92672>.</span>setLevel<span style=color:#f92672>(</span><span style=color:#a6e22e>Level</span><span style=color:#f92672>.</span><span style=color:#a6e22e>WARN</span><span style=color:#f92672>)</span>
    <span style=color:#f92672>}</span>
  <span style=color:#f92672>}</span>
<span style=color:#f92672>}</span>
</code></pre></div></li><li><p>打包文件simple.sbt输入命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala>$ cd <span style=color:#f92672>/</span>usr<span style=color:#f92672>/</span>local<span style=color:#f92672>/</span>spark<span style=color:#f92672>/</span>mycode<span style=color:#f92672>/</span>flume_to_kafka
$ vim simple<span style=color:#f92672>.</span>sbt
</code></pre></div><p>内容如下：</p><pre><code>name := &quot;Simple Project&quot;
version := &quot;1.0&quot;
scalaVersion := &quot;2.11.8&quot;
libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;2.1.0&quot;
libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-streaming_2.11&quot; % &quot;2.1.0&quot;
libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-streaming-kafka-0-8_2.11&quot; % &quot;2.1.0&quot;
</code></pre><p>要注意版本号一定要设置正确,在/usr/local/spark/mycode/flume_to_kafka目录下输入命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark/mycode/flume_to_kafka
$ find .
</code></pre></div><p>打包之前，这条命令用来查看代码结构，目录结构如下所示：</p></li></ul><p><img src="https://img-blog.csdnimg.cn/20190926164807779.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xqXzU1MDU2NjE4MQ==,size_16,color_FFFFFF,t_70" alt=在这里插入图片描述></p><ul><li>打包编译</li></ul><p>一定要在/usr/local/spark/mycode/flume_to_kafka目录下运行打包命令。</p><p>输入命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark/mycode/flume_to_kafka
$ /usr/local/sbt/sbt package
</code></pre></div><p>第一次打包的过程可能会很慢，请耐心等待几分钟。打包成功后，会看到SUCCESS的提示。</p><ul><li>启动zookeeper和kafka</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e>#启动zookeeper：</span>
$ cd /usr/local/kafka
$ ./bin/zookeeper-server-start.sh config/zookeeper.properties

<span style=color:#75715e># 新开一个终端，启动Kafka：</span>
$ cd /usr/local/kafka
$ bin/kafka-server-start.sh config/server.properties
</code></pre></div><ul><li>运行程序KafkaWordCounter</li></ul><p>打开一个新的终端，我们已经创建过topic，名为test（这是之前在flume_to_kafka.conf中设置的topic名字），端口号2181。在终端运行<code>KafkaWordCounter</code>程序，进行词频统计，由于现在没有启动输入，所以只有提示信息，没有结果。</p><p>输入命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/spark
$/usr/local/spark/bin/spark-submit --driver-class-path /usr/local/spark/jars/*:/usr/local/spark/jars/kafka/* --class <span style=color:#e6db74>&#34;org.apache.spark.examples.streaming.KafkaWordCounter&#34;</span> /usr/local/spark/mycode/flume_to_kafka/target/scala-2.11/simple-project_2.11-1.0.jar
</code></pre></div><blockquote><p>其中”/usr/local/spark/jars/“和”/usr/local/spark/jars/kafka/”用来指明引用的jar包，“org.apache.spark.examples.streaming.KafkaWordCounter”代表包名和类名，这是编写KafkaWordCounter.scala里面的包名和类名，最后一个参数用来说明打包文件的位置。</p></blockquote><p>执行该命令后，屏幕上会显示程序运行的相关信息，并会每隔10秒钟刷新一次信息，用来输出词频统计的结果，此时还只有提示信息，如下所示：</p><p>[外链图片转存失败(img-ZnMkMxIN-1569486879035)(result_one.png)]</p><p>在启动Flume之前，Zookeeper和Kafka要先启动成功，不然启动Flume会报连不上Kafka的错误。</p><ul><li>启动flume agent</li></ul><p>打开第四个终端，在这个新的终端中启动Flume Agent</p><p>输入命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cd /usr/local/flume
$ bin/flume-ng agent --conf ./conf --conf-file ./conf/flume_to_kafka.conf --name a1 -Dflume.root.logger<span style=color:#f92672>=</span>INFO,console
</code></pre></div><p>启动agent以后，该agent就会一直监听localhost的33333端口，这样，我们下面就可以通过“telnet localhost 33333”命令向Flume Source发送消息。这个终端也不要关闭，让它一直处于监听状态。</p><ul><li>发送消息</li></ul><p>打开第五个终端，发送消息。输入命令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ telnet localhost <span style=color:#ae81ff>33333</span>
</code></pre></div><p>这个端口33333是在flume conf文件中设置的source</p><p>在这个窗口里面随便敲入若干个字符和若干个回车，这些消息都会被Flume监听到，Flume把消息采集到以后汇集到Sink，然后由Sink发送给Kafka的topic(test)。因为spark Streaming程序不断地在监控topic，在输入终端和前面运行词频统计程序那个终端窗口内看到统计结果。</p><h2 id=分布式环境搭建及相关demo>分布式环境搭建及相关DEMO<a hidden class=anchor aria-hidden=true href=#分布式环境搭建及相关demo>#</a></h2><h3 id=flume-1>Flume<a hidden class=anchor aria-hidden=true href=#flume-1>#</a></h3><p>Flume在分布式环境下跟单机下一致，只需要在一台机器上搭建即可。</p><h3 id=kafka-1>Kafka<a hidden class=anchor aria-hidden=true href=#kafka-1>#</a></h3><p><strong>搭建高吞吐量Kafka分布式发布订阅消息集群</strong></p><ul><li>Zookeeper集群: 121.48.163.195:2181 , 113.54.154.68:2181，113.54.159.232:2181</li><li>kafka 集群: 121.48.163.195 , 113.54.154.68，113.54.159.232</li></ul><p><strong>搭建 kafka 集群</strong></p><p>kafka 集群: 121.48.163.195 , 113.54.154.68，113.54.159.232</p><ol><li><p><strong>下载kafka和zookeeper</strong>步骤和前面单机版一致</p></li><li><p>修改配置</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ vim /usr/local/kafka_2.12-0.11.0.0/config/server.properties 
   
设置broker.id
第一台为broker.id <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
第二台为broker.id <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
第三台为broker.id <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
注意这个broker.id每台服务器不能重复
然后设置zookeeper的集群地址
zookeeper.connect<span style=color:#f92672>=</span>121.48.163.195:2181 , 113.54.154.68:2181，113.54.159.232:2181
</code></pre></div></li><li><p>修改zookeeper配置文件</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ vim /usr/local/zookeeper-3.4.5/conf/zoo.cfg
<span style=color:#75715e>#添加server.1 server.2 server.3</span>
   
server.1<span style=color:#f92672>=</span>121.48.163.195:2888:3888
server.2<span style=color:#f92672>=</span>113.54.154.68:2888:3888
server.3<span style=color:#f92672>=</span>113.54.159.232:2888:3888
<span style=color:#75715e>#添加id</span>
$ sudo echo “1” &gt; /usr/local/zookeeper-3.4.5/data/myid<span style=color:#f92672>(</span>每台机器的id可以和brokerid保持一致<span style=color:#f92672>)</span>
</code></pre></div></li><li><p>启动服务</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># 每台机器运行命令，但是在实际大型集群中可以使用脚本的方式一键启动</span>
$ bin/kafka-server-start.sh config/server.properties &amp;
</code></pre></div></li><li><p>创建主题</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ /usr/local/kafka_2.12-0.11.0.0/bin/kafka-topics.sh --create --zookeeper 121.48.163.195:2181 , 113.54.154.68:2181，113.54.159.232:2181 --replication-factor <span style=color:#ae81ff>2</span> --partitions <span style=color:#ae81ff>1</span> --topic ymq 
   
–replication-factor <span style=color:#ae81ff>2</span> <span style=color:#75715e>#复制两份</span>
–partitions <span style=color:#ae81ff>1</span> <span style=color:#75715e>#创建1个分区</span>
–topic <span style=color:#75715e>#主题为ymq</span>
<span style=color:#75715e># 运行list topic命令，可以看到该主题：</span>
$ /usr/local/kafka_2.12-0.11.0.0/bin/kafka-topics.sh –list –zookeeper 121.48.163.195:2181 , 113.54.154.68:2181，113.54.159.232:2181
</code></pre></div></li><li><p>其它操作其它操作基本语法差不多一致，不再赘述，详情可以参考官网</p></li><li><p>Kafka Manager<a href=https://github.com/yahoo/kafka-manager>Yahoo开源Kafka集群管理器Kafka Manager</a></p></li></ol><h3 id=spark-1>Spark<a hidden class=anchor aria-hidden=true href=#spark-1>#</a></h3><ul><li><p>选取三台服务器</p><ul><li>121.48.163.195 主节点</li><li>113.54.154.68 从节点</li><li>113.54.159.232 从节点</li></ul><p>设置三台服务器root用户，之后操作都用root用户进行，便于管理</p></li><li><p>修改hosts文件</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ sudo vim /etc/hosts
<span style=color:#75715e># 在上面加上服务器ip</span>
121.48.163.195 Master
113.54.154.68  Slave1
113.54.159.232 Slave2
</code></pre></div><p>修改完之后</p><pre><code>source /etc/hosts
</code></pre></li><li><p>SSH无密码验证配置</p><ul><li><p>安装和启动ssh协议我们需要两个服务：ssh和rsync。可以通过下面命令查看是否已经安装：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash> rpm -qa|grep openssh
    
　　   rpm -qa|grep rsync
　　如果没有安装ssh和rsync，可以通过下面命令进行安装：
　　   sudo apt  install ssh （安装ssh协议）
　　   sudo apt  install rsync （rsync是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件）
　　   service sshd restart （启动服务）
</code></pre></div></li><li><p>配置Master无密码登录所有Slave配置Master节点，以下是在Master节点的配置操作。</p><ul><li>在Master节点上生成密码对，在Master节点上执行以下命令：</li></ul><p>ssh-keygen -t rsa -P ‘’</p><p>生成的密钥对：id_rsa和id_rsa.pub，默认存储在”/root/.ssh”目录下。</p><ul><li>接着在Master节点上做如下配置，把id_rsa.pub追加到授权的key里面去。</li></ul><p>cat ~/.ssh/id_rsa.pub &#187; ~/.ssh/authorized_keys</p><ul><li><p>修改ssh配置文件"/etc/ssh/sshd_config"的下列内容，将以下内容的注释去掉，在三台机器上均进行修改：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash> RSAAuthentication yes <span style=color:#75715e># 启用 RSA 认证</span>
      
　　   PubkeyAuthentication yes <span style=color:#75715e># 启用公钥私钥配对认证方式</span>
　　   AuthorizedKeysFile .ssh/authorized_keys <span style=color:#75715e># 公钥文件路径（和上面生成的文件同）</span>
</code></pre></div></li><li><p>重启ssh服务，才能使刚才设置有效。service sshd restart</p></li><li><p>验证无密码登录本机是否成功ssh localhost</p></li><li><p>接下来的就是把公钥复制到所有的Slave机器上。使用下面的命令进行复制公钥：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ scp /root/.ssh/id_rsa.pub root@Slave1:/root/
      
$ scp /root/.ssh/id_rsa.pub root@Slave2:/root/
　　
</code></pre></div></li></ul><p>接着配置Slave节点，以下是在Slave1节点的配置操作。</p><p>1>在”/root/“下创建”.ssh”文件夹，如果已经存在就不需要创建了。</p><p>mkdir /root/.ssh</p><p>2）将Master的公钥追加到Slave1的授权文件”authorized_keys”中去。</p><p>cat /root/id_rsa.pub &#187; /root/.ssh/authorized_keys</p><p>3）修改”/etc/ssh/sshd_config”，具体步骤参考前面Master设置的第3步和第4步。</p><p>4）用Master使用ssh无密码登录Slave1</p><p>ssh 114.55.246.77</p><p>5）把”/root/“目录下的”id_rsa.pub”文件删除掉。</p><p>rm –r /root/id_rsa.pub</p><p>重复上面的5个步骤把Slave2服务器进行相同的配置。</p></li><li><p>配置Slave无密码登录Master以下是在Slave1节点的配置操作。</p><p>1）创建”Slave1”自己的公钥和私钥，并把自己的公钥追加到”authorized_keys”文件中，执行下面命令：</p><p>ssh-keygen -t rsa -P ‘’</p><p>cat /root/.ssh/id_rsa.pub &#187; /root/.ssh/authorized_keys</p><p>2）将Slave1节点的公钥”id_rsa.pub”复制到Master节点的”/root/“目录下。</p><p>scp /root/.ssh/id_rsa.pub root@Master:/root/</p><p>以下是在Master节点的配置操作。</p><p>1）将Slave1的公钥追加到Master的授权文件”authorized_keys”中去。</p><p>cat ~/id_rsa.pub &#187; ~/.ssh/authorized_keys</p><p>2）删除Slave1复制过来的”id_rsa.pub”文件。</p><p>rm –r /root/id_rsa.pub</p><p>配置完成后测试从Slave1到Master无密码登录。</p><p>ssh 114.55.246.88</p><p>按照上面的步骤把Slave2和Master之间建立起无密码登录。这样，Master能无密码验证登录每个Slave，每个Slave也能无密码验证登录到Master。</p></li><li></li></ul></li><li><p>安装基础环境（JAVA和SCALA环境）这里不再赘述</p></li><li><p>Hadoop2.7.3完全分布式搭建以下是在Master节点操作：</p><ul><li><p>下载二进制包hadoop-2.7.7.tar.gz</p></li><li><p>解压并移动到相应目录，我习惯将软件放到/opt目录下，命令如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ tar -zxvf hadoop-2.7.3.tar.gz
    
$ mv hadoop-2.7.7 /opt
</code></pre></div></li><li><p>修改对应的配置文件，修改/etc/profile，增加如下内容：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash> export HADOOP_HOME<span style=color:#f92672>=</span>/opt/hadoop-2.7.3/
 export PATH<span style=color:#f92672>=</span>$PATH:$HADOOP_HOME/bin
 export PATH<span style=color:#f92672>=</span>$PATH:$HADOOP_HOME/sbin
 export HADOOP_MAPRED_HOME<span style=color:#f92672>=</span>$HADOOP_HOME
 export HADOOP_COMMON_HOME<span style=color:#f92672>=</span>$HADOOP_HOME
 export HADOOP_HDFS_HOME<span style=color:#f92672>=</span>$HADOOP_HOME
 export YARN_HOME<span style=color:#f92672>=</span>$HADOOP_HOME
 export HADOOP_ROOT_LOGGER<span style=color:#f92672>=</span>INFO,console
 export HADOOP_COMMON_LIB_NATIVE_DIR<span style=color:#f92672>=</span>$HADOOP_HOME/lib/native
 export HADOOP_OPTS<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;-Djava.library.path=</span>$HADOOP_HOME<span style=color:#e6db74>/lib&#34;</span>
</code></pre></div></li><li><p>修改完成后执行<code>$ source /etc/profile</code></p></li><li><p>修改$HADOOP_HOME/etc/hadoop/hadoop-env.sh，修改JAVA_HOME 如下： <code>export JAVA_HOME=/usr/local/jdk1.8.0_121</code></p></li><li><p>修改$HADOOP_HOME/etc/hadoop/slaves，将原来的localhost删除，改成如下内容：</p><ul><li>Slave1</li><li>Slave2</li></ul></li><li><p>修改$HADOOP_HOME/etc/hadoop/core-site.xml</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=color:#f92672>&lt;configuration&gt;</span>
      <span style=color:#f92672>&lt;property&gt;</span>
          <span style=color:#f92672>&lt;name&gt;</span>fs.defaultFS<span style=color:#f92672>&lt;/name&gt;</span>
          <span style=color:#f92672>&lt;value&gt;</span>hdfs://Master:9000<span style=color:#f92672>&lt;/value&gt;</span>
      <span style=color:#f92672>&lt;/property&gt;</span>
      <span style=color:#f92672>&lt;property&gt;</span>
         <span style=color:#f92672>&lt;name&gt;</span>io.file.buffer.size<span style=color:#f92672>&lt;/name&gt;</span>
         <span style=color:#f92672>&lt;value&gt;</span>131072<span style=color:#f92672>&lt;/value&gt;</span>
     <span style=color:#f92672>&lt;/property&gt;</span>
     <span style=color:#f92672>&lt;property&gt;</span>
          <span style=color:#f92672>&lt;name&gt;</span>hadoop.tmp.dir<span style=color:#f92672>&lt;/name&gt;</span>
          <span style=color:#f92672>&lt;value&gt;</span>/opt/hadoop-2.7.7/tmp<span style=color:#f92672>&lt;/value&gt;</span>
     <span style=color:#f92672>&lt;/property&gt;</span>
<span style=color:#f92672>&lt;/configuration&gt;</span>
</code></pre></div></li><li><p>修改$HADOOP_HOME/etc/hadoop/hdfs-site.xml</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=color:#f92672>&lt;configuration&gt;</span>
    <span style=color:#f92672>&lt;property&gt;</span>
      <span style=color:#f92672>&lt;name&gt;</span>dfs.namenode.secondary.http-address<span style=color:#f92672>&lt;/name&gt;</span>
      <span style=color:#f92672>&lt;value&gt;</span>Master:50090<span style=color:#f92672>&lt;/value&gt;</span>
    <span style=color:#f92672>&lt;/property&gt;</span>
    <span style=color:#f92672>&lt;property&gt;</span>
      <span style=color:#f92672>&lt;name&gt;</span>dfs.replication<span style=color:#f92672>&lt;/name&gt;</span>
      <span style=color:#f92672>&lt;value&gt;</span>2<span style=color:#f92672>&lt;/value&gt;</span>
    <span style=color:#f92672>&lt;/property&gt;</span>
    <span style=color:#f92672>&lt;property&gt;</span>
      <span style=color:#f92672>&lt;name&gt;</span>dfs.namenode.name.dir<span style=color:#f92672>&lt;/name&gt;</span>
      <span style=color:#f92672>&lt;value&gt;</span>file:/opt/hadoop-2.7.7/hdfs/name<span style=color:#f92672>&lt;/value&gt;</span>
    <span style=color:#f92672>&lt;/property&gt;</span>
    <span style=color:#f92672>&lt;property&gt;</span>
      <span style=color:#f92672>&lt;name&gt;</span>dfs.datanode.data.dir<span style=color:#f92672>&lt;/name&gt;</span>
      <span style=color:#f92672>&lt;value&gt;</span>file:/opt/hadoop-2.7.7/hdfs/data<span style=color:#f92672>&lt;/value&gt;</span>
    <span style=color:#f92672>&lt;/property&gt;</span>
<span style=color:#f92672>&lt;/configuration&gt;</span>
</code></pre></div></li><li><pre><code>cp mapred-site.xml.template mapred-site.xml
</code></pre><p>，并修改$HADOOP_HOME/etc/hadoop/mapred-site.xml</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=color:#f92672>&lt;configuration&gt;</span>
 <span style=color:#f92672>&lt;property&gt;</span>
    <span style=color:#f92672>&lt;name&gt;</span>mapreduce.framework.name<span style=color:#f92672>&lt;/name&gt;</span>
    <span style=color:#f92672>&lt;value&gt;</span>yarn<span style=color:#f92672>&lt;/value&gt;</span>
  <span style=color:#f92672>&lt;/property&gt;</span>
  <span style=color:#f92672>&lt;property&gt;</span>
          <span style=color:#f92672>&lt;name&gt;</span>mapreduce.jobhistory.address<span style=color:#f92672>&lt;/name&gt;</span>
          <span style=color:#f92672>&lt;value&gt;</span>Master:10020<span style=color:#f92672>&lt;/value&gt;</span>
  <span style=color:#f92672>&lt;/property&gt;</span>
  <span style=color:#f92672>&lt;property&gt;</span>
          <span style=color:#f92672>&lt;name&gt;</span>mapreduce.jobhistory.address<span style=color:#f92672>&lt;/name&gt;</span>
          <span style=color:#f92672>&lt;value&gt;</span>Master:19888<span style=color:#f92672>&lt;/value&gt;</span>
  <span style=color:#f92672>&lt;/property&gt;</span>
<span style=color:#f92672>&lt;/configuration&gt;</span>
</code></pre></div></li><li><p>修改$HADOOP_HOME/etc/hadoop/yarn-site.xml</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=color:#f92672>&lt;configuration&gt;</span>
     <span style=color:#f92672>&lt;property&gt;</span>
         <span style=color:#f92672>&lt;name&gt;</span>yarn.nodemanager.aux-services<span style=color:#f92672>&lt;/name&gt;</span>
         <span style=color:#f92672>&lt;value&gt;</span>mapreduce_shuffle<span style=color:#f92672>&lt;/value&gt;</span>
     <span style=color:#f92672>&lt;/property&gt;</span>
     <span style=color:#f92672>&lt;property&gt;</span>
         <span style=color:#f92672>&lt;name&gt;</span>yarn.resourcemanager.address<span style=color:#f92672>&lt;/name&gt;</span>
         <span style=color:#f92672>&lt;value&gt;</span>Master:8032<span style=color:#f92672>&lt;/value&gt;</span>
     <span style=color:#f92672>&lt;/property&gt;</span>
     <span style=color:#f92672>&lt;property&gt;</span>
         <span style=color:#f92672>&lt;name&gt;</span>yarn.resourcemanager.scheduler.address<span style=color:#f92672>&lt;/name&gt;</span>
         <span style=color:#f92672>&lt;value&gt;</span>Master:8030<span style=color:#f92672>&lt;/value&gt;</span>
     <span style=color:#f92672>&lt;/property&gt;</span>
     <span style=color:#f92672>&lt;property&gt;</span>
         <span style=color:#f92672>&lt;name&gt;</span>yarn.resourcemanager.resource-tracker.address<span style=color:#f92672>&lt;/name&gt;</span>
         <span style=color:#f92672>&lt;value&gt;</span>Master:8031<span style=color:#f92672>&lt;/value&gt;</span>
     <span style=color:#f92672>&lt;/property&gt;</span>
     <span style=color:#f92672>&lt;property&gt;</span>
         <span style=color:#f92672>&lt;name&gt;</span>yarn.resourcemanager.admin.address<span style=color:#f92672>&lt;/name&gt;</span>
         <span style=color:#f92672>&lt;value&gt;</span>Master:8033<span style=color:#f92672>&lt;/value&gt;</span>
     <span style=color:#f92672>&lt;/property&gt;</span>
     <span style=color:#f92672>&lt;property&gt;</span>
         <span style=color:#f92672>&lt;name&gt;</span>yarn.resourcemanager.webapp.address<span style=color:#f92672>&lt;/name&gt;</span>
         <span style=color:#f92672>&lt;value&gt;</span>Master:8088<span style=color:#f92672>&lt;/value&gt;</span>
     <span style=color:#f92672>&lt;/property&gt;</span>
<span style=color:#f92672>&lt;/configuration&gt;</span>
</code></pre></div></li><li><p>复制Master节点的hadoop文件夹到Slave1和Slave2上</p></li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ scp -r /opt/hadoop-2.7.7 root@Slave1:/opt
$ scp -r /opt/hadoop-2.7.7 root@Slave2:/opt
</code></pre></div><ul><li>在Slave1和Slave2上分别修改/etc/profile，过程同Master一样</li><li>在Master节点启动集群，启动之前格式化一下namenode：<ul><li>Hadoop namenode -format</li><li>启动：<code>/opt/hadoop-2.7.7/sbin/start-all.sh</code></li><li>至此hadoop的完全分布式搭建完毕</li></ul></li><li>查看集群是否启动成功：</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>  $ jps -m 
  
　　Master显示：
　　   SecondaryNameNode
　　   ResourceManager
　　   NameNode
　　Slave显示：
　　   NodeManager
　　   DataNode
</code></pre></div></li><li><p>Spark完全分布式环境搭建以下操作都在Master节点进行。</p><ul><li><p>下载二进制包spark-2.4.3-bin-hadoop2.7.tgz</p></li><li><p>解压并移动到相应目录，命令如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ tar -zxvf spark-2.4.3-bin-hadoop2.7.tgz
    
$ mv hadoop-2.7.3 /opt
</code></pre></div></li><li><p>修改相应的配置文件，修改/etc/profie，增加如下内容：</p><pre><code>export SPARK_HOME=/opt/spark-2.4.3-bin-hadoop2.7/
export PATH=$PATH:$SPARK_HOME/bin
</code></pre></li><li><p>复制spark-env.sh.template成spark-env.sh</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cp spark-env.sh.template spark-env.sh
</code></pre></div></li><li><p>修改$SPARK_HOME/conf/spark-env.sh，添加如下内容：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>export   JAVA_HOME<span style=color:#f92672>=</span>/usr/local/jdk1.8.0_121   
export   SCALA_HOME<span style=color:#f92672>=</span>/usr/share/scala   
export   HADOOP_HOME<span style=color:#f92672>=</span>/opt/hadoop-2.7.3   
export   HADOOP_CONF_DIR<span style=color:#f92672>=</span>/opt/hadoop-2.7.3/etc/hadoop   
export   SPARK_MASTER_IP<span style=color:#f92672>=</span>114.55.246.88   
export   SPARK_MASTER_HOST<span style=color:#f92672>=</span>114.55.246.88   
export   SPARK_LOCAL_IP<span style=color:#f92672>=</span>114.55.246.88   
export   SPARK_WORKER_MEMORY<span style=color:#f92672>=</span>1g   
export   SPARK_WORKER_CORES<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>   
export   SPARK_HOME<span style=color:#f92672>=</span>/opt/spark-2.4.3-bin-hadoop2.7   
export   SPARK_DIST_CLASSPATH<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>/opt/hadoop-2.7.3/bin/hadoop classpath<span style=color:#66d9ef>)</span>   
</code></pre></div></li><li><p>复制slaves.template成slaves</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cp slaves.template slaves
</code></pre></div></li><li><p>修改$SPARK_HOME/conf/slaves，添加如下内容：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>Master
Slave1
Slave2
</code></pre></div></li><li><p>将配置好的spark文件复制到Slave1和Slave2节点</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ scp /opt/spark-2.4.3-bin-hadoop2.7 root@Slave1:/opt
$ scp /opt/spark-2.4.3-bin-hadoop2.7 root@Slave2:/opt
</code></pre></div></li><li><p>修改Slave1和Slave2配置在Slave1和Slave2上分别修改/etc/profile，增加Spark的配置，过程同Master一样。</p><p>在Slave1和Slave2修改$SPARK_HOME/conf/spark-env.sh，将export SPARK_LOCAL_IP=114.55.246.88改成Slave1和Slave2对应节点的IP。</p></li><li><p>在Master节点启动集群<code>/opt/spark-2.4.3-bin-hadoop2.7/sbin/start-all.sh</code></p></li><li><p>查看集群是否启动成功</p><pre><code>$ jps -m
Master在Hadoop的基础上新增了：
    
　　   Master
    
　　Slave在Hadoop的基础上新增了：
    
　　   Worker
</code></pre></li></ul></li></ul><p>在我的<a href=https://mp.csdn.net/mdeditor/www.laixiaojieblog.cn>博客</a>查看更多</p></div><footer class=post-footer><div style="padding:10px 0;margin:20px auto;width:100%;font-size:16px;text-align:center"><button id=rewardButton disable=enable onclick="var qr=document.getElementById('QR');if(qr.style.display==='none'){qr.style.display='block';}else{qr.style.display='none'}">
<span>打赏</span></button><div id=QR style=display:none><div id=wechat style=display:inline-block><a class=fancybox rel=group><img id=wechat_qr src=/wechat.jpeg alt="WeChat Pay"></a><p>微信打赏</p></div><div id=alipay style=display:inline-block><a class=fancybox rel=group><img id=alipay_qr src=/alipay.jpeg alt=Alipay></a><p>支付宝打赏</p></div></div></div><ul class=post-tags><li><a href=http://tuuna.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE>大数据</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2020 <a href=http://tuuna.top>洛文小站</a></span>
<span>&#183;</span>
<span>备案号 <a href=http://www.beian.miit.gov.cn/ rel="noopener noreferrer">蜀ICP备15022210号-3</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top" accesskey=g><button class=top-link id=top-link type=button><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>window.onload=function(){if(localStorage.getItem("menu-scroll-position")){document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position");}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});});});var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft);}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>